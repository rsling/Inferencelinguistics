% !Rnw root = ../main.Rnw
<<setupfisher, cache=T, echo=FALSE, include=FALSE, results='asis'>>=
opts_knit$set(self.contained=FALSE)

set.seed(949)

tea0 <- sample(rep(c(0,1),4))
tea.exp <- sample(rep(c(0,1),4))

sim.guess.n <- 1000
sim.guess.p <- rep(NULL, sim.guess.n)
sim.guess.s <- 1000
for (i in 1:sim.guess.n) {
  .tea <- sample(rep(c(0,1), sim.guess.s/2))
  .exp <- sample(rep(c(0,1), sim.guess.s/2))
  sim.guess.p[i] <- fisher.test(table(.tea, .exp), alternative = "gr")$p.value
}

sim.better.n <- 1000
sim.better.s <- 1000

sim.fiftyone.p <- rep(NULL, sim.better.n)
sim.fiftyone.q <- 0.51
for (i in 1:sim.better.n) {
  sim.fiftyone.c <- rbinom(sim.better.s, 1, sim.fiftyone.q)
  .tea <- sample(rep(c(0,1), sim.better.s/2))
  .exp <- ifelse(sim.fiftyone.c, .tea, !.tea)
  sim.fiftyone.p[i] <- fisher.test(table(.tea, .exp), alternative = "gr")$p.value
}

sim.fiftytwo.p <- rep(NULL, sim.better.n)
sim.fiftytwo.q <- 0.52
for (i in 1:sim.better.n) {
  sim.fiftytwo.c <- rbinom(sim.better.s, 1, sim.fiftytwo.q)
  .tea <- sample(rep(c(0,1), sim.better.s/2))
  .exp <- ifelse(sim.fiftytwo.c, .tea, !.tea)
  sim.fiftytwo.p[i] <- fisher.test(table(.tea, .exp), alternative = "gr")$p.value
}

sim.fiftyfive.p <- rep(NULL, sim.better.n)
sim.fiftyfive.q <- 0.55
for (i in 1:sim.better.n) {
  sim.fiftyfive.c <- rbinom(sim.better.s, 1, sim.fiftyfive.q)
  .tea <- sample(rep(c(0,1), sim.better.s/2))
  .exp <- ifelse(sim.fiftyfive.c, .tea, !.tea)
  sim.fiftyfive.p[i] <- fisher.test(table(.tea, .exp), alternative = "gr")$p.value
}

sim.seventy.p <- rep(NULL, sim.better.n)
sim.seventy.q <- 0.70
for (i in 1:sim.better.n) {
  sim.seventy.c <- rbinom(sim.better.s, 1, sim.seventy.q)
  .tea <- sample(rep(c(0,1), sim.better.s/2))
  .exp <- ifelse(sim.seventy.c, .tea, !.tea)
  sim.seventy.p[i] <- fisher.test(table(.tea, .exp), alternative = "gr")$p.value
}

@

\chapter{Guessing and Counting}
\label{sec:fisher}

\section*{Overview}

In this chapter, we introduce the notion of frequentist probability.
It's the probability of a certain event under the assumption of pure randomness.
Pure randomness is not a very precise technical term, but we hope it gives the right impression.
For example, frequentist probability of rolling six pips with a fair die is $1\div 6$ (or roughly 0.17) before you roll the die.
This also implies that the proportion of rolls of a fair die showing six pips in a very long or endless sequence of rolls will be $1\div 6=0.17$ (or 17\%).
Notice that after rolling the die, it either shows six pips or it doesn't show six pips, and there is no longer a probability attached to the result.
It becomes a fact.
We apply reasoning around frequentist probability to lotteries, parlour games with self-proclaimed psychics, tests of alleged tea gourmetism, and corpus studies.
It is shown that unexpected outcomes under the assumption of pure randomness are those that have a low frequentist probability.
In other words, those outcomes would be rare if we repeated the experiment over and over again, and if only pure randomness were in control of the outcomes (rather than some influencing factor, such as the manipulation of a die).
To quantify these notions and make some careful and limited scientific inferences based on this quantification, we introduce several mathematical concepts.
The \alert{binominal coefficient} is a way of calculating the number of ways of choosing a specific number of elements from a set of elements regardless of their order and without replacement.
The \alert{p-value calculated in Fisher's Exact Test} is a metric that reconstructs the pre-experiment frequentist probability of obtaining a certain result or a more extreme result in simple experiments involving two two-valued variables and counts of their instances.
The \alert{effect strength} for Fisher's Exact Test is a measure of how strongly two variables interact.
Finally, \alert{probabilty density functions} and \alert{cumulative distribution functions} are general functions to calculate (and plot) such probabilities.

Readers should be warned that this chapter is a tad narrative, slow, and maybe even repetitive, which is necessary to give readers an insight into the basic ideas of frequentist inference.
We consider this vital in order to remedy problems with frequentist practice (not with the frequentist philosophy).
Take your time and rest assured that we will up the pace in later chapters.
Also, we promise that we will never talk about tea again after this chapter.

\Problem{If there's Nothing Going On \ldots}{%
Let's consider three rather simple questions:
(i) You know that you aren't prescient, but you decide to play the lottery anyway.
How surprised would you be if you won the big prize?
(ii) You don't believe that your friend, who claims to be a psychic, actually has psychic abilities.
Nevertheless, you give them a chance and invite them to a party where they have to guess the phone numbers of all other guests.
How surprised would you be if they guessed the phone numbers of all your other guests correctly?
(iii) Given that most grammatical theories (which have something to say about passives) claim that the verb \textit{sleep} cannot be passivised at all or only under very marginal circumstances, how surprised would you be to find ten passives of \textit{sleep} in a corpus of English?
\alert{Please think thoroughly about your answers to these questions before continuing on.}}

\section{Unexpected Outcomes}\label{sec:unexpected}

Did you think about the questions from the Problem Statement?
Really?
Good.
Here's one possible discussion.
Most importantly, the questions from the Problem Statement cannot be answered properly in their given form, simply because there are significant data missing.
As for (i), the question doesn't specify what kind of lottery we're considering.
Is it a simple urn at a funfair from which you get to draw one out of a thousand lots, and only one of the thousand lots is a win?
Or is it the Eurojackpot, where you have to guess five numbers out of fifty (plus some additional single numbers) correctly to win the big prize?
Most likely, you either decided that you can't answer the question, or you answered it with respect to some specific type of lottery by way of example.
Maybe you even wondered whether the lottery is supposed to be fair or not.
However, when presented with this specific example, most people typically don't worry too much about how the lottery was conducted and whether it was fair.
At least with big national lotteries, they tend to put trust in there being sufficient oversight and the draw being---here it comes---properly random.
Most importantly (and disappointingly), they see no way to rigging the lottery in their own favour.
Considering the urn at the funfair, people likely assume that it's rigged anyway, but they don't care (at least in the Free World).%
\footnote{\textit{It doesnâ€™t get more American than this, my friend. Fatty foods, ugly decadence, rigged games.} (Murray Bauman, Episode~7 of Stranger Things 3)}

The scenario in (ii) is similar, and there's also relevant information missing.
You probably decided whether your degree of surprise would critically depend on the number of guests at the party and the number of digits phone numbers have.
In my youth, smaller German villages (like Twiste, located in the Twistetal district) still had three-digit phone numbers, for example.
If the local Twiste psychic only had to guess one such phone number, guessing that number correctly even without psychic powers would be much less awe-inspiring than guessing the ten-digit U.S. phone numbers of 28 guests at a party in NYC with the same accuracy, for example.
Furthermore---and most likely because it involves a psychic---this scenario makes people much more suspicious of whether and how it was ensured that the psychic didn't cheat.
Maybe they have a secret app that exploits a vulnerability in close-by mobile phones, and they simply read the numbers off of peoples' phones.
Maybe the party was announced in a group chat on some messenger app, and they tracked all the guests' numbers down in the app before the party.
Maybe the host or some other guest conspired with the psychic and gave them all the numbers, either as a practical joke or even because they want to get people to pay for the psychic's services (tracking down dead ancestors who lived as maids and servants at the court of Louis XIV of France).

Example (iii) is much more intricate and, in a way, boring, which is why it only intrigues linguists.
Some linguists would smirk at you and claim that they don't care about corpus examples because it was determined once and for all by a cherubic figure (who never laughs) that examples from corpora don't count for anything.
Some linguists, on the other hand, would take the ten sentences as conclusive evidence that whatever random modification to their theory they came up with is provably correct, or that somebody else's theory is provably incorrect.%
\footnote{\textit{Whenever I find even one example that contradicts a claim, I consider that claim refuted.} (an unnamed linguist, p.\,c.)\label{fn:jm}}
What were your thoughts?
We certainly hope that you don't belong to either of the aforementioned tribes of linguists and that you saw the parallels to the first two scenarios.
Above all, quantitative considerations play a role, among others:
How large is the corpus?
How often does \textit{sleep} occur in the corpus, regardless of its voice?
How many active and passive verbs occur in the corpus?
Also, the question of whether it was a fair draw is vastly more complicated than in the case of a lottery.
For example, is it a corpus of language produced by native speakers, children, L2 learners of English, frontier large language models, or even some cute language bot from 1998?
Does the composition matter considering your research question?
What exactly is your research question?
Finally, the underlying theory from which it allegedly follows that \textit{sleep} cannot be passivised needs further inspection.
Does it also exclude the figura etymologica for such unaccusative verbs?
After all, maybe all ten sentences are instances of silliness such as (\ref{ex:figetym}).
Would the result still count as unexpected, regardless of any quantitative evaluation?

\begin{exe}
  \ex The sleep of Evil has been slept by many a demon. \label{ex:figetym}
\end{exe}

It's a muddle!
Therefore, we'll use a simple non-linguistic example in Section~\ref{sec:tea} to introduce some important statistical concepts that concern the numerical side of this muddle.
The example is about tea, and it's extremely famous, so anyone interested in statistical inference should be aware of it, even if they're not in Tea Studies.
Then, we'll return to the questions from the problem statement (except question [ii], which is deferred to the exercises).

\section{Tea and Milk}\label{sec:tea}

\subsection{An Introduction to Unexpected Outcomes}

What unites the examples in the Problem Statement is that they describe a confrontation with chance.
Given this confrontation, you're asked \alert{what kind of a result would be unexpected under the assumption that there is nothing going on}:
(i) you're not prescient, or (ii) the psychic isn't actually a psychic.
Matters are more complex for the corpus example (iii), and we'll return to it later (for example in Section~\ref{sec:fisherexact}).
In this section, we formalise the notion of \Key{unexpected outcome} in relation to such situations and experiments.

First of all, an \textit{unexpected outcome} cannot be one which is deemed totally impossible.
If it were impossible to win the lottery, you wouldn't play it.
If it had already been established (for example at previous parties) that your psychic friend couldn't guess phone numbers, you wouldn't humiliate them and ask them to guess numbers at your party.
Finally, if it were established beyond reasonable doubt that \textit{sleep} cannot be passivised, you wouldn't bother to do a corpus search for passivised forms of that verb.
Clearly, unexpected outcomes are not miracles where everything we know about the world is up for debate.

What we usually mean when we deem an outcome \textit{unexpected} is that it had a very slim chance---a low probability---of occurring before we made it occur.
Mathematically, the most straightforward case is the one with the urn at the funfair.
If there are a thousand lots in the urn, one of them is a win, all the others are duds, and you draw exactly one, most people have an intuitive understanding that you have a \textit{chance of one in a thousand} (or 1:1000) to win.
Usually, it is also understood that this means that if you played this game over and over again, you would end up winning in one of a thousand rounds on average.
(Playing the game over and over again, each time with a fresh urn of one thousand lots, not gradually emptying one and the same urn, of course.)
That's why playing it once and winning is unexpected or surprising:
Winning is a rare event given the way the urn was set up (one winning lot and 999 duds).
The maths is slightly more complex for the Eurojackpot because you have to choose five numbers out of fifty and not one lot out of a thousand, but it essentially follows the same logic.
For the psychic guessing phone numbers, the idea is also the same once the number of phone numbers and the number of the digits per phone number has been determined.
We will return to the third scenario (the corpus study) later, but we can apply a similar logic even to that example.

\subsection{The Total Number of Possible Outcomes}\label{sec:numberoutcomes}

In each of the scenarios, we need to know the number of potential outcomes in order to quantify how unexpected a single specific outcome is.
The higher the number of overall possible outcomes, the more unexpected a specific outcome is.
A seminal application of this idea to scientific reasoning is reported in \citet{Fisher1935a}, and we'll introduce it here before applying the same reasoning to the scenarios from the Problem Statement.
In that book, Ronald A.~Fisher reports an event where Muriel Bristow, herself a scientist, claimed that she could taste whether the milk or the tea was poured into a cup first.
While it is not impossible that some physical properties of the mixed liquids differ depending on their order of being poured into the cup, some doubt was in order.
Therefore, Fisher devised an experiment to shed light on the substance of Bristow's claim.
She was presented with eight cups, four tea-first cups and four milk-first cups.
Otherwise, the cups were identical.
Her task in the experiment was to find the four tea-first cups merely by tasting.
Very much like winning a lottery after buying just a single lot, some outcomes of this experiment might surprise us by being relatively unexpected if Bristow didn't have the ability she claims to have.
We still wouldn't consider it proven beyond all doubt that she does indeed have the ability if that happened.
However, we'd at least not consider her claims of being a tea expert refuted if she guessed a surprising number of cups correctly.
The question is: What's a surprising number?
How many cups does she have to classify correctly for us to call it an unexpected outcome?

Statistics doesn't offer a final answer to this question.
However, it provides the maths upon which we need to base our answer.
Remember that Muriel Bristow has to choose four cups out of eight, and we first need to calculate how many distinct sets of fours cups out of eight she could potentially choose, without even considering whether she chose the right ones.
Let's do it.
In Figure~\ref{fig:cupsall}, we illustrate the 8 cups.%
\footnote{We switch to writing numerals using arabic numbers for clarity, even if that violates one or two style guides.}
While they would all look exactly the same in the real experiment, we've made it easier to follow the argument by showing the tea-first cups with steam and the milk-first ones without steam.
Furthermore, we've coloured the cups to make them identifiable individually.
There is one grey, one red, one blue, and one green cup for each of the conditions (milk-first or tea-first).
Again, in the real experiment, cups should have the exact same physical properties.

\begin{figure}
\begin{center}
  \begin{tabular}{P|c|c|c|c|c|c|c|c|c|}
  \cline{2-9}
  \textbf{Choices: 8} &
    \includegraphics[width=0.06\textwidth]{images/teab2}&
    \includegraphics[width=0.06\textwidth]{images/teab1}&
    \includegraphics[width=0.06\textwidth]{images/teaa1}&
    \includegraphics[width=0.06\textwidth]{images/teaa2}&
    \includegraphics[width=0.06\textwidth]{images/teab4}&
    \includegraphics[width=0.06\textwidth]{images/teaa3}&
    \includegraphics[width=0.06\textwidth]{images/teaa4}&
    \includegraphics[width=0.06\textwidth]{images/teab3}\\
  \cline{2-9}
  \textbf{Chosen: 0} &
    \includegraphics[width=0.06\textwidth]{images/teablank}&
    \includegraphics[width=0.06\textwidth]{images/teablank}&
    \includegraphics[width=0.06\textwidth]{images/teablank}&
    \includegraphics[width=0.06\textwidth]{images/teablank}&
    \includegraphics[width=0.06\textwidth]{images/teablank}&
    \includegraphics[width=0.06\textwidth]{images/teablank}&
    \includegraphics[width=0.06\textwidth]{images/teablank}&
    \includegraphics[width=0.06\textwidth]{images/teablank}\\
  \cline{2-9}
  \end{tabular}
\end{center}
\caption{Four tea-first cups (steaming) and four milk-first cups (not steaming) for Muriel Bristow to choose from; in the actual experiment, they'd all look exactly the same (without colours).}
\label{fig:cupsall}
\end{figure}

Before choosing her first cup, Ms Bristow obviously has 8 choices.
She could pick the red steaming cup, the grey steaming cup, the grey non-steaming cup, etc.
Figure~\ref{fig:cupselect1} shows the situation after an arbitrary first cup was chosen.

\begin{figure}
\begin{center}
\begin{tabular}{P|c|c|c|c|c|c|c|c|c|}
  \cline{2-9}
  \textbf{Choices: 7} &
    \includegraphics[width=0.06\textwidth]{images/teab2}&
    \includegraphics[width=0.06\textwidth]{images/teab1}&
    \includegraphics[width=0.06\textwidth]{images/teaa1}&
    \includegraphics[width=0.06\textwidth]{images/teaa2}&
    \includegraphics[width=0.06\textwidth]{images/teablank}&
    \includegraphics[width=0.06\textwidth]{images/teaa3}&
    \includegraphics[width=0.06\textwidth]{images/teaa4}&
    \includegraphics[width=0.06\textwidth]{images/teab3}\\
  \cline{2-9}
  \textbf{Chosen: 1} &
    \includegraphics[width=0.06\textwidth]{images/teablank}&
    \includegraphics[width=0.06\textwidth]{images/teablank}&
    \includegraphics[width=0.06\textwidth]{images/teablank}&
    \includegraphics[width=0.06\textwidth]{images/teablank}&
    \includegraphics[width=0.06\textwidth]{images/teab4}&
    \includegraphics[width=0.06\textwidth]{images/teablank}&
    \includegraphics[width=0.06\textwidth]{images/teablank}&
    \includegraphics[width=0.06\textwidth]{images/teablank}\\
  \cline{2-9}
  \end{tabular}
\end{center}
\caption{That's the first cup chosen! Choices left: 7.}
\label{fig:cupselect1}
\end{figure}

Before she continues on and picks the second cup, only 7 choices are still available.
Notably, \alert{for each of the 8 distinct choices she had in the beginning, she now has 7 distinct subsequent choices}.
In the illustration, she chose the blue steaming cup and has the other 7 cups still available.
Had she chosen the red steaming cup instead in the first step, she'd now be confronted with a different set of 7 options.
That means that after picking another cup (Figure~\ref{fig:cupselect2}), she has already decided on one specific choice from among $8\cdot 7=56$ possible choices.%
\footnote{This is not exactly true.
There's a catch to which we'll return presently.
Do you remember from secondary school maths what it is?\label{fn:catch}}
Put differently, she has taken 1 out of 56 possible decision paths to choose 2 out of 8 cups.

\begin{figure}
\begin{center}
\begin{tabular}{P|c|c|c|c|c|c|c|c|c|}
  \cline{2-9}
  \textbf{Choices: 6} &
    \includegraphics[width=0.06\textwidth]{images/teab2}&
    \includegraphics[width=0.06\textwidth]{images/teablank}&
    \includegraphics[width=0.06\textwidth]{images/teaa1}&
    \includegraphics[width=0.06\textwidth]{images/teaa2}&
    \includegraphics[width=0.06\textwidth]{images/teablank}&
    \includegraphics[width=0.06\textwidth]{images/teaa3}&
    \includegraphics[width=0.06\textwidth]{images/teaa4}&
    \includegraphics[width=0.06\textwidth]{images/teab3}\\
  \cline{2-9}
  \textbf{Chosen: 2} &
    \includegraphics[width=0.06\textwidth]{images/teablank}&
    \includegraphics[width=0.06\textwidth]{images/teab1}&
    \includegraphics[width=0.06\textwidth]{images/teablank}&
    \includegraphics[width=0.06\textwidth]{images/teablank}&
    \includegraphics[width=0.06\textwidth]{images/teab4}&
    \includegraphics[width=0.06\textwidth]{images/teablank}&
    \includegraphics[width=0.06\textwidth]{images/teablank}&
    \includegraphics[width=0.06\textwidth]{images/teablank}\\
  \cline{2-9}
  \end{tabular}
\end{center}
\caption{After another cup was chosen, there are now 6 choices left.}
\label{fig:cupselect2}
\end{figure}

The story goes on in a similar vein.
Let's assume she chooses the red steaming cup as her third pick, as in Figure~\ref{fig:cupselect3}.
Now, she has made 1 out of $8\cdot 7\cdot 6=336$ possible choices, since for each of the $8\cdot 7$ options left over after her previous decision, she had 6 distinct choices available.

\begin{figure}
\begin{center}
\begin{tabular}{P|c|c|c|c|c|c|c|c|c|}
  \cline{2-9}
  \textbf{Choices: 5} &
    \includegraphics[width=0.06\textwidth]{images/teablank}&
    \includegraphics[width=0.06\textwidth]{images/teablank}&
    \includegraphics[width=0.06\textwidth]{images/teaa1}&
    \includegraphics[width=0.06\textwidth]{images/teaa2}&
    \includegraphics[width=0.06\textwidth]{images/teablank}&
    \includegraphics[width=0.06\textwidth]{images/teaa3}&
    \includegraphics[width=0.06\textwidth]{images/teaa4}&
    \includegraphics[width=0.06\textwidth]{images/teab3}\\
  \cline{2-9}
  \textbf{Chosen: 3} &
    \includegraphics[width=0.06\textwidth]{images/teab2}&
    \includegraphics[width=0.06\textwidth]{images/teab1}&
    \includegraphics[width=0.06\textwidth]{images/teablank}&
    \includegraphics[width=0.06\textwidth]{images/teablank}&
    \includegraphics[width=0.06\textwidth]{images/teab4}&
    \includegraphics[width=0.06\textwidth]{images/teablank}&
    \includegraphics[width=0.06\textwidth]{images/teablank}&
    \includegraphics[width=0.06\textwidth]{images/teablank}\\
  \cline{2-9}
  \end{tabular}
\end{center}
\caption{As Ms Bristow picks another cup, we're down to 5 choices.}
\label{fig:cupselect3}
\end{figure}

To make things more interesting for later calculations, she now makes her first incorrect guess.
She picks the green non-steaming one, which is a milk-first cup.
She has now decided on 1 specific configuration from $8\cdot 7\cdot 6\cdot 5=1680$ possible configurations.
Or has she?
As we mentioned in Footnote~\ref{fn:catch}, there's a catch.

\begin{figure}
\begin{center}
\begin{tabular}{P|c|c|c|c|c|c|c|c|c|}
  \cline{2-9}
  \textbf{Not chosen: 4} &
    \includegraphics[width=0.06\textwidth]{images/teablank}&
    \includegraphics[width=0.06\textwidth]{images/teablank}&
    \includegraphics[width=0.06\textwidth]{images/teaa1}&
    \includegraphics[width=0.06\textwidth]{images/teaa2}&
    \includegraphics[width=0.06\textwidth]{images/teablank}&
    \includegraphics[width=0.06\textwidth]{images/teablank}&
    \includegraphics[width=0.06\textwidth]{images/teaa4}&
    \includegraphics[width=0.06\textwidth]{images/teab3}\\
  \cline{2-9}
  \textbf{Chosen: 4} &
    \includegraphics[width=0.06\textwidth]{images/teab2}&
    \includegraphics[width=0.06\textwidth]{images/teab1}&
    \includegraphics[width=0.06\textwidth]{images/teablank}&
    \includegraphics[width=0.06\textwidth]{images/teablank}&
    \includegraphics[width=0.06\textwidth]{images/teab4}&
    \includegraphics[width=0.06\textwidth]{images/teaa3}&
    \includegraphics[width=0.06\textwidth]{images/teablank}&
    \includegraphics[width=0.06\textwidth]{images/teablank}\\
  \cline{2-9}
  \end{tabular}
\end{center}
\caption{Ms Bristow has picked her last cup.}
\label{fig:cupselect4}
\end{figure}

First, she chose the blue steaming cup, then the grey steaming cup, then the red steaming cup, and finally the the green non-steaming cup.
But is this really the only way to arrive at the same result?
In the first step, she had 8 options, and she chose the blue steaming cup, leaving her with 7 choices, etc.
She could have chosen the red steaming one and still arrived at the same end result via a different path.
For example, she could have chosen the red steaming cup first, then the grey steaming cup, followed by the blue steaming one, and finally the green non-steaming one.
Put in quantitative terms, there are groups within the set of 1680 decision chains that yield identical results, at least if the order in which the cups were selected is irrelevant.
And for the purpose of this experiment, the order is indeed irrelevant.

How do we know how many of the 1680 decision paths lead to identical results?
Well, how many different ways of ordering 4 cups are there?
Imagine you had to put 4 cups on a table from left to right one by one.
In the first step, you can choose from among the 4 cups.
For each of these 4 distinct choices, there are 3 distinct subsequent choices because you'll have 3 cups left.
Then there are 2 choices each, then just 1.
Hence, the groups of identical outcomes should each have a size of $4\cdot 3\cdot 2\cdot 1=24$.
There are thus

\begin{center}\begin{math}
  \cfrac{8\cdot 7\cdot 6\cdot 5}{4\cdot 3\cdot 2\cdot 1}=\cfrac{1680}{24}=70
\end{math}\end{center}

\noindent truly distinct sets of 4 cups to be chosen from among a set of 8 cups.%
\footnote{And for those who are beginning to remember their elementary stochastics:
We get different results if the order is not irrelevant and if a cup can be chosen more than once (replacement).}

This just gives us the number of distinct sets of four cups we can choose from 8 cups.
So, how unexpected is her performance (3 cups detected correctly) given that there are 70 different ways of choosing 4 cups out of 8?
This would have been easier to quantify if she had guessed all 4 cups correctly.
Clearly, there is only 1 such immaculate result.
Had Ms Bristow chosen the green steaming cup instead of the green non-steaming cup, it would have been the immaculate guess.
Obviously, by merely guessing (without any special sensory ability), Muriel Bristow would produce such a perfect result in 1 out of 70 runs if the experiment were repeated over and over again.
Put differently, there is a 1:70 chance of guessing the four cups by mere luck.
In technical terms, the \Key{frequentist probability} of hitting the tea jackpot by uninformed guessing is $1\div 70\approx 0.014$.
This probability is sometimes converted to a percentage, in this case $1.4\%$.%
\footnote{From our perspective, this conversion to a percentage is not at all wrong inasmuch as $1.4\%$ of an endless sequence of tries would result in an immaculate result, even if the taster is really just guessing.
However, in scientific contexts, probabilities are expressed properly as numbers between 0 and 1, not as percentages.}
Would this be a highly unexpected result?
So unexpected maybe that you'd doubt that Muriel Bristow merely got lucky?
Well, you tell us!

\subsection{\Indepth\ Probability}\label{sec:probability}

Before we continue on to calculate the probability of outcomes that are not perfect---such as 3 correctly classified cups---let's stop and briefly revise our notion of \textit{probability} and informally introduce some terminology and some notation.
If this section is too technical for some readers, we encourage them to skip to Section~\ref{sec:lessthanperfect} and maybe return later.
Probability Theory is a complex field comprising formal aspects (the axiomatisation of the maths) as well as philosophical aspects (the interpretation of the maths).
There is neither just one unique axiomatisation nor one unique interpretation of probability.
We present one view (in a highly simplified form) that is useful for practitioners who want to use statistical inference in the analysis of empirical data.

We only contemplate situations that are \Key{experiments} or resemble experiments (such as a lottery).
A real and well-designed experiment is characterised by the fact that we know in advance what might happen.
For example, we just calculated in Section~\ref{sec:numberoutcomes} the number of possible outcomes in a classical Tea Tasting Experiment (which was 70).
That was possible because the protocol of the experiment was defined exactly, such that we could even exhaustively enumerate all 70 possible outcomes on one or two pages of paper.
This set of potential outcomes is called the \Key{sample space}, and it's an important property of a proper random experiment that the sample space must be defined (although not necessarily enumerable).
Thus, the sample space is a theoretical construct that is fixed before the experiment is conducted.

In a proper Tea Tasting Experiment, exactly 4 cups have to be chosen.
If more or less cups are chosen, the experiment wasn't actually conducted properly---it failed.
For the statistical analysis, selecting 4 cups counts as one single event or outcome.
Each of the 70 distinct events in the sample space $\{E_1, \dots, E_{70}\}$ is assigned a \Key{probability}.
Under the assumption that we're just guessing, none of these 70 outcomes is preferred, and we split the probability between them evenly, hence for each $E_i$, we get $Pr(E_i)=1\div 70\approx 0.014$, where $Pr(\cdot)$ is the \Key{probability function} that maps events onto their probability.
What this means is that if we repeated the experiment over and over again by guessing, each possible configuration of 4 cups would be selected equally often.
Hence, the notion of probability advocated in this book draws its interpretation from the concept of a hypothetically infinite sequence of repeated events.
While it's hypothetical, it's still quite useful and related to everyday experience.
It's the reason why you can't rely on a fair die to win any money in the long run.
There is no way to predict the number of pips because each number has the same probability.
No matter which number you bet on, you'll lose in $5\div 6=0.83$ or 83\% of all rolls.

Continuing on a slightly more technical note, the so-called \Key{event space} is the set of all possible subsets of the sample space.
It contains singleton sets $\{E_i\}$ but also any other subset such as $\{E_i, E_j\}$ or $\{E_i, E_j, E_k\}$ (for $i,j,k\in\{1..70\}$).
Crucially, the probability for each of these subsets must be determinable before the experiment.
For a singleton set such as $\{E_i\}$, the probability is just the probability of its single member.
In the Tea Tasting Experiment, it's $Pr(E_i)=0.014$.
The probability of a set such as $\{E_i, E_j\}$ is easy to determine.
What is the probability that \textit{either} $E_i$ \textit{or} $E_j$ will occur?
(Remember that $E_i$ and $E_j$ stand for one of 70 unique selections of 4 cups in the example.)
It's the sum of the individual probabilities, just as your chances of winning a lottery double if you buy 2 lots instead of 1.
Expressing \textit{or} for events as $\cup$ (the set intersection symbol), we get:

\begin{center}
\begin{math}
  Pr(E_i\cup E_j)=Pr(E_i)+Pr(E_j)
\end{math}
\end{center}

\noindent It's $0.028$ for any two outcomes of the Tea Tasting Experiment.
An essential task in frequentist statistics is to determine the probability of a specific set from the event space.
Immediately below, in Section~\ref{sec:lessthanperfect}, we'll determine the probability of the set containing all outcomes with three or more correctly chosen cups in order to determine how impressive Ms Bristow's performance were if she classified 3 cups correctly.

The sample space (often denoted $\Omega$), the event space (often denoted $\mathcal{F}$), and the probabilities assigned by the probability function $Pr$ to the sets contained in the event space are key ingredients of probability theory.
Furthermore, we introduced the notion of the probability of one \textit{or} ($\cup$) more events.
There is one final notion we need to introduce, namely the probability of two independent events to both occur.
The simplest example (once again) is a fair die.
If you roll it once, one of six possible events\slash outcomes will occur, and each outcome has the same probability.
For example, $Pr(\Dice{4})=1\div 6\approx 0.167$ (using the symbol \Dice{4} instead of some less intuitive $E_i$).
If you roll the die again, the first roll doesn't influence the second roll in any way, and the probability is once again $Pr(\Dice{4})=1\div 6\approx 0.167$.
However, what's the probability of any sequence of outcomes, such as \textit{4 pips} followed by \textit{3 pips}?
It's the probability of one event \textit{and} (written with the set union symbol $\cap$) another independent event: $\Dice{4}\cap\Dice{3}$, or more generally $E_i\cap E_j$.
It's calculated by multiplication:

\begin{center}
\begin{math}
  Pr(E_i\cap E_j)=Pr(E_i)\cdot Pr(E_j)
\end{math}
\end{center}

\noindent Hence:%
\footnote{It's by pure accident that we ended up with two different results in this section which are numerically identical at 0.028.}

\begin{center}
\begin{math}
  Pr(\Dice{4}\cap\Dice{3})=Pr(\Dice{4})\cdot Pr(\Dice{3})=\cfrac16\cdot\cfrac16=\cfrac1{36}=0.028
\end{math}
\end{center}

Above, we pointed out that from the view of probability theory and experiment design, the whole experiment is regarded as one event.
However, this event can often be divided up into a series of sub-events.
In the example, each act of choosing a cup is one such sub-event, which is how we introduced it in Section~\ref{sec:numberoutcomes}.
The maths for calculating probabilities of such sequences of events is exactly the same as for sequences of experiments as long as the sub-events are \Key{independent events}.\label{abs:independent}
Independent events are those which don't influence each other's probability.
If you roll a die twice, the number of pips you got in the first roll does not influence the second roll.
But a speaker of any natural language uses a certain lexeme in a sentence, the probability of the same lexeme being used in the next sentence increases sharply.
For example, a word such as \textit{antidisestablishmentarianism} has a shockingly low probability of ever occurring, even in British English.
Once it has been used in a sequence of sentences, however, the probability that it's used again in the next sentence might rise to the levels of words like \textit{technicality} or \textit{opposition}.
Clearly, the events are not independent as the first one influences the probabilities of the second one.
Please keep this in mind throughout the book (and your career in linguistics).

By the way, in all cases discussed so far the probabilities were evenly distributed among the possible outcomes, which is called a \Key{uniform random distribution}.
A fair die has a uniform random distribution of the number of pips, whereas a manipulated die doesn't.
A uniform random distribution is by no means a necessity nor particularly typical, and we'll encounter many cases where other probability distributions are applicable later in this book.
This concludes our very short and informal introduction to some rudimentary elements of probability theory.
When we add and multiply probabilities in the next section, you can verify that we're doing it according to the foundations laid out in this section.

\subsection{The Number of Some Less Than Perfect Outcomes}\label{sec:lessthanperfect}

Before proceeding to delicate matters of scientific inference, let's calculate the probability of guessing 3 cups correctly and getting 1 wrong, as in our example from Section~\ref{sec:numberoutcomes}.
To do that, we first introduce a convenient general notation for the maths of choosing $k$ items out of $n$.
First, notice that in the calculations above we often multiplied a natural number with the next smaller natural number, then the next smaller number, and so on.
For example, we calculated $4\cdot 3\cdot 2\cdot 1$.
Such an operation, where we multiply a natural number repeatedly with its next smaller neighbour until we reach 1, is called a \Key{factorial}, and it is expressed as $n!$ such that $4!=4\cdot 3\cdot 2\cdot 1$ if $n=4$.
To calculate the number of possible decision chains for 4 cups out of 8 cups, we calculated $8\cdot 7\cdot 6\cdot 5$, but then we didn't go down all the way to 1.
For two cups out of eight, we'd have calculated $8\cdot 7$ (two choices, then stop), etc.
In general and using $n$ as the variable encoding the number of items and $k$ as the variable encoding the number of items to choose, this can be expressed as

\begin{center}\begin{math}
  \cfrac{n!}{(n-k)!}
\end{math}\end{center}

\noindent To illustrate, we insert the concrete numbers from our example above:

\begin{center}\begin{math}
  \cfrac{8!}{(8-4)!}=\cfrac{8!}{4!}=\cfrac{8\cdot 7\cdot 6\cdot 5\cdot \stk{4}\cdot \stk{3}\cdot \stk{2}\cdot \stk{1}}{\stk{4}\cdot \stk{3}\cdot \stk{2}\cdot \stk{1}}=8\cdot 7\cdot 6\cdot 5
\end{math}\end{center}

\noindent To account for the decision paths that led to identical results, we divided this further by $4!$ because $k=4$ items can be arranged in $4\cdot 3\cdot 2\cdot 1$ ways.
Generally, we need to divide by $k!$
This gives us the \Key{binomial coefficient} used to calculate the number of distinct sets of $k$ items from $n$ items without replacement and irrespective of their order:

\begin{center}\begin{math}
  \dbinom{n}{k}\defeq\cfrac{n!}{(n-k)!k!} % \cfrac{\cfrac{n!}{(n-k)!}}{k!}=\cfrac{n!}{(n-k)!k!}
\end{math}\end{center}

\noindent The binomial coefficient is usually read \textit{n choose k}, but for us non-mathematicians it's okay to read it as \textit{k chosen from n}.
Since the factorial results in very large numbers even for relatively low input numbers, it is often practically infeasible to calculate the binomial coefficient using the above formula, and several alternative methods for calculating it are available.
They can be found on Wikipedia or in any text book teaching applied maths.

With this, we can now finally calculate how many ways there are of choosing 3 tea-first cups and 1 milk-first cup out of 8 cups in total where there are 4 tea-first and 4 milk-first cups.
This is easy if we regard the 8 cups as two sets of 4 cups (milk-first and tea-first).
Muriel Bristow thus chose 3 cups out of 4 correctly and 1 cup out of 4 incorrectly, hence:

\begin{equation}
  \dbinom43\dbinom41=\cfrac{4!}{1!3!}\cfrac{4!}{3!1!}=\cfrac{24}{6}\cdot\cfrac{24}{6}=4\cdot 4=16
  \label{eq:threefour}
\end{equation}

\noindent There are thus 16 distinct sets of 3 correct cups and 1 incorrect cup.
We already know that there are 70 ways of choosing any 4 cups out of 8, and hence the frequentist probability of achieving such a result by chance is:

\begin{equation}
  \frac{\dbinom44\dbinom40}{\dbinom84}=\cfrac{16}{70}\approx 0.23
  \label{eq:hypergeomex1}
\end{equation}

\noindent In the long run (repeating the experiment over and over again), even a random guesser would classify 3 cups correctly with a probability of 0.23, which corresponds to 23\% of all experiments.
So, are 3 correct guesses a highly unexpected result?
Not exactly, because in about a quarter ($23\%$) of an endless sequence of runs of such an experiment, anyone would reach this level of accuracy just by guessing.

There is one final amendment that we should make.
In order to evaluate how unexpected a result with three correctly chosen cups is, it is more informative to ask how often such a result \alert{or an even better result} would be when someone's just guessing.
Hence, we should add the number of possible configurations with four correct cups, which is 1:

\begin{center}\begin{math}
  \dbinom43\dbinom41+\dbinom44\dbinom40=4\cdot 4+1\cdot 1=16+1=17
\end{math}\end{center}

\noindent The probability of obtaining 3 or 4 correct cups by chance is thus:

\begin{equation}
  \frac{\dbinom43\dbinom41+\dbinom44\dbinom40}{\dbinom84}=\cfrac{17}{70}\approx 0.24
  \label{eq:hypergeomex2}
\end{equation}

But still, in light of this number, choosing 3 cups correctly wouldn't be an unexpected result at all.
It would provide no grounds whatsoever for rejecting the hypothesis that Muriel Bristow is just guessing.
Incidentally, she allegedly chose all four cups correctly when the experiment was actually conducted.
Do you find this impressive?

\subsection{Towards Making an Inference}

What does this statistical anecdote show?
We hope to have illustrated that there is a way of calculating how unexpected specific results of certain experiments are before the experiment is conducted and under the assumption that the experiment works essentially like a lottery:
The person does not have the ability in question, the effect predicted by a theory or a specific hypothesis doesn't exist, the results are completely random.
Then, if we run the experiment and the outcome happens to be one of the highly unexpected ones, our surprise might lead us to believe that the experiment wasn't just a lottery, but that there is something going on:
The person does indeed have the ability in question, the effect predicted by a theory or a specific hypothesis does indeed exist, etc.
In general, we'd assume that there is something going on.
We would be making a \Key{statistical inference} based on the outcome of the experiment.
Since inference is a process fraught with peril, much more will have to be said about it, for example in Section~\ref{sec:inferencefisher}.

However, a typical type of misunderstandings that comes with ill ways of phrasing the mathematical results of frequentist statistics must be mentioned immediately.
This is the perfect time to mention it because it sounds absurd to most people at this stage.
But once they've been taught statistics incorrectly, many people can't get this  misunderstanding out of their brains again.
The wrong story goes like this:

\begin{quote}
  \itshape
  \Argh Because guessing three or more cups correctly has a probability of only 0.24, the probability that Muriel Bristow has the ability to detect tea-first cups is $1-0.24=0.76$.
  We can be 76\% certain that she has the ability in question.
\end{quote}

This is all completely wrong.
You should find such interpretations surprisingly unwarranted if you followed our argument.
First, consider statements such as \textit{the probability that she has the ability in question is 0.76} and how nonsensical they are regardless of the specific number.
In reality, it's a fact that she either has the ability or she doesn't have it, and there's no probability attached to a fact.
Probability only plays a role when there is a confrontation with chance, and before the proverbial dice are rolled.
The probability we calculated above \alert{was} the probability of obtaining the result which we then actually obtained \alert{before we conducted the experiment} and \alert{under the assumption that there was no ability, effect, etc.\ at work but just pure randomness}.
We frequently call this hypothetical absence of an effect the \alert{assumption that there's nothing going on}.

Second, we'd like to ask you what it really means to be \textit{so-and-so percent certain of something}.
We consider this notion (sometimes also called a \textit{subjective probability} or similar) to be undefined and above all irrelevant to science.
Science is about how things really are and not about our subjective beliefs of how they are.
However, we suggest you also give those people a chance who think the complete opposite is true.
Google \textit{subjective Bayesianism}, read a few blogs, and then read \citet{Senn2011}.
However, everybody (including---we think---Bayesians of all kinds) agrees that the \alert{frequentist probabilities calculated in this book have nothing to do with \textit{being certain} of anything to a quantifiable degree}.
They serve to \alert{test scientific claims} in order to slowly establish them as unrefuted in a piecemeal fashion.
Please keep the following Big Point in mind when reading the next section, which is about inference.

\Bigpoint{Unexpected Outcomes}{%
The outcome of an experiment is unexpected if it had a low probability before the experiment was conducted under the assumption that there's nothing going on.
After that, the outcome is a fact and doesn't have a meaningful frequentist probability attached to it.
A low probability of a specific outcome merely means that it would be rare if the experiment were conducted very often.}

\section{Inference With Fisher's Exact Test}\label{sec:inferencefisher}

\subsection{P-Values and Sig-Levels}\label{sec:pandsig}

The frequentist probability that a certain experimental result had before it was actually obtained (when calculated after the experiment) is often called a \Key{p-value}.
In a Tea Tasting Experiment with 8 cups, 4 tea-first cups, and 3 correctly detected tea-first cups, we can calculate $p=0.24$ with Equation~\ref{eq:hypergeomex2}.
The p-value is often used to make automatic inferences along the following lines:

\begin{quote}
\itshape
  There were 8 cups in total, 4 of them tea-first cups.
  Ms Bristow agreed to choose 4 cups to demonstrate that she can successfully detect tea-first cups.
  She detected 3 tea-first cups correctly, hence $p=0.24$.
  Since this p-value is higher than the accepted \Key{significance threshold} of 0.05, we conclude that the experiment has produced no evidence that Ms Bristow has above-chance abilities to detect tea-first cups.
\end{quote}

\noindent The purpose of this section is to reconstruct and critique such inferences.%
\footnote{By the way, if you're wondering why the accepted threshold should be 0.05, we cannot help you.
It was a vague and loose suggestion by Ronald A.\ Fisher, and it developed a life of its own.
We don't recommend using it.}
However, please keep in mind that \textit{p-value} is just another name for what we calculated in Section~\ref{sec:lessthanperfect} with Equation~\ref{eq:hypergeomex2}.
The maths has been established, and this section is about its interpretation.

There are two important aspects with respect to making inferences from unexpected outcomes.
Can we be \textit{sure} that Muriel Bristow had the ability of discerning tea-first cups from milk-first cups just because in the real experiment roughly 100 years ago, she pointed out 4 correct cups?
The answer is clearly negative because, well, $p\approx 0.014$ and not $p=0$.
To illustrate, consider my grandfather on my father's side, Karl.
Karl used to play the German national lottery with a bunch of friends, and in 1962, they won the big prize.
(True story!)
They guessed 6 numbers out of 49 correctly.
Would you take their win as evidence that they were prescient and could foresee the number that would be drawn?
Even better, would you take it as evidence that extra-sensory perception (ESP) exists?
After all, the p-value is bloody low:

\begin{center}
\begin{math}
  p=\cfrac{1}{\dbinom{49}{6}}=\cfrac{1}{13,983,816}\approx 0.0000000715
\end{math}
\end{center}

Clearly, most readers would not make such an inference, regardless of how low the p-value is.
The interpretation of a statistical result needs to be well-informed about the mechanism that brought about the result, and it must be made with great care.
First of all, no known physical and\slash or cognitive mechanism that we know of could account for ESP, which is why most people don't even bother to run experiments investigating ESP.
Good hypotheses about why and how ESP should be real simply don't exist.
Also, notice that we did not, in fact, conduct an experiment about my grandfather's ESP abilities, but I merely told a story about him winning the lottery.
At some point I started calling it an experiment, probably unnoticed by most readers.
I could have told a similar true story about any random person I knew (a friend's girlfriend's uncle or whomever) if it so happened that that person won the lottery at some arbitrary point in the past instead of my grandfather.
If you allow yourself to look anywhere for evidence of something, you're bound to find it somewhere, and a low p-value becomes utterly meaningless.
Interestingly, Karl and his friends repeated the pseudo-experiment over and over again, playing the national lottery every week for roughly ten years in total (over five hundred draws) without ever winning any considerable amount of money again.
This really makes it look like they were just guessing numbers and got lucky exactly once.
Obviously, repeating experiments (so-called \Key{replication}) is a very good way of further testing inferences made based on unexpected outcomes.

Astonishingly, researchers in soft sciences are often satisfied if $p<0.05$ in order to proceed to a substantive inference from an experiment.
This is the ominous significance threshold mentioned at the beginning of this section.
Such thresholds are often called the $\alpha$ level, although we prefer \Key{sig-level} (for \textit{significance level}).
Setting $sig\defeq 0.05$ means that researchers are satisfied to make an inference if the outcome of an experiment would only be expected in 1 out of 20 experiments under the assumption that there is nothing going on, \ie, that there really isn't any effect.
While this may be justified in some cases, automatically assuming such a sig-level is ill-advised and outright insane.
We'll come back to this point again and again, but to show you that a chance of 1 out of 20 usually wouldn't give you much confidence when there is any important matter at stake, think about the old game show \textit{Let's Make a Deal}.
In that show, contestants regularly had to choose one door out of three, and there was a big prize behind one of the doors and duds called \textit{Zonk} (or at least much less impressive prizes) behind the other two.
Let's modify the rules slightly:
In the soft-sciences version of \textit{Let's Make a Deal} there are 20 doors.
Behind 19 of them, there are prizes worth a significant fortune (grant money, which means money plus prestige), but one of the doors hides an automated gun turret which instantly kills the contestant if they choose that door.
Would you expect any sane person to participate in such a game show?
Of course you wouldn't.
People who---given a choice---wouldn't take any substantial risk in real life with a 1 in 20 chance are happy to bet the future of linguistics or social psychology on such a chance by setting $sig\defeq 0.05$ (while not doing replication).
On the other hand, we have seen that even $p\approx 0.0000000715$ might be meaningless.
Clearly, doing maths is easy, but making good inferences isn't.
Therefore, two of the major themes in this book are (i) that you shouldn't take unnecessarily high risks in making scientific inferences from data and (ii) that there is no recipe-like procedure that leads to good inferences.

In the next section, we'll return to the corpus example from the Problem Statement and formalise the procedure described above in the form of Fisher's Exact Test or just Fisher Test.
You'll see that the logic of the Tea Tasting Experiment lies behind the omnipresent 2x2 tables that often pop up in the corpus literature, especially in research on collocations and collostructions \citep{StefanowitschGries2003,Evert2008}.

\subsection{Fisher's Exact Test and Null Hypotheses}\label{sec:fisherexact}

Our examples from the Problem Statement and the Tea Tasting Experiment are all about comparing counts of events.
How many cups of tea were chosen correctly relative to the number of possible events of choosing 4 cups of tea?
The comparison of these numbers allowed us to calculate the probability of an outcome as extreme as or more extreme than the actual outcome under the assumption that the process generating the guesses (for example, Muriel Bristow) is completely random.
Such counts are customarily summarised in \Key{contingency tables}, see Table~\ref{tab:contingency}.

<<contingency, echo=F, cache=T, out.width="85%", results="asis">>=
  contingency.cont <- matrix(c(3, 1, 4, 1, 3, 4, 4, 4, 8), nrow = 3)
  colnames(contingency.cont) <- c("Tea-First", "Milk-First", "Sum")
  rownames(contingency.cont) <- c("Tea-First", "Milk-First", "Sum")

  contingency.cont %>%
    as.data.frame.matrix %>%
    kable(align = c("c", "c"), booktabs = T,
          caption = "The contingency table for the outcome with 3 correctly classified cups",
          label = "contingency") %>%
    add_header_above(c(" ", "Reality"=2), bold = T) %>%
    group_rows(group_label = c("Bristow"), start_row = 1, end_row = 2) %>%
    column_spec(4, bold = T) %>%
    row_spec(0, bold=TRUE) %>%
    row_spec(3, bold = T)
@

A contingency table tabulates counts of items, acts, or events characterised by two \Key{variables}, each having two or more discrete possible values.
Here, one variable (called \textit{Reality} in Table~\ref{tab:contingency}) characterises cups as \alert{being a real tea-first or a milk-first cup}, and the other variable (called \textit{Bristow} in Table~\ref{tab:contingency}) characterises cups as \alert{designated by Ms Bristow as a tea-first or a milk-first cup}.
One variable is shown in columns, the other one in rows, and the table shows us how often which values of the two variables co-occur.
In row 1, we put the 4 tea-first cups according to Muriel Bristow.
In row 2, we put the 4 milk-first cups as classified by her.
The row sums in the last column reflect the fact that there were indeed 4 cups for each condition.
In the two columns, we count the real tea-first and milk-first cups.
As you can see, of the 4 real tea-first cups, Muriel Bristow classified 3 as tea-first (cell 1,1) and 1 as milk-first (cell 2,1).%
\footnote{In matrices and tables, it is customary to index cells first by rows, then by columns.
Hence, cell 1,1 is the upper-left cell. Cell 2,1 is the lower-left cell.
The upper-right and lower-right cells are indexed 1,2 and 2,2, respectively.}
The opposite is true for the real milk-first cups.

To calculate the p-value for Fisher's Exact Test---which is exactly what we've been introducing in this chapter---we only need to consider 6 of the 9 cells of the contingency table:
the shaded cells in Table~\ref{tab:fisherone}.
With those counts, we can calculate the probability of drawing 3 cups from a set of 4 and independently 1 cup from a different set of 4, which corresponds exactly to the binomial coefficient calculated in Equation~\ref{eq:threefour}.

<<fisherone, echo=F, cache=T, out.width="85%", results="asis">>=
  fisherone.cont <- matrix(c(3, 1, 4, 1, 3, 4, 4, 4, 8), nrow = 3)
  colnames(fisherone.cont) <- c("Tea-First", "Milk-First", "Sum")
  rownames(fisherone.cont) <- c("Tea-First", "Milk-First", "Sum")

  fisherone.cont %>%
    as.data.frame.matrix %>%
    kable(align = c("c", "c"), booktabs = T,
          caption = "The contingency table for the outcome with 3 correctly classified cups; the relevant cells are highlighted",
          label = "fisherone") %>%
    add_header_above(c(" ", "Reality"=2), bold = T) %>%
    group_rows(group_label = c("Bristow"), start_row = 1, end_row = 2) %>%
    column_spec(4, bold = T) %>%
    row_spec(0, bold=TRUE) %>%
    row_spec(1,
      background = c("white", the.lightcols[2], the.lightcols[1], "lightgray")) %>%
    row_spec(3, bold = T,
      background = c("white", the.lightcols[2], the.lightcols[1], "lightgray"))
@

As was shown above, we need to add the probability of obtaining a more extreme result, which is illustrated for completeness in Table~\ref{tab:fishertwo}.

<<fishertwo, echo=F, cache=T, out.width="85%", results="asis">>=
  fishertwo.cont <- matrix(c(4, 0, 4, 0, 4, 4, 4, 4, 8), nrow = 3)
  colnames(fishertwo.cont) <- c("Tea-First", "Milk-First", "Sum")
  rownames(fishertwo.cont) <- c("Tea-First", "Milk-First", "Sum")

  fishertwo.cont %>%
    as.data.frame.matrix %>%
    kable(align = c("c", "c"), booktabs = T,
          caption = "The contingency table for the \\textit{even more extreme result}",
          label = "fishertwo") %>%
    add_header_above(c(" ", "Reality"=2), bold = T) %>%
    group_rows(group_label = c("Bristow"), start_row = 1, end_row = 2) %>%
    column_spec(4, bold = T) %>%
    row_spec(0, bold=TRUE) %>%
    row_spec(1,
      background = c("white", the.lightcols[2], the.lightcols[1], "lightgray")) %>%
    row_spec(3, bold = T,
      background = c("white", the.lightcols[2], the.lightcols[1], "lightgray"))
@

Finally, we turn to the corpus example from the Problem Statement.
%\footnote{Be warned that people actually use the test like this, but that this is a slightly incorrect use.
%There will be ample discussion of this caveat throughout the book.}
As we pointed out in Section~\ref{sec:unexpected}, the Problem Statement mentions 10 passives of the verb \textit{sleep}, but we need more information.
Let's introduce that information and the argument that comes with it, roughly following the logic behind collostructional analysis.%
\footnote{Paradoxically, we consider collostructional analysis to be suboptimal as an illustration of Fisherian logic of inference.
However, as it used to be the most prominent use case of Fisher's Exact Test in linguistics, we use it to introduce the test and critique the specific use of it in the process.}
Assume that we drew 90 active sentences containing \textit{sleep} in addition to the 10 passives.
Furthermore, assume that the corpus contains 1100 sentences in total, 890 of them active sentences, 210 passive sentences.
The corresponding contingency table is shown in Table~\ref{tab:corpussleep}.

<<corpussleep, echo=F, cache=T, out.width="85%", results="asis">>=
  corpussleep.cont <- matrix(c(90, 800, 890, 10, 200, 210, 100, 1000, 1100), nrow = 3)
  colnames(corpussleep.cont) <- c("Active", "Passive", "Sum")
  rownames(corpussleep.cont) <- c("Sleep", "Other", "Sum")

  corpussleep.cont %>%
    as.data.frame.matrix %>%
    kable(align = c("c", "c"), booktabs = T,
          caption = "A contingency table approximately as found in collostructional analysis",
          label = "corpussleep") %>%
    add_header_above(c(" ", "Voice"=2), bold = T) %>%
    group_rows(group_label = c("Verb"), start_row = 1, end_row = 2) %>%
    column_spec(4, bold = T) %>%
    row_spec(0, bold=TRUE) %>%
    row_spec(1,
      background = c("white", the.lightcols[2], the.lightcols[1], "lightgray")) %>%
    row_spec(3, bold = T,
      background = c("white", the.lightcols[2], the.lightcols[1], "lightgray"))
@

Descriptively, it is the case that \textit{sleep} occurs less frequently in the passive than all other verbs.
Only $10\%$ of all occurrences of \textit{sleep} are passives, but $20\%$ of all other verbs are passives.
Using the maths introduced in this chapter, we can attempt to quantify how unexpected this result is under the assumption that there's nothing going on, and the story goes as follows.
If we drew 100 sentences randomly from this corpus, what would be the frequentist probability of drawing exactly 90 active sentences and 10 passive sentences, given the overall distribution of voice in the corpus?
Clearly, it would be:

\begin{center}\begin{math}
  \cfrac{\dbinom{890}{90}\dbinom{210}{10}}{\dbinom{1100}{100}}\approx\cfrac{6.573\cdot 10^{141}}{1.423\cdot 10^{144}}\approx 0.005
\end{math}\end{center}

\noindent This probability is intersting because \alert{we obtained the result by querying for all sentences containing \textit{sleep}, not by drawing random sentences}.
Based on this, we can now inch our way towards making an inference about \textit{sleep}.
Our theory states that there should be no or at least very few passives of verbs like \textit{sleep} (unaccusatives) compared to other verbs, many of which can be readily passivised (transitives, unergatives).
If \textit{sleep} behaved like the average verb, a sample of sentences containing it would be expected to resemble a random sample from the corpus with respect to the number of actives and passives.
The more extreme the distribution of verbal voice in the \textit{sleep} sample is, the more we are inclined to assume that \textit{sleep} does not behave like the rest of the verbs with respect to passivisation.
This is actually a similar logic as in the Tea Tasting Experiment.
An unexpected result under the assumption of mere guessing on Muriel Bristow's part is conceptually very similar to an unexpected result regarding the distribution of verbal voice in a corpus sample under the assumption that the sample is not in some way different from the rest of the corpus.
The Fisherian type of the frequentist logic of inference is based on this kind of argument:
Since it is very difficult to come up with quantitative evidence in favour of research hypotheses (see Chapter~\ref{sec:inference}), a \Key{Null Hypothesis} (or simply \textit{Null}, symbolised $H_0$) is constructed.
The Null states in some way that \alert{the effect predicted by the theory is absent}.
If the result obtained in the experiment has a very low pre-experiment probability under the assumption that the Null is true (\ie, it's an unexpected result), the experiment is assumed to lend some limited support for the theory (or rather for a minor hypothesis which is part of the theory).
The usual phrase is that \textit{the Null was rejected}.
Unexpected results are not in any way taken as proof of the theory or a part of the theory, and it should be obvious why.
First, we never know whether a rare (unexpected) event has occurred by chance, regardless of how unexpected it was \alert{before we conducted the experiment}.
Our calculations are based on the realisation that it is not at all impossible to obtain unexpected results by chance.
On the contrary, the p-value quantifies the probability of the actual result under a given Null (\ie, by chance) with perfect precision.
Second, take the aforementioned pseudo-experiment regarding my grandfather's ESP abilities with $p=0.0000000715$.
It's practically irrelevant how low the p-value is, most people will not take it as evidence that my grandfather or one of his friends could foresee the numbers drawn in next week's national lottery.
Thus, the strength of the evidence depends on many factors, among them the design of the experiment and the p-value.

Let's keep this in mind and complete the maths.
The value of $p=0.005$ calculated above is just the probability of obtaining exactly 10 passives and 90 actives under the informally stated \alert{$H_0$: The verb \textit{sleep} is passivised as often as all other verbs}.
Since any more extreme (\ie, even lower) number of passives would be at least as good evidence against the Null, we should include them.
Hence:

% sum(unlist(lapply(90:100, function(x) {(choose(890,x)*choose(210,100-x))/choose(1100,100)})))

\begin{center}\begin{math}
  p=\cfrac{\dbinom{890}{90}\dbinom{210}{10}}{\dbinom{1100}{100}}+\cfrac{\dbinom{890}{91}\dbinom{210}{9}}{\dbinom{1100}{100}}+\dots+\cfrac{\dbinom{890}{100}\dbinom{210}{0}}{\dbinom{1100}{100}}\approx 0.008
\end{math}\end{center}

\noindent This is indeed the p-value as calculated in Fisher's Exact Test.
It's not the only possible p-value, as we'll show immediately.

\subsubsection{The Direction of the Null Hypothesis}

Let's take stock.
We've shown that the Tea Tasting Experiment and other confrontations with chance can be analysed statistically with a very simple logic.
If Ms Bristow has no special tea-tasting abilities, she would still guess 3 or 4 cups correctly in 24\% of an infinite or at least very long sequence of trials.
If \textit{sleep} did indeed behave like the totality of all other verbs with respect to passivisation in the fictitious corpus described above, 100 instances of \textit{sleep}, randomly sampled from a corpus would still lead to just 10 or less passives in 0.8\% of an infinite or at least very long sequence of trials.
If my grandfather and his friends weren't prescient, they'd still have won the big prize in 0.00000715\%  of an infinite or at least very long sequence of draws.

In the case of the corpus study, we had in mind a specific substantive hypothesis generated by many theories of syntax and the lexicon, namely that unaccusative verbs cannot be passivised or at least are very rarely passivised.
Hence, we calculated the probability of obtaining a sample with as few or fewer passives of \textit{sleep} in order to weaken the Null and mildly support the family of hypotheses to which the substantive hypothesis belongs.%
\footnote{Please review Chapter~\ref{sec:inference} if the exact wording of this sentence is less than crystal clear to you.}
This means, however, that the Null couldn't have been: \textit{The verb `sleep' passivises as often as the totality of all other verbs.}
If this were the Null, then \alert{any strong deviation (of \textit{sleep}) from the overall distribution of passives would be sufficient to weaken it}.
We'd have to take a result with, for example, 90 passives and 10 actives of \textit{sleep} (\ie, a strong deviation in the other direction) as equally good evidence against the Null as the result that was actually obtained.
The test would be a \Key{two-sided test} because it would test for deviations to both sides at the same time.

Based on our substantive hypothesis, however, we were only interested in deviations into one direction, namely \alert{a lower relative number (proportion) of occurrences of \textit{sleep} in the passive voice compared to the overall proportion of active and passive voice in the corpus}.
Hence, the actual Null must have something like: \textit{The verb `sleep' passivises as often or less often than the totality of all other verbs verb.}
The test was a \Key{single-sided test}, in this case a right-sided test (see Section~\ref{sec:hypergeometric}).
If $q_{\textsc{passive}(\textit{sleep})}$ denotes the proportion of passives of \textit{sleep} and $q_{\textsc{passive}(\neg\textit{sleep})}$ denotes the proportion of passives of all the other verbs, then the formalisation of the Null is:

\begin{center}
\begin{math}
  H_0: q_{\textsc{passive}(\textit{sleep})}\geq q_{\textsc{passive}(\neg\textit{sleep})}
\end{math}
\end{center}

There might be other verbs for which the same theory predicts the exact opposite: that their passive forms occur more often than those of the average verb.
For example, \textit{arrest} could be such a verb, as it is not only transitive but also often used in reports of arrests with phrases like: \textit{A man was arrested by the police}.
In order to test a Null based on this substantive hypothesis, we have to reverse the relation between the proportions:

\begin{center}
\begin{math}
  H_0: q_{\textsc{passive}(\textit{arrest})}\leq q_{\textsc{passive}(\neg\textit{arrest})}
\end{math}
\end{center}

<<arrest, echo=F, cache=T, out.width="85%", results="asis">>=
  arrest.cont <- matrix(c(69, 800, 869, 31, 200, 231, 100, 1000, 1100), nrow = 3)
  colnames(arrest.cont) <- c("Active", "Passive", "Sum")
  rownames(arrest.cont) <- c("Arrest", "Other", "Sum")

  arrest.cont %>%
    as.data.frame.matrix %>%
    kable(align = c("c", "c"), booktabs = T,
          caption = "The fictitious result for the voice distribution of \\textit{arrest}",
          label = "arrest") %>%
    add_header_above(c(" ", "Voice"=2), bold = T) %>%
    group_rows(group_label = c("Verb"), start_row = 1, end_row = 2) %>%
    column_spec(4, bold = T) %>%
    row_spec(0, bold=TRUE) %>%
    row_spec(1,
      background = c("white", the.lightcols[2], the.lightcols[1], "lightgray")) %>%
    row_spec(3, bold = T,
      background = c("white", the.lightcols[2], the.lightcols[1], "lightgray"))
@

\noindent In this case, a result as convincing as the result for \textit{sleep} is described numerically in Table~\ref{tab:arrest}, assuming there were also exactly 100 instances of \textit{arrest} overall.
The p-value for Fisher's Exact Test can be calculated following the know well-established method as follows:

% sum(unlist(lapply(69:0, function (x) {(choose(869,x)*choose(231,100-x))/choose(1100,100)})))

\begin{center}\begin{math}
  p=\cfrac{\dbinom{869}{69}\dbinom{231}{31}}{\dbinom{1100}{100}}+\cfrac{\dbinom{869}{68}\dbinom{231}{32}}{\dbinom{1100}{100}}+\dots+\cfrac{\dbinom{869}{0}\dbinom{231}{100}}{\dbinom{1100}{100}}\approx 0.009
\end{math}\end{center}

\noindent Since we are limited by the corpus size of 1100 and the fixed sample size of 100, we cannot reach exactly 0.008.
However, 0.009 is close enough.
Such a result could be interpreted as weakening the Null which states that \textit{arrest} is passivised as often or more often than all other verbs.
While we can't argue that a precise substantive hypothesis was directly supported, at least a global negation of such a hypothesis failed the test.
The directionality involved in formulating the Null is obviously informed by the substantive hypothesis.
Hence, it wouldn't be correct to think that Fisher's testing philosophy completely ignores substantive hypotheses.
In Chapter~\ref{sec:powerseverity}, we'll turn to methods of directly supporting hypotheses within the Neyman-Pearson philosophy and Deborah Mayo's Severity approach to frequentist inference.

\section{Sample Size and Effect Strength}\label{sec:sampleeffectsize}

When we obtain a low p-value, we can assume that \alert{the Null is false or a rare event has occurred}.
The Null usually states that some effect is absent, that there is nothing going on beyond random chance.
We have already illustrated (using the soft-sciences version of \textit{Let's Make a Deal} and my grandfather's lottery win) that there cannot be a fixed threshold for what counts as rare.
In this section, we examine how the size of the sample and the p-value are related.
Then, we informally introduce the notion of the effect strength, to which we'll return repeatedly in later chapters, very crucially in Chapter~\ref{sec:powerseverity}.

Assume that Fisher and Ms Bristow, after fiercely discussing (as one should) the theoretical foundations of her potential tea-tasting abilities that might have shown in the first experiment, decided to repeat the experiment but with a much larger sample.
This time, she agrees to taste 80 cups of tea, 40 of which are tea-first.
After the Tea Tasting Marathon, Fisher counts the successes and failures and aggregates the numbers in Table~\ref{tab:teamarathon}.

<<teamarathon, echo=F, cache=T, out.width="85%", results="asis">>=
  teamarathon.cont <- matrix(c(30, 10, 40, 10, 30, 40, 40, 40, 80), nrow = 3)
  colnames(teamarathon.cont) <- c("Tea-First", "Milk-First", "Sum")
  rownames(teamarathon.cont) <- c("Tea-First", "Milk-First", "Sum")

  teamarathon.cont %>%
    as.data.frame.matrix %>%
    kable(align = c("c", "c"), booktabs = T,
          caption = "Contingency table of the Tea Tasting Marathon",
          label = "teamarathon") %>%
    add_header_above(c(" ", "Reality"=2), bold = T) %>%
    group_rows(group_label = c("Bristow"), start_row = 1, end_row = 2) %>%
    column_spec(4, bold = T) %>%
    row_spec(0, bold=TRUE) %>%
    row_spec(1,
      background = c("white", the.lightcols[2], the.lightcols[1], "lightgray")) %>%
    row_spec(3, bold = T,
      background = c("white", the.lightcols[2], the.lightcols[1], "lightgray"))
@

In this second experiment, the results are similar to those reported in Table~\ref{tab:fisherone} inasmuch as she guessed 75\% of the tea-first cups correctly.
However, if we calculate the pre-experiment frequentist probability of obtaining this result or an even better result without tea-tasting abilities by chance (\ie, the p-value), we get:

% fisher.test(matrix(c(3,1,1,3),2), alternative = "gr")
% fisher.test(matrix(c(30,10,10,30),2), alternative = "gr")

\begin{equation}
  p=\cfrac{\dbinom{40}{30}\dbinom{40}{10}}{\dbinom{80}{40}}+\cfrac{\dbinom{40}{31}\dbinom{40}{9}}{\dbinom{80}{40}}+\dots+\cfrac{\dbinom{40}{40}\dbinom{40}{0}}{\dbinom{80}{40}}\approx 7.44\cdot 10^{-6}
  \label{eq:tea803010}
\end{equation}

\noindent Do you recognise the exponential notation $7.44\cdot 10^{-6}$?
With a negative exponent of $-6$, it means that you have to move the point 6 digits to the left in the significand (the part before the multiplication sign), hence $7.44\cdot 10^{-6}=0.00000744$.
Although the rate of accuracy is the same, the p-value is much lower than the p-value corresponding to Table~\ref{tab:fisherone}, which was 0.24.
This should be an intuitively desirable result.
If she has the ability in question, this will show much better in a larger number of trials.
Once again, an urn serves as a good and even simpler example of this effect.
Let's consider a situation where there the suspicion arises that an urn at a funfair might be unfair inasmuch as it does not contain the advertised number of winning lots.
The showman claims there are 1000 lots in the urn, 300 of them winners, but you suspect that there are much less winners.
To test his claim that the urn contains 30\% winning lots, you agree to draw a sample of 10 lots.
Even if you ended up with 1 winning lot and 9 duds, it wouldn't be (again, intuitively) much evidence of anything because such a small sample is generally not very trustworthy.
If you drew a sample of 100 lots, however, and you ended up with 10 winners and 90 duds, the showman would be in trouble.%
\footnote{The overall topic of sample size and sample accuracy will be discussed in Chapter~\ref{sec:confidence}.}
In the same vein, the Tea Tasting Marathon can be compared with the small-sample Tea Tasting Experiment.
Since the p-value is interpreted as providing stronger evidence the smaller it is, it behaves as we'd intuitively think it should.

One cautionary note is in order.
An experiment is an attempt to find out something about the real world, to detect whether an effect is real.
For example, we're curious whether Ms Bristow really has the ability in question or whether the corpus frequencies of certain verbs' voices are in line with our theories (and hence whether our theories are in line with reality).
If we draw a larger sample, nothing changes about the real world.
We simply \alert{increase our chances of detecting an effect (indirectly, by rejecting the Null) if it is there}.
We have to keep in mind that the p-value obtained in the experiment is just a particular result of us conducting the experiment, which is in essence a confrontation with chance.
If there is no effect in the real world, \alert{each p-value has exactly the same chance of emerging as the result of our experiment before we actually conduct the experiment}, regardless of the sample size.%
\footnote{For this to be provably true, certain conditions need to hold, as we will discuss in Chapter~\ref{sec:confidence}.}
If this sounds weird and kind of wrong to you, please read on until Section~\ref{sec:pdist}.

Alas, there is another catch, and it's one which becomes ever more important when moving on to realistic experiments from made up and toy scenarios like the Tea Tasting Experiment.
Usually, we don't expect humans to be deterministic, and we don't expect human behaviour to be discrete.
Even if Ms Bristow could detect tea-first cups, she probably had a non-zero error rate.
In fact, this is why we need the type of analysis provided by Fisher's Exact Test in the first place.
If she claimed she had infallible perfect taste, we'd probably just test her with a single cup from time to time and wait for her first error, which would prove her claim wrong beyond doubt.
Anyone who misclassifies a single cup does not have truly perfect taste.
Clearly, such proof would be much more satisfying than merely rejecting a Null (thus providing weak evidence in favour of a hypothesis, etc.).
To everybody's chagrin, the world usually isn't as simple as that.
Similarly, even if \textit{sleep} is an unaccusative verb, we expect it to pop up in the passive voice from time to time, either because we consider grammar to be probabilistic (as in many usage-based approaches), or because we attribute such occurrences to performance effects (as in generative and formal approaches with a psycholinguistic interest in processing).
We don't take sides here because we don't have to.
In any case, both corpora and controlled experiments are based on human behaviour such as linguistic output or reactions to linguistic input.
This behaviour is never deterministic, and we cannot expect perfect results from humans.
If we could, and if unaccusativity blocked passives strictly, we could just look for one example of any unaccusative verb in the passive voice in order to prove the underlying theory wrong (see Footnote~\ref{fn:jm}).

Why is this important?
For the penultimate time, think about Ms Bristow.
If she could detect tea-first cups with 75\% accuracy, she had the ability of interest, except it wasn't perfect.
Also notice that the fictitious corpus study about passives was introduced from the beginning with caveats regarding marginal contexts where even unaccusative verbs could be passivised, etc.
We never assumed the results would be perfect in the sense that we'd never encounter \textit{sleep} in the passive voice.
Under many probabilistic\slash usage-based approaches, verbs are even regarded as unaccusative only to a certain degree, such that \textit{sleep} might less unaccusative than \textit{burst}, for example.
Nobody expects to never ever find any unaccusative verb in the passive voice among the sentences produced by real speakers, and nobody expects to get unanimous and perfect rejection of all unaccusative verbs in the passive voice in a rating experiment.
Therefore, the effect (\eg, unaccusativity leads to reduced passivisation) has a certain \Key{effect strength}.
The effect strength is a property of the real-world phenomenon we're trying to capture with our experiment.
It is crucially different from the sample size, which is merely a property of our experiment.
The sample size only affects our chances of measuring an effect that is real, but it doesn't make an effect real.
In other words, it doesn't change the distribution of the p-values.
Different effect strengths, on the other hand, correlate with different probabilities for the p-values.
Again, Section~\ref{sec:pdist} will shed some more light on this, but it's highly important to understand the difference right from the beginning.
A large effect is easier to detect per se.
The better Ms Bristow's tea-first detection, the easier it is to detect in any experiment \alert{because a higher effect strength increases the probability of low p-values}.
The more prototypical a verb's unaccusativity, the easier it is to recognise as an unaccusative verb in an experiment \alert{because a higher effect strength increases the probability of low p-values}.
A \alert{larger sample just makes it easier to detect an effect if it is real}.

Did we repeat the same statements several times in the previous paragraph?
That was intentional, and we're going to repeat those statements in different form in subsequent chapters.
Many practical introductions to statistics blur the distinction by simply stating that \textit{both a large sample size and a high effect strength make it easier to obtain a low p-value}.
Among other quite negative effects, this often gives the impression that it's legitimate to increase the sample size until we finally catch that pesky effect.
We agree with even the most fanatic Bayesian that this is beyond evil.%
\footnote{If you haven't guessed already, Bayesian statistics is a different approach to statistical inference.
We praise many Bayesians for mercilessly exposing bad frequentist practice, and we definitely encourage you to look at Bayesianism.
At the same time, we aren't convinced that it's the final solution to all problems of statistical inference.}
Only a true understanding of frequentist inference ensures that is applied correctly in everyday research.
Therefore, we now dive deeper into the nature of p-values.

\section{The Distribution of P-Values (With Simulations)}\label{sec:pdist}

Did you notice that we talked about \alert{the probability of obtaining a low p-value}?
We agree that it might sound confusing to speak of the probability of obtaining a p-value, because the p-value is something like a probability itself.
However, the p-value is merely a metric that tells us \alert{what the probability of obtaining the result (that was actually obtained) was before we actually obtained it under the assumption that Null is true}.
Argh!
This statistics thing is harder than expected.
A p-value clearly isn't an innocent probability itself.
If anything, it \alert{was a hypothetical probability before we rolled the proverbial die}.

To clarify this admittedly twisted notion, we resort to simulations.
There are introductions based on and devoted to simulations (see \citealt{VasishthBroe2011} or the more advanced \citealt{CarseyHarden2014}), but we'll keep the fundamentals short since we're not trying to teach you how to do simulations yourself.
Luckily, modern computers are powerful machines, and they have very good random number generators.%
\footnote{Here, \textit{good} means that they approach true randomness very well without reconstructible patterns in the sequence of generated numbers.}
Therefore, we can use them to simulate processes that generate data, where a \textit{process} can be anything from Ms Bristow labelling cups of tea to verbs occurring in the active and passive voice.
In the simulation, we have the luxury of controlling all numerical properties of the simulated real world.
For example, we could set up a simulation that behaves like a tea-first detecting lady with a 75\% accuracy rate.
Or we could set up a simulation of a corpus where all verbs have the same tendency to occur in the passive, or a corpus where \textit{sleep} has a tendency much below average to occur in the passive, but this tendency is even stronger for \textit{burst}, etc.
The simulation can spit out any amount of data generated according to our specifications.
By subsequently analysing those data with our statistical tests, it is possible to see how the tests perform in uncovering the true nature of the process generating the data, \ie, the true nature of the simulated reality.
While simulations cannot be used to find out much about the actual real world, there are two advantages to this approach in the evaluation of our statistical tools.
First, the data are virtually free.
Anyone who has ever conducted an experiment or a corpus study knows that empirical work is tedious and expensive.
It's much easier to generate simulated data with a few lines of code.
Second, we never know what reality is like in doing real empirical work.
On the contrary, we rely on our statistical methods and our knowledge of them when doing empirical work in order to make valid inferences and uncover what reality is like.
Therefore, real experiments are mostly useless in the testing and demonstration of statistical methods.
Instead, we can use simulations to test the properties of our statistical tools, and we can use them to demonstrate these properties in teaching statistics.
In this book, they are used exclusively for demonstration.

Let's simulate a completely untalented tea-taster who really just guesses tea-first and milk-first with equal probability.%
\footnote{When we present a simulation, all data are always generated by code with a random generator.
They are not hand-picked or manipulated in any way.
Random seed settings are used to make them reproducible.
The open-source code for the simulations is written in R, and it can be consulted as part of the Knitr sources for this book.}
Based on this uninformed guesser, we simulate a classic Tea Tasting Experiment with four tea-first and four milk-first cups.
Table~\ref{tab:teasim1} shows the raw results, and Table~\ref{tab:teasim1cont} shows the contingency table.

<<teasim1, echo=F, cache=T, out.width="85%", results="asis">>=
  teaexp1.raw <- rbind(ifelse(tea.exp==0, "Tea-First", "Milk-First"),
                       ifelse(tea0==0, "Tea-First", "Milk-First"))
  colnames(teaexp1.raw) <- paste0("Cup ", 1:8)
  rownames(teaexp1.raw) <- c("Reality", "Guess")

  teaexp1.raw %>%
    kable(align = rep("c", 8), booktabs = T,
          caption = "The simulated outcome of a Tea Tasting Experiment",
          label = "teasim1") %>%
    row_spec(0, bold=TRUE) %>%
    column_spec(1, bold=TRUE) %>%
    kable_styling(latex_options = "scale_down")
@

<<teasim1cont, echo=F, cache=T, out.width="85%", results="asis">>=
  teaexp1.cont <- table(tea0, tea.exp) %>%
    as.data.frame.matrix
  teaexp1.cont <- cbind(teaexp1.cont, apply(teaexp1.cont, 1, sum))
  colnames(teaexp1.cont) <- c("Tea-first", "Milk-first", "Sum")
  teaexp1.cont <- rbind(teaexp1.cont, apply(teaexp1.cont, 2, sum))
  rownames(teaexp1.cont) <- c("Tea-first", "Milk-first", "Sum")

  teaexp1.cont %>%
    as.data.frame.matrix %>%
    kable(align = c("c", "c"), booktabs = T,
          caption = "Contingency table for the above",
          label = "teasim1cont") %>%
    add_header_above(c(" ", "Reality"=2), bold = T) %>%
    group_rows(group_label = c("Guess"), start_row = 1, end_row = 2) %>%
    column_spec(4, bold = T) %>%
    row_spec(0, bold = T) %>%
    row_spec(3, bold = T)
@

\noindent In this single simulated experiment, the random guesser guessed 2 cups correctly and 6 incorrectly.
Since the appropriate test is single-sided, the p-value is different from the one corresponding to Table~\ref{tab:fisherone} although the results looks superficially similar.
It's $\Sexpr{round(fisher.test(table(tea0, tea.exp), alternative = "gr")$p.value, 3)}$.
Intuitively, many practitioners see such a result and think something like this:%
\footnote{Please don't ask how we know this.}

\begin{quote}
  \itshape
  \Argh Yay!
  I totally get this result!
  After all, the person is just guessing, which means that there is no effect.
  Hence, the p-value should be high, and we can't reject the Null---which is correct.
  I've finally wrapped my mind around this testing thing.
  I'm going to finish my thesis now and do some serious empirical work!
\end{quote}

\noindent Unfortunately, this is fundamentally incorrect, as we will demonstrate using simulations now.%
\footnote{If you're writing a thesis (or you're about to write one) involving statistical analyses of empirical data, and you don't immediately see why this is false, don't do it.
You're not ready yet.
Much worse, if you're advising such theses, and you still don't see the problem, it's time to stop and think.}

The true purpose of running simulations is not to run them once but to run them very often.
By looking at the results of a large number of so-called replications, we can check whether the claims made by the frequentist theory are true.
Let's run \Sexpr{sim.guess.n} simulations of the same experiment where the person is just guessing.
To make the properties of the test pop out more clearly, we also change the number of cups per experiment from 8 to \Sexpr{sim.guess.s}.
Since we cannot inspect the results for \Sexpr{sim.guess.n} times \Sexpr{sim.guess.s} cups individually, we aggregate them in a certain type of plot---a \Key{histogram}---in Figure~\ref{fig:hist}.

<<hist, fig.cap=paste0("Histogram of p-values of ", sim.guess.n, " simulated experiments with ", sim.guess.s, " cups where the participant is merely guessing"), echo=F, fig.pos="t", out.width="85%", cache=T>>=
  h <- hist(sim.guess.p, breaks = 10, xaxt = "n", xlab = "P-Values", main ="", border=F)
  text(h[["mids"]], h[["counts"]]-5, labels = h[["counts"]])
  axis(1, seq(0, 1, 0.1), seq(0, 1, 0.1))
@

A histogram shows how often certain values occur in a sample or a population.
In this case, the values are p-values (on the x-axis).
The height of the bars (y-axis) corresponds linearly to the number of p-values that lie in the interval delimited by the respective bar.
For example, p-values between 0 and 0.1 were obtained in \Sexpr{h[["counts"]][1]} simulated experiments.
P-Values between 0.1 and 0.2 were obtained in \Sexpr{h[["counts"]][2]} simulated experiments, and so on.
Some fluctuations aside, it looks like all p-values have the same probability if there is no effect.%
\footnote{Due to the nature of Fisher's Exact Test and the symmetry of the experiment design, there is some patterning in the p-values, which is hidden by the histogram in Figure~\ref{fig:hist}.
It's really not relevant at the moment, and our point is still absolutely valid.
In Chapter~\ref{sec:zandt}, even better examples will be shown.\label{fn:patterns}}

This is a perfectly expected result.
\alert{Under the Null, each p-value has the same chance of occurring as the result of an experiment.}
Please revise the calculations in Section~\ref{sec:tea}.
The whole point was to count the possible outcomes of the experiment.
(Remember how we got to the 70 different possible outcomes of Fisher's original experiment?)
If there's nothing going on, each of these outcomes has the same chance of occurring, which also means that each p-value gets the same chance.
It's explicitly not the case that we can expect high p-values if there is no effect.
This point is mostly neglected in introductions for practitioners, and it's one of the most fatally misunderstood points in applied frequentist statistics.
At least some texts state that \textit{high p-values should not be interpreted as evidence against the substantive hypothesis}---or---\textit{high p-values should not be interpreted as evidence for the Null}.
That's absolutely correct, but now you know why.
If the substantive hypothesis is false and the Null describes reality, then high, low, and medium p-values all have the same probability.
That's why only low p-values (if any) have an inferential interpretation.

\Bigpoint{Distribution of P-Values under the Null}{%
The null hypothesis states in some way, shape, or form that there is no effect.
If it is true, all p-values from 0 to 1 have the same probability.
This is the reason why high p-values have no inferential interpretation and must not be taken as evidence for the Null and\slash or against the substantive hypothesis.}

Let's see what happens with the p-values if the participant has the ability to guess tea-first cups with slightly better-than-chance accuracy.
Figure~\ref{fig:histbetter} shows the distributions of p-values for \Sexpr{sim.better.s} experiments with \Sexpr{sim.better.n} cups.
The different panels plot the p-values for simulated participants with a varying tea-first detection accuray.
In the upper-left panel, it's 51\%, in the upper-right 52\%, in the lower-left 55\%, and in the lower-right 70\%.

<<histbetter, fig.cap=paste0("Histograms of p-values of ", sim.better.n, " simulated experiments with ", sim.better.s, " cups where the participant has increasing prediction accuracy (increasing from upper-left to lower-right); all y-axes are scaled to 1000 for better comparability"), echo=F, fig.pos="t", out.width="85%", cache=T>>=
  par(mfrow=c(2,2))
  h <- hist(sim.fiftyone.p, breaks = seq(0,1,0.1), xaxt = "n", xlab = "P-Values",
            main = "51% Accuracy", ylim = c(0,1100), xlim= c(0,1), border=F)
  axis(1, seq(0, 1, 0.1), seq(0, 1, 0.1))

  h <- hist(sim.fiftytwo.p, breaks = seq(0,1,0.1), xaxt = "n", xlab = "P-Values",
            main = "52% Accuracy", ylim = c(0,1100), xlim= c(0,1), border=F)
  axis(1, seq(0, 1, 0.1), seq(0, 1, 0.1))

  h <- hist(sim.fiftyfive.p, breaks = seq(0,1,0.1), xaxt = "n", xlab = "P-Values",
            main = "55% Accuracy", ylim = c(0,1100), xlim= c(0,1), border=F)
  axis(1, seq(0, 1, 0.1), seq(0, 1, 0.1))

  h <- hist(sim.seventy.p, breaks = seq(0,1,0.1), xaxt = "n", xlab = "P-Values",
            main = "70% Accuracy", ylim = c(0,1100), xlim= c(0,1), border=F)
  axis(1, seq(0, 1, 0.1), seq(0, 1, 0.1))

  par(mfrow=c(1,1))
@

This is an expected result.
If the Null is not true, low p-values are obtained with a higher chance than high p-values.
However, we see that it's not irrelevant how well the participant performs the task, \ie, how large the effect is (see Section~\ref{sec:sampleeffectsize}).
Even at an accuracy of 1\% above chance (51\%), \Sexpr{length(which(sim.fiftyone.p<0.05))} of \Sexpr{sim.better.s} p-values are already lower than 0.05.
At an accuracy of 2\% above chance (52\%), it's already \Sexpr{length(which(sim.fiftytwo.p<0.05))} of \Sexpr{sim.better.s} p-values, and at 5\% above chance (55\%) it's an astonishing \Sexpr{length(which(sim.fiftyfive.p<0.05))}.
If we applied $\text{sig}\defeq 0.05$ in order to test whether there is an effect, we'd almost be guaranteed to \textit{reach significance} (which is what people call it when they reach their pre-set sig-level) at 55\% accuracy, albeit the tea-tasting abilities of the participant are middling (to say the least).
It's highly doubtful whether this would be good for anything except claiming victory or getting a paper published.
It almost certainly wouldn't advance our knowledge of the real world.

What we've just shown is that the effect strength tilts the distribution of p-values towards 0.
The stronger the effect is, the more pronounced the tilt.
Combined with a large sample, the distribution of the p-values collapses around 0 very easily, which means that mechanical inferences based on pre-set sig-levels are almost always successful, even if the effect is weak and barely above chance level.
However, don't misunderstand this logic.
Figure~\ref{fig:hist} showed that a large sample alone doesn't mean that we reach significance.
Then, Figure~\ref{fig:histbetter} showed that as soon as there is even the tiniest effect, a large sample has a very high chance of detecting it by allowing a rejection of the Null.%
\footnote{The usual caveats apply when we use phrases like \textit{rejecting the Null} or \textit{detecting an effect}.
Fisher's frequentist logic never delivers proof nor decisive evidence nor even straightforward support for the substantive hypothesis.}
This is because Fisher's Exact Test was designed for small samples such as 8 cups, 5 different types of barley, etc.
In analyses of large data sets like corpus data with $n=1000$, $n=100000$ (or even $n=200$) Fisher's Exact Test (and virtually all other frequentist tests applicable for the analysis of such data) have a much too high \Key{sensitivity}.
A test's sensitivity is its capability of rejecting the Null when there is an effect, and it will be a major topic in Chapter~\ref{sec:powerseverity}.
Also related is the introduction of the odds ratio in Chapter~\ref{sec:describing}, which serves as a measure of effect strength with Fisher's Exact Test.

Why go through all the trouble to learn about frequentist inference when it's no good after all?
(\dots\ we hear you scream.)
Well, we don't know why you came here.
Only you know what your research questions are, which sources of data you have available, what your previous expertise in data analysis an statistics is.
If you've just found out that the methods discussed in this book aren't for you and that you're wasting your time, more power to you!
However, please give us chance to convince you to read on.
First, we've never said that the methods aren't any good per se.
If you have to work with small samples out of necessity and you've come up with a good design for your study and you apply frequentist tests wisely, you will benefit from this book.
Second, frequentist tests are used everywhere, and your ability to understand and critique published research depends crucially on your deep knowledge of frequentist methods.
Unless you're one of very few researchers who have invested a lot of time in understanding statistics, this book might help you in ironing out some misunderstandings and sharpen your awareness of the omnipresent misuse of frequentist statistics.
Third, Section~\ref{sec:sampleeffectsize} and the present section make things look slightly worse than they are.
We had to crank up the sample size quite aggressively and hide some quirks of the test (see Footnote~\ref{fn:patterns}) in order to make our point.
This is because Fisher's Exact Test is extremely well-suited to introduce the logic of frequentist inference (the primary goal of this chapter), but it is not the best example when talking about the distribution of p-values and related issues.
Fourth, we will formalise notions like a test's sensitivity when introducing the Neyman-Pearson philosophy of frequentist inference in Chapter~\ref{sec:powerseverity}.
This will enable you to use frequentist tests much more wisely.
Fifth, we will always point out where tests fail or are unnecessary, and what you can do instead.

Our final remark in this section concerns such a case, namely a case where it's much better to use something other than a test or a p-value.
If you're not a corpus linguist or if you have no interest in collostructional analyses, you can skip this paragraph.
In Section~\ref{sec:fisherexact} we claimed that our example application of Fisher's Exact Test was similar to its use in research on collostructions.
That's not exactly true, and we urge you to consult papers like \citet{Gries2014a,Gries2015b,Gries2022}, maybe also \citet{SchmidKuechenhoff2013,KuechenhoffSchmid2015}.
You will discover that the design of such analyses is different from our toy example.
Most importantly, it was never the goal of collostructional analyses to arrive at scientific inferences via Fisher's logic of statistical inference.
It was an exploratory method from the beginning, never an inferential method.
The p-values were primarily used to rank lexemes, not to infer substantive hypotheses about individual lexemes.%
\footnote{There was some erroneous talk of \textit{significant co-lexemes} and similar things in the early papers.
We do not consider such quirks relevant as of today, especially as those papers are now twenty years old.}
While it was a bad decision to use p-values from Fisher's Exact Test in collostructional analyses, it is now widely recognised that measures of effect strength are much more appropriate for the purpose.
Also, possible discussions about the applicability of Fisher's Exact Test in situations where the marginal sums are not fixed (such as most corpus studies) are mostly moot.
Virtually all alternative tests are even more sensitive, and we've seen that Fisher's Exact Test is oversensitive, if anything.
With that clarification, we proceed to the very last section in this chapter, introducing the concept of probability distributions and their defining functions.

\section{\Indepth\ The Hypergeometric Distribution}\label{sec:hypergeometric}

In this chapter, we tried to introduce the notions of probability and inferences based on probabilities intuitively and by building the necessary maths step by step.
In probability theory, knowledge about random phenomena is customarily formulated in the form of probability density functions and their relatives.
Here is what they are:
In statistics, a variable is a measurement of some kind of property of events.
In the experiments described in this chapter, the variable was always a count of a two-valued underlying variable like \alert{success and failure} or actives and passives.
Other variables we'll encounter measure different things such as lengths or reaction times.
The whole purpose of statistics is to quantify the frequentist probabilities of the values of those variables.
For example:
What's the probability of guessing $k$ cups correctly?
What's the probability of encountering a sentence that is 6 words long?
What's the probability of measuring a reaction time of 350 ms or higher?

Section~\ref{sec:tea} was devoted to deriving the maths that specify the probability of obtaining $k=3$ successes (correctly detected tea-first cups) when there are $K=4$ potential successes (tea-first cups on the table) and we have $n=4$ choices to make (cups to choose) from $N=8$ potential choices in total (the overall number of cups).
This effort culminated in Equation~\ref{eq:hypergeomex2} on page~\pageref{eq:hypergeomex2}, repeated here for convenience:

\begin{center}\begin{math}
  \frac{\dbinom43\dbinom41}{\dbinom84}=\cfrac{16}{70}\approx 0.23
\end{math}\end{center}

\noindent The general form of this equation is (as you should verify):

\begin{equation}
  \frac{\dbinom{K}{k}\dbinom{N-K}{n-k}}{\dbinom{N}{n}}
  \label{eq:hypergeom}
\end{equation}

Now, think about the design of experiments like the Tea Tasting Experiment:
The three parameters $N$, $K$, and $n$ are fixed by the design, only the number of successes $k$ varies depending on the performance of the taster.
If we plot the probabilities (y-axis) of all possible outcomes of $k$ successes (x-axis) while keeping $K$, $N$, and $n$ fixed at certain values, we get the graph in Figure~\ref{fig:hypergeom844} (left panel).
We also provide the plot for the Tea Tasting Marathon from Section~\ref{sec:sampleeffectsize} (right panel).
Notice that the grey lines are merely visual helper lines.
As the distribution is discrete (see below), it does not define a curve.

<<hypergeom844,  fig.cap=paste0("Probability density for k successes in Tea Tasting Experiments; the density function of the Hypergeometric Distribution"), echo=F, fig.pos="t", out.width="75%", cache=T>>=
  par(mfrow=c(1,2))

  ks <- 0:4
  ps <- dhyper(ks, 4, 4, 4)
  plot(ks, ps, pch=20, bty="n",
       xlab = "k successes",
       ylab = "Probability",
       main = "N=8, K=4, and n=4",
       ylim = c(0, 0.6))
  lines(ks, ps, col="lightgray", lwd = the.lwd)
  points(ks, ps, pch=20, cex=2)

  ks <- 0:40
  ps <- dhyper(ks, 40, 40, 40)
  plot(ks, ps, pch=20, bty="n",
       xlab = "k successes",
       ylab = "Probability",
       main = "N=80, K=40, and n=40",
       ylim = c(0, 0.2))
  lines(ks, ps, col="lightgray", lwd = the.lwd)
  points(ks, ps, pch=20, cex=2)

  par(mfrow=c(1,1))
@

The two graphs in Figure~\ref{fig:hypergeom844} are examples of the \Key{Hypergeometric Distribution}, the probability distribution that's the basis of Fisher's Exact Test.
Since the outcomes $k$ are discrete counts, there is a defined probability for each outcome that can be calculated with arbitrary precision using the discrete \Key{density function} in Equation~\ref{eq:hypergeom}.%
\footnote{The term \textit{density function} is a bit unfortunate for thedistributions like the Hypergeometric Distribution as it applies more accurately to continuous distributions (see Chapter~\ref{sec:zandt}).
Hence, the adjective \textit{discrete} should not be omitted.}

Some terminological clarifications are in order.
We used the term \Key{parameter} above to describe $N$, $K$, and $n$.
Informally speaking, there is not one Hypergeometric Distribution but a large (infinite) class of distributions, one for each fixed permutation of $N$, $K$, and $n$.
Thus, the parameters define the concrete function and determine the exact shape of the curve for the admissible values of $k$, which is not itself a parameter.
Furthermore, this distribution is \Key{discrete} because it give probabilities for discrete outcomes.
After all, there cannot be an experiment where someone guesses 3.5 cups or anything like that correctly.
We'll encounter other very much different distributions later.

A necessary property of such functions is that the sum of the probabilities for all $k$ given a specific set of parameters is 1.
Intuitively, this should indeed be a necessary property.
For each outcome (a number of successes), the function specifies a probability, and we can be absolutely certain that exactly one outcome will occur if the experiment is conducted.
For example, we can only achieve between 0 and 4 correctly classified cups in a classical Tea Tasting Experiment if the experiment is terminated properly (and neither 5 nor 100 nor -1, etc.).%
\footnote{Notice that the distribution for given parameters $N$, $K$, and $n$ only describes properly terminated experiments set up according to those parameters.
If the cat jumps onto the table in the middle of the experiment and breaks all cups, the Hypergeometric Distribution is no longer applicable.}
The exact maths aside, as exactly one outcome has to occur, the probability of the union of the outcomes must be~1 (see Section~\ref{sec:probability}).

<<hypergeom844c, fig.cap=paste0("Cumulative distribution function for k successes in Tea Tasting Experiments; the distribution function of the Hypergeometric Distribution"), echo=F, fig.pos="t", out.width="75%", cache=T>>=
  par(mfrow=c(1,2))

  ks <- 0:4
  ps <- phyper(ks, 4, 4, 4)
  plot(ks, ps, pch=20, bty="n",
       xlab = "k successes",
       ylab = "Cumulative Probability",
       main = "N=8, K=4, and n=4",
       ylim = c(0, 1.1))
  lines(ks, ps, col="lightgray", lwd = the.lwd)
  points(ks, ps, pch=20, cex=2)

  ks <- 0:40
  ps <- phyper(ks, 40, 40, 40)
  plot(ks, ps, pch=20, bty="n",
       xlab = "k successes",
       ylab = "Cumulative Probability",
       main = "N=80, K=40, and n=40",
       ylim = c(0, 1.1))
  lines(ks, ps, col="lightgray", lwd = the.lwd)
  points(ks, ps, pch=20, cex=2)

  par(mfrow=c(1,1))
@

Finally, for each (discrete or non-discrete) density function there is a \Key{cumulative distribution function}.
It gives for each outcome $k$ the summed probabilities of all outcomes to its left and itself.
Figure~\ref{fig:hypergeom844c} shows the two distributions corresponding to the densities in Figure~\ref{fig:hypergeom844}.%
\footnote{Unfortunately, the maths of cumulative distribution functions are often much more complicated than that of the density function, and we do not provide their mathematical formulations.
They can be found easily on Wikipedia.}
The distribution function is highly useful in determining the \alert{probability of an outcome as extreme as or more extreme} for some $k$.
With Equation~\ref{eq:hypergeomex2}, we calculated the probability of achieving 3 correctly classified cups and 1 incorrectly classified cup, and the result (\Sexpr{round(phyper(1, 4, 4, 4), 2)}) is the value of the hypergeometric cumulative distribution function with parameters $N=8$, $K=4$, and $n=4$ at $k=1$.
Similarly, we calculated the probability of achieving 30 correctly and 10 incorrectly classified cups in the Tea Tasting Marathon with Equation~\ref{eq:tea803010} as $7.44\cdot 10^{-6}$.
This is the value of the Hypergeometric cumulative distribution function with parameters $N=80$, $K=40$, and $n=40$ at $k=10$.%
\footnote{Do you see why it is sufficient to look up the value at $k=1$ and $k=10$, respectively?
Hint: Contemplate the graphs in Figure~\ref{fig:hypergeom844c} and think about single-sided tests.}

We will demonstrate several such functions of probability distributions throughout the book.
While knowledge of these function is not strictly necessary in order to understand applied frequentist statistics, it might come in handy.
First, the proofs underlying frequentism (and Bayesianism, for that matter) make excessive reference to probability distributions.
It's therefore helpful for anyone who wants to dig deeper and study more sophisticated text books after this one.
Second, statistics programs such as R give us access to these functions, and it's much easier to use them with a minimal understanding of their workings.

