% !Rnw root = ../main.Rnw
<<setupconfidence, cache=T, echo=FALSE, include=FALSE, results='asis'>>=
opts_knit$set(self.contained=FALSE)

set.seed(524)

### Binary variable ###

rt.mean <- 120
rt.sd <- 20
rt.reps <- 100

rts <- function(n) {
  .t <- rnorm(n, rt.mean, rt.sd)
  mean(.t)
}

rt.sims <- list()
rt.ns <- c(1,2,3,4,5,10,20,50,100,1000,2000,5000,10000)

for (n in rt.ns) {
  .sample <- unname(unlist(lapply(1:rt.reps, function(x) {rts(n)})))
  rt.sims[[as.character(n)]] <- .sample
}

# Single sample for illu plot.
.normsample <- rnorm(100, rt.mean, rt.sd)

### Numeric variable ###

berry.prob <- 0.3
berry.reps <- 100

berries <- function(n) {
  .t <- rbinom(n, size = 1., prob = berry.prob)
  .t <- length(which(.t == 1))
  .t/n
}

berry.sims <- list()
berry.ns <- c(1,2,3,4,5,10,20,50,100,1000,2000,5000,10000)

for (n in berry.ns) {
  .sample <- unname(unlist(lapply(1:berry.reps, function(x) {berries(n)})))
  berry.sims[[as.character(n)]] <- .sample
}


@

\chapter{Estimation: Means and Proportions}
\label{sec:confidence}

\section*{Overview}

In this chapter, we introduce the error intervals (usually called confidence intervals, which is slightly misleading).
They're related to frequentist tests which we'll introduce in later chapters, but they serve a different purpose than tests.
We almost always measure means of numeric variables or proportions of nominal variables in samples.
However, we're almost always interested in the population or in properties of the process that generates the data.
Therefore, error intervals are a way of controlling the \textit{safety} and the \textit{precision} of estimating the population parameter from the sample value.

By using error intervals we acknowledge the fact that the sample mean or proportion is virtually never identical to the true parameter.
Therefore, we construct an interval around the value calculated from the sample.
By adjusting the width of the interval (using special maths) we can adjust the safety and the precision of the estimation.
A wider interval has a higher frequentist probability of actually containing the true value (high safety), but it's also less precise because it contains more possible values.
The only way to increase the precision while also increasing the safety of the estimation is to increase the sample size.
In all of this, it is vital to keep in mind what a frequentist probability is.
With error intervals, its primary (to some, its only) purpose is to control the long-run error rate of the estimation.

We begin by showing how the accuracy of sample estimates increases with the sample size and decreases with the variance in the data.
The related statistic is the \alert{standard error}, and we introduce the standard error for sample means and sample proportions.
The maths of error intervals is then shown to be based on standard errors.
We close with a short section on the density function of the Normal (or Gaussian) Distribution.

\Problem{How Reliable Is Your Sample?}{%
Have you ever tried Haribo Berries?
They come in packs containing two delicious flavours: raspberry and blackberry.
The two flavours are clearly distinct, and you should really prefer blackberry flavour.
However, many people feel that there are on average less blackberries than raspberries per pack.
Before complaining to Haribo, you decide to take a sample and buy a pack containing 100g.
You count them and find that there are 18 raspberries and 12 blackberries.
Hm.
Is this a precise estimate of the distribution of berries in the average pack?
You consider buying a 3kg pack and counting the 900 berries in it.
But how much better would the resulting estimate be compared to the 100g bag?
Would it be 30 times better?
What does this even mean?

Later that day (feeling a tad queasy because you've destroyed the evidence), you go back to ``the lab'' to do a self-paced reading experiment in order to find out the average per-word reading speed of adult Japanese speakers in a pre-study.
It's an exploratory study, and you don't have a hypothesis for the average.
Of the 30 invited participants, 4 show up, each reading 10 words.
Adjusted for word-length, it took them a mean 100ms per word.
A colleague from the theory department pops over for a coffee, looks at the results, and recommends increasing your sample size to get a better estimate.
She says you should have at least 100 participants read 1000 words each.
How much better would your estimate be?
Would it be 2500 times better?
What does this even mean?
}

\section{Sampling From a Population}

\subsection{Data Generating Processes}

Before we discuss sampling accuracy, we need to clarify what we mean by a population.
In Section~\ref{sec:populationssamples} we colloquially compared three populations: (i)~all galaxies in the Laniakea Supercluster, (ii)~all adult Tories in Buckinghamshire, and (iii)~all sentences of contemporary German.
Clearly, these are very different in type and count.
There are (according to Wikipedia) roughly 100000 galaxies in Laniakea.
The population in Buckinghamshire is roughly 850000, and it's a very conservative county.
Let's estimate 70\% of the population are Tory supporters.%
\footnote{According to the Buckinghamshire Council website, there are 105 conservative councillors, 15 are independent, 15 are liberal democrats, 6 belong to Labour, and 5 have other affiliations as of 25 February 2025.
That's approximately 70\% Tories.
However, given the British voting system, the council is a bad estimator of the composition of the electorate in terms of political affiliation.
(\url{https://buckinghamshire.moderngov.co.uk/mgMemberIndex.aspx})}
The population of Tory supporters in Buckinghamshire probably consists of some 400000 to 450000 humans (having subtracted 20\% of the total population to account for minors and apolitical people).
How many \textit{sentences of contemporary German} are there?
That's a much more difficult question.
What counts as German?
What counts as a sentence?
Do sentences spoken and written by L2 learners count?
How advanced do these L2 learners have to be?
Is CEFR level B2 good enough, or do we require C1?
These questions can't be answered in a statistics text book.%
\footnote{They should have been answered by linguists (at least those who rely on empirical data) before they started to do research on German per se (as opposed to research on the linguistic behaviour of samples of German-speaking subjects).
If you're a corpus linguist who believes that your corpus of choice \textit{represents} a population, you should have a very good answer to these questions.
In this respect, the most important property of works such as \citet{Biber1993} is that they're over 30 years old.}
What matters is that none of these populations is static.
Over the lifespan of the universe, the number of galaxies in Laniakea went from 0 to 100000, and it will return to 0.
Galaxies form or are drawn into superclusters all the time, and galaxies are torn apart and will eventually fall apart (in layman's terms) in the distant future.
People in Buckinghamshire are born and die, and people change their political affiliation all the time.
German sentences are produced at a breathtaking rate, and even the population of speakers of German changes every minute.
If the population really mattered as a fixed construct of a well-defined (even if unknown) size, we'd have to repeat all empirical work every day, second, or even millisecond.

While it's sometimes stressed that the population needs to be conceptuall infinite or at least significantly larger than the sample (for mathematical reasons), we see this rarely ever playing an important role in empirical science.
What's more, the whole idea of a fixed and huge population from which we draw a sample is not very helpful.
Why does any linguist draw a sample of German sentences?
In the post-structuralist era, it's not because we want to find out the properties of a massive collection of sentences but because we're interested in the mechanism that generates such sentences.
It doesn't matter which model of grammar a linguist believes in:
When they look at a sample of sentences they ask what is the grammar like that produces such sentences, be it a formal or a cognitive type of grammar.
Hence, we consider it much more appropriate and intuitive to speak of the \Key{data-generating process} (DGP) rather than the population, although we conveniently drop back to talking about a \textit{population} because it sometimes just sounds better in a sentence.
The data-generating process can be conceived of as cognitive, social, or even purely formal.
In fact, our simulations (see Section~\ref{sec:pdist}) simulate exactly such processes.
Since any scientifically interesting process can, in principle, generate ever new data points, the population of those data points is conceptually infinite.

In this chapter, we ask a very crucial question.
How reliable can we expect a sample to be in representing the DGP (or, in old-school terms, the population)?
Imagine English speakers were programmed (through cognitive constraints) to produce only 2\% passives of \textit{sleep}.
In this case, the true parameter with which the DGP was set up is 2\% (a proportion of 0.02).
How well can you expect to approximate this percentage\slash proportion with a sample of size $n=1$, $n=10$, $n=100$, and so on?
Do you see how this question is related to the logic of testing introduced in Chapter~\ref{sec:fisher}?
However, this chapter is not about \textit{testing} but rather about the \Key{estimation} a parameter of the population from a sample.
In order to get there, we need to talk about the distribution of results in repeated experiments in relation to true values of the DGP.
We'll use the results from this chapter to develop a general framework for inferential testing in Chapter~\ref{sec:zandt}.

\subsection{Sampling Berries: One, Two, Three, Many}\label{sec:samplinberries}

We begin with a simulation of a known situation.
As explained in Section~\ref{sec:pdist}, we can never do this in actual empirical work.
We do empirical research to find out what reality is like, precisely because we don't know what it's like.
However, simulating a known (even fictitious) reality allows us to explore and illustrate what happens in actual empirical work.
Hence, we now simulate a berry-generating Haribo process that produces blackberries at a rate of $\Sexpr{berry.prob}$ (or $\Sexpr{berry.prob*100}$\%).
In the long run, bags of Haribo Berries produced by this process will contain a proportion of $q\Sub{blackberry}=\Sexpr{berry.prob}$ on average.
That does not mean, however, that \textit{each} bag will contain \textit{exactly} $\Sexpr{berry.prob*100}$\% blackberries.
Also, we expect larger samples to better approximate the true proportion of blackberries.
We've been re-iterating this point and variations of it throughout the previous chapters.

The simulated process allows us to pretend that we have a machine that randomly fills bags with berries from a production line that produces $\Sexpr{berry.prob*100}$\% blackberries and $\Sexpr{(1-berry.prob)*100}$\% raspberries.
We call each simulated filling of such bag a \textit{simulation run} or a \Key{replication}.
In Figure~\ref{fig:berryone}, we plot the results for \Sexpr{berry.reps} replications at a sample size of $n=1$.
Each dot represents one bag containing a single berry filled by a machine that produces $\Sexpr{berry.prob*100}$\% blackberries by design.
We plot the proportion of blackberries in the sample.
As a sample of 1 berry contains either a blackberry or a raspberry, and there isn't an in-between option.
Hence, the proportion of blackberries can only be either 0 or 1 at $n=1$.

<<berryone, fig.cap=paste0("Proportions measured in 100 replications at n=1 with a true proportion of 0.3"), echo=F, fig.pos="t", cache=T>>=
  par.defaults()
  p <- plot(unlist(berry.sims["1"]),
            pch = 19, col = alpha("white", 0), cex = 1.5, ylim=c(0,1),
            xaxt = "n", bty = "n", ylab = "Proportion of Blackberries",
            xlab = "Individual Simulated Samples at n=1"
            )
  lines(c(0,100), rep(berry.prob, 2), col = the.cols[2], lwd = the.lwd)
  points(unlist(berry.sims["1"]),
            pch = 19, col = alpha(the.cols[1], 0.4), cex = 1.5)
  text(x = 70, y = 0.35, paste0("True proportion: ", berry.prob), col = the.cols[2])
@

We got \Sexpr{length(which(unlist(berry.sims["1"])==0))} samples of size $n=1$ containing no blackberry (proportion $q=0$) and \Sexpr{berry.reps-length(which(unlist(berry.sims["1"])==0))} containing one blackberry (proportion $q=1$).
The individual samples do the best they can to approximate the true proportion of $\Sexpr{berry.prob}$, but at $n=1$, the options are severely limited.
In this extreme case, however, the \textit{proportion of samples} where there was a blackberry (\Sexpr{(berry.reps-length(which(unlist(berry.sims["1"])==0)))/berry.reps}) approximates the true value reasonably well.

We now increase the sample size step by step.
At each increment, we simulate \Sexpr{berry.reps} replications and plot the proportion of blackberries per replication in Figure~\ref{fig:berrythreetoten}.

<<berrythreetoten, fig.cap=paste0("Proportions measured in 100 replications at n=3, n=4, n=5, and n=10 with a true proportion of 0.3"), echo=F, fig.pos="t", cache=T>>=
  par(mfrow=c(2,2), mar = c(2,2,2,2), family = "Plotfont")
  for (i in c(3,4,5,10)) {
    p <- plot(unlist(berry.sims[as.character(i)]),
              pch = 19, col = alpha("white", 0), cex = 1.5, ylim=c(0,1),
              xaxt = "n", bty = "n", ylab = "",
              xlab = "", main = paste0("n=", i)
              )
    lines(c(0,100), rep(berry.prob, 2), col = the.cols[2], lwd = the.lwd)
    points(unlist(berry.sims[as.character(i)]),
              pch = 19, col = alpha(the.cols[1], 0.4), cex = 1.5)
  }
 par.defaults()
@

\noindent We don't need to analyse these results numerically.
It should be immediately obvious that the samples approximate the true value more reliably.
With very small samples (up to $n=5$ in this example), results are still very limited in their possible outcomes, and it's impossible to hit the true proportion of \Sexpr{berry.prob} precisely.
But most of the samples drift towards the best approximation possible.
(Keep in mind that each dot represents one sample of the respective sample size.)
At $n=10$, the sample proportions start to form a clearly distinguishable cloud around the true value.
Let's see what happens with samples of more substantial sizes in Figure~\ref{fig:berrybig}.

<<berrybig, fig.cap=paste0("Proportions measured in 100 replications at n=20, n=50, n=100, and n=1000 with a true proportion of 0.3"), echo=F, fig.pos="t", cache=T>>=
  par(mfrow=c(2,2), mar = c(2,2,2,2), family = "Plotfont")
  for (i in c(20, 50, 100, 1000)) {
    p <- plot(unlist(berry.sims[as.character(i)]),
              pch = 19, col = alpha("white", 0), cex = 1.5, ylim=c(0,1),
              xaxt = "n", bty = "n", ylab = "",
              xlab = "", main = paste0("n=", i)
              )
    lines(c(0,100), rep(berry.prob, 2), col = the.cols[2], lwd = the.lwd)
    points(unlist(berry.sims[as.character(i)]),
              pch = 19, col = alpha(the.cols[1], 0.4), cex = 1.5)
  }
 par.defaults()
@

While it's not at all impossible to sample 0 or 1000 blackberries if the true proportion of blackberries is \Sexpr{berry.prob}, such a result becomes extremely rare event at this sample size.
We see in the lower-right panel of Figure~\ref{fig:berrybig} that even results lower than 0.2 or higher than 0.4 are so rare that none occurred in the $\Sexpr{berry.reps}$ replications.
For a binary (or nominal or ordinal) variable, larger samples approximate the true proportion more reliably.
In the next section, we try the same with a numeric variable.

\subsection{Sampling Milliseconds: One, Two, Three, Many}\label{sec:milli}

We now turn to the per-word reading speed from the Problem Statement.
Before we run the simulation, we have to set the parameters that define the simulated reality.
Let's assume that the real word-length-adjusted reading speed is at $\mu=\Sexpr{rt.mean}$ (in milliseconds) per word, and the standard deviation is $\sigma=\Sexpr{rt.sd}$.
With the simulations, we can now ask what will the outcomes be if we take many samples of size $n=1$, $n=2$, etc.
First take a look at Figure~\ref{fig:rts}.

<<rts, fig.cap=paste0("Raw data plot of a single random sample with n=100 from a DGP that has μ=", rt.mean, " and σ=", rt.sd), echo=F, fig.pos="t", cache=T>>=

 par.defaults()
# par(mfrow=c(2,1), mar = c(4,3,4,2))
#
# rt.minmax <- c(40, 200)
# rt.dnorm <- function(x) {dnorm(x, mean = rt.mean, sd = rt.sd)}
#
# plot(x = seq(rt.minmax[1], rt.minmax[2], 0.1),
#      y = rt.dnorm(seq(rt.minmax[1], rt.minmax[2], 0.1)),
#      type = "l", lwd = 0.5, col = the.darkgray, main = "Probability Density",
#      bty="n", xaxt="n", yaxt = "n", xlab = "Value (in ms)", ylab = "Density",
#      xlim = c(rt.minmax[1], rt.minmax[2])
# )
# axis(1, at = seq(rt.minmax[1], rt.minmax[2], 20))
# lines(x = c(rt.mean, rt.mean), y = c(0, rt.dnorm(rt.mean)),
#       lwd = the.lwd, col = the.lightcols[2])
# lines(x = c(rt.mean-rt.sd, rt.mean-rt.sd), y = c(0, rt.dnorm(rt.mean-rt.sd)),
#       lwd = the.lwd, col = the.lightcols[1])
# lines(x = c(rt.mean+rt.sd, rt.mean+rt.sd), y = c(0, rt.dnorm(rt.mean+rt.sd)),
#       lwd = the.lwd, col = the.lightcols[1])
# lines(x = seq(rt.minmax[1], rt.minmax[2], 0.1),
#      y = rt.dnorm(seq(rt.minmax[1], rt.minmax[2], 0.1)),
#      lwd = the.lwd, col = the.darkgray
#      )
#
# text(x = rt.mean+5, y = rt.dnorm(rt.mean)/2, labels = "μ", col = the.cols[2])
# text(x = (rt.mean-rt.sd)+7.5, y = rt.dnorm(rt.mean-rt.sd)/3, labels = "μ-σ",
#      col = the.cols[1])
# text(x = (rt.mean+rt.sd)+7.5, y = rt.dnorm(rt.mean+rt.sd)/3, labels = "μ+σ",
#      col = the.cols[1])

p <- plot(.normsample,
          pch = 19, col = alpha("white", 0), cex = 1.5, ylim=c(60,170),
          xaxt = "n", bty = "n",
          xlab = "Individual data points", main = "",
          ylab = "Value (in ms)"
          )
lines(c(0,100), rep(rt.mean, 2), col = the.cols[2], lwd = the.lwd)
points(.normsample,
          pch = 19, col = alpha(the.cols[1], 0.4), cex = 1.5)
@

It shows a single sample with $n=100$ generated by a DGP that is set to $\mu=\Sexpr{rt.mean}$ and $\sigma=\Sexpr{rt.sd}$.
The dots are single data points (\ie, one person reading one word).
Values close to $\mu=\Sexpr{rt.mean}$ have the highest probability (and hence the highest frequency in repeated sampling).
The average deviation of the values is at $\mu-\sigma=\Sexpr{rt.mean-rt.sd}$ and at $\mu+\sigma=\Sexpr{rt.mean+rt.sd}$.
Since this is just the average deviation, and we do not expect there to be a higher number of measurements around these two points, of course.
They're just the central tendency of the negative and positive deviations, respectively.
Figure~\ref{fig:rtsmall} shows what happens in \Sexpr{rt.reps} simulated samples at sizes of $n=1$, $n=3$, $n=5$, and $n=10$.

<<rtsmall, fig.cap=paste0("Means measured in 100 replications at n=1, n=3, n=5, and n=10 with a true mean of σ=120 and a true standard deviation of μ=20"), echo=F, fig.pos="t", cache=T>>=
  par(mfrow=c(2,2), mar = c(2,2,2,2), family = "Plotfont")
  for (i in c(1,3,5,10)) {
    p <- plot(unlist(rt.sims[as.character(i)]),
              pch = 19, col = alpha("white", 0), cex = 1.5, ylim=c(60,160),
              xaxt = "n", bty = "n", ylab = "",
              xlab = "", main = paste0("n=", i)
              )
    lines(c(0,100), rep(rt.mean, 2), col = the.cols[2], lwd = the.lwd)
    points(unlist(rt.sims[as.character(i)]),
              pch = 19, col = alpha(the.cols[1], 0.4), cex = 1.5)
  }
 par.defaults()
@

We see a very similar effect as in the case of the binary variable.
At a sample size of $n=1$, the mean is virtually identical to a single sampled value.
Therefore, the distribution of sample means in the upper left panel of Figure~\ref{fig:rtsmall} looks like a distribution of individual data points in Figure~\ref{fig:rts}.%
\footnote{Keep in mind: The dots in Figure~\ref{fig:rts} represent individual data points.
The dots in Figure~\ref{fig:rtsmall} represent individual sample means calculated from samples (of different sizes per panel).}
The larger the sample, the closer the sample means move (on average) towards the real value as the variance between single data points plays an ever smaller role.
This trend continues with much larger samples, see Figure~\ref{fig:rtsbig}.

<<rtsbig, fig.cap=paste0("Means measured in 100 replications at n=20, n=50, n=100, and n=1000 with a true mean of σ=120 and a true standard deviation of μ=20"), echo=F, fig.pos="t", cache=T>>=
  par(mfrow=c(2,2), mar = c(2,2,2,2), family = "Plotfont")
  for (i in c(20, 50, 100, 1000)) {
    p <- plot(unlist(rt.sims[as.character(i)]),
              pch = 19, col = alpha("white", 0), cex = 1.5, ylim=c(60,160),
              xaxt = "n", bty = "n", ylab = "",
              xlab = "", main = paste0("n=", i)
              )
    lines(c(0,100), rep(rt.mean, 2), col = the.cols[2], lwd = the.lwd)
    points(unlist(rt.sims[as.character(i)]),
              pch = 19, col = alpha(the.cols[1], 0.4), cex = 1.5)
  }
 par.defaults()
@

Why did we begin with samples of size $n=1$?
After all, it's a ridiculous sample size.
We did it because it allowed us to illustrate how a sample approximates parameters like the mean increasingly better than a single measurement.
A single value (equivalently a sample with $n=1$) approximates a parameter of the DGP with a certain reliability, and the reliability is higher the lower the variance in the DGP is.
The variance determines how far single data points bounce around the actual parameter, so with larger variance comes higher uncertainty.
However, the more such single data points you have in your sample, the more the bouncing around averages out.
It's still possible to draw a very extreme sample, but the probability of drawing such a sample gets lower and lower with larger samples.

We hope that our illustration was intuitive enough and laid the foundations for the statistics introduced in Section~\ref{sec:variancesamplemeans} and \ref{sec:variabilitysampleproportions}.
The way we introduced the idea of sampling variation---we hope---has made it clear that it's a frequentist concept.
If we took a lot of samples (100 in the examples above), then we'd see the patterns that we saw in the plots.
However, nothing can be known for certain or with a quantifiable reliability about the DGB from a single sample.
Please keep this in mind.
The Statistics Wars were fought about misunderstandings of this and other simple facts (see Chapter~\ref{sec:powerseverity}).

\Bigpoint{Sample Size in Parameter Estimation}{%
When we use a sample to estimate a population parameter, we can rely on some simple facts about variance and sample size.
A larger variance in the data-generating process (DGP) makes estimates of their mean less accurate.
On the other hand, larger samples average over the variance that makes the individual data points bounce around the true parameter.
Hence, parameter estimates from larger samples have a better chance of estimating the true parameter more accurately.
However, no amount of quantifiable certainty about the true parameter can ever be gained from a sample!
}

\section{Error Intervals for Normal Numeric Measurements}
% \section{Variance of Sample Means}
\label{sec:variancesamplemeans}

\subsection{The Variance of Sample Means}

Now we know that each increment in sample size counteracts the variance that's inherent in the data points.
In other words, repeated (not individual!) samples have the same or less variance than the raw data points.
They only have the same variance (under repeated sampling) if $n=1$.
Even with $n=2$, the variance already decreases (on average).
As it happens, we can express this mathematically and at the same time very intuitively for means like so:

\begin{center}
  \begin{math}
    \text{variance of sample means at sample size }n=\cfrac{\sigma^2}{n}
  \end{math}
\end{center}

The variance of the sample means is the variance of the individual measurements $\sigma^2$ divided by the sample size $n$.
Hence, when the variance among the individual measurements goes up, the variance among the sample means goes up.
On the other hand, when the sample size goes up, the variance among the sample means goes down.%
\footnote{Notice that, from this angle, we can answer \textit{yes} to the questions from the Problem Statement asking something like:
\textit{Would a sample 10 times larger be 10 times more accurate?}}
In order to get the \textit{average deviation of sample means from the true mean at sample size $n$}, we need to take the square root.
In Section~\ref{sec:quantifyvariance}, we did the same to calculate the standard deviation from the variance.
The result is called the \Key{standard error} of the mean $\ssigma_{\mu}$.
It's simply the standard deviation of sample means at a given sample size $n$ and given the variance among the individual data points $\sigma^2$.
The formula in Equation~\ref{eq:semean} is often expressed by the equivalent Equation~\ref{eq:semeanalt}, but we consider the first variant more transparent and easier to memorise.

\begin{equation}
   \ssigma_{\mu}=\sqrt{\cfrac{\sigma^2}{n}}
   \label{eq:semean}
\end{equation}

\begin{equation}
   \ssigma_{\mu}=\cfrac{\sigma}{\sqrt{n}}
   \label{eq:semeanalt}
\end{equation}

For example, the fictitious distribution of reading times with $\mu=120$ and $\sigma=20$ has the following standard error $\ssigma_{\mu}$ for samples of size $n=16$:

\begin{center}
  \begin{math}
    \ssigma_{\mu}=\frac{20}{\sqrt{16}}=\frac{20}{4}=5
  \end{math}
\end{center}

We have omitted one important detail.
The above is only perfectly true for data that follow the \Key{Normal Distribution} or \Key{Gaussian Distribution}.
A normally distributed random variable has a characteristic probability density function.
It has two parameters: the mean $\mu$ and the standard deviation $\sigma$.
Figure~\ref{fig:normal} shows a plot of the normal probability density with $\mu=0$ and $\sigma=1$.

<<normal, fig.cap=paste0("Theoretical population distribution for a normal distribution with μ=0 and σ=1"), echo=F, fig.pos="t", cache=T>>=

par.defaults()
normal.mu <- 0
normal.sigma <- 1
normal.minmax <- c(-3, 3)
normal.dnorm <- function(x) {
  dnorm(x, mean = normal.mu, sd = normal.sigma)
}

plot(x = seq(normal.minmax[1], normal.minmax[2], 0.1),
     y = normal.dnorm(seq(normal.minmax[1], normal.minmax[2], 0.1)),
     type = "l", lwd = the.lwd,
     bty="n", ylab = "Probability Density", xlab = "Value (x)",
     xlim = normal.minmax, col = "white"
)
lines(x = c(normal.mu, normal.mu), y = c(0, normal.dnorm(normal.mu)),
      lwd = the.lwd, col = the.cols[2])
lines(x = c(normal.mu-normal.sigma, normal.mu-normal.sigma), y = c(0, normal.dnorm(normal.mu-normal.sigma)),
      lwd = the.lwd, col = the.cols[1])
lines(x = c(normal.mu+normal.sigma, normal.mu+normal.sigma), y = c(0, normal.dnorm(normal.mu+normal.sigma)),
      lwd = the.lwd, col = the.cols[1])
lines(x = seq(normal.minmax[1], normal.minmax[2], 0.1),
     y = normal.dnorm(seq(normal.minmax[1], normal.minmax[2], 0.1)),
     type = "l", lwd = the.lwd, col = the.darkgray
)
text(x = normal.mu+0.2, y = normal.dnorm(normal.mu)/2, labels = "μ", col = the.cols[2])
text(x = (normal.mu-normal.sigma)+0.3, y = normal.dnorm(normal.mu-normal.sigma)/3, labels = "μ-σ",
     col = the.cols[1])
text(x = (normal.mu+normal.sigma)+0.3, y = normal.dnorm(normal.mu+normal.sigma)/3, labels = "μ+σ",
     col = the.cols[1])
@

You've probably seen the Normal Distribution before with the characteristic bell-shaped curve of its density function.%
\footnote{See Section~\ref{sec:gaussian} for a slightly more in-depth look at the Normal Distribution.}
Not only do many numeric measurements in nature follow this distribution, it also pops up in an almost creepy way in a family of fundamental proofs in statistics, the so-called \textit{Central Limit Theorem} (which has a lot to do with what we're in the middle of introducing at the moment).
Be that as it may, for samples from normally distributed data, the simple formula for the standard error of the mean given above is provably correct.
Notice that the density curve is fully defined by the mean and the standard deviation (see also the in-depth Section~\ref{sec:gaussian}).
This means that we can calculate the probability for each value of $x$ or each interval of $x$.
For example, we can calculate the probability of $x\ge 1$, written: $Pr(x\ge 1)$.
As the area under any probability density curve is 1, $Pr(x\ge 1)$ is the integral of the normal density from 1 to infinity.
Piece of cake.
Let's have that piece of cake now (step by step) and call it the Error Interval.

<<rtsnormal, fig.cap=paste0("Density of a normal distribution with μ=", rt.mean, " and σ=", rt.sd), echo=F, fig.pos="t", cache=T>>=

par.defaults()
rt.minmax <- c(60, 180)
rts.dnorm <- function(x) {
  dnorm(x, mean = rt.mean, sd = rt.sd)
}
rts.stderr <- function(x, n) {
  dnorm(x, mean = rt.mean, sd = rt.sd/sqrt(n))
}

plot(x = seq(rt.minmax[1], rt.minmax[2], 0.1), ylim = c(0, 0.1),
     y = rts.dnorm(seq(rt.minmax[1], rt.minmax[2], 0.1)),
     type = "l", lwd = the.lwd, yaxt="n",
     bty="n", ylab = "", xlab = "Value (x)",
     xlim = rt.minmax, col = "white"
)
lines(x = seq(rt.minmax[1], rt.minmax[2], 0.1),
     y = rts.dnorm(seq(rt.minmax[1], rt.minmax[2], 0.1)),
     type = "l", lwd = the.lwd, col = the.darkgray
)

# StdErrs

.c <- 0
for (n in c(2, 10, 20)) {
  lines(x = seq(rt.minmax[1], rt.minmax[2], 0.1),
       y = rts.stderr(seq(rt.minmax[1], rt.minmax[2], 0.1), n),
       type = "l", lwd = the.lwd, col = darken(the.lightcols[1], .c)
  )
  .c <- .c + 0.2
}

legend("topleft", legend = c("Data", "Means at n=2", "Means at n=10", "Means at n=20"), lwd = the.lwd, bty = "n",
       col = c(the.darkgray, the.lightcols[1], darken(the.lightcols[1], 0.2), darken(the.lightcols[1], 0.4))
)
@

First, Figure~\ref{fig:rtsnormal} shows the theoretical normal density plot of the reading times in milliseconds as introduced in Section~\ref{sec:milli}.%
\footnote{In density plots, we omit y axis labels whenever possible as they are rarely interpreted directly.}
We've added three density functions for the distributions of means at sample sizes 2, 10, and 20.
With larger samples, the probability mass piles up around the mean very quickly, and the normal curve narrows sharply.
This graph illustrates again what we've shown before:
The sample means are centred around the true mean, and they become better and better estimators of the mean with increasing sample size.

Let's focus on one of those curves, the density function of the means at $n=10$.
It's shown in Figure~\ref{fig:meandensten}.
In addition, the centre area under the curve that spans from the mean $1SE$ to the left and to the right ($\mu\pm 1SE$) is shaded in blue.
The are covering $\mu\pm 2SE$ is shaded in green.
As the Normal Distribution has known mathematical properties and we know the real mean and the real standard deviation, we can calculate the size of the shaded areas.
Well, \textit{we} here stands for \textit{fully qualified statisticians}, and for the purpose of this book, we just tell you some useful values (see also Tables below).
The blue area is approximately $\Sexpr{round(pnorm(1, 0, 1, T)-pnorm(-1, 0, 1, T), 2)}$, \ie, $\Sexpr{round((pnorm(1, 0, 1, T)-pnorm(-1, 0, 1, T))*100, 0)}$\% of the whole area under the curve.
The green area (plus the blue area) is approximately $\Sexpr{round(pnorm(2, 0, 1, T)-pnorm(-2.01, 0, 1, T), 2)}$, \ie, $\Sexpr{round(pnorm(2, 0, 1, T)-pnorm(-2.01, 0, 1, T), 2)*100}$\%.

<<meandensten, fig.cap=paste0("Density of the distribution of the means at n=20 with μ=", rt.mean, " and σ=", rt.sd, "; are at μ±σ is highlighted"), echo=F, fig.pos="t", cache=T>>=

par.defaults()

rt.se <- rt.sd/sqrt(20)

rtm.minmax <- c(105, 135)
.xs <- seq(rtm.minmax[1], rtm.minmax[2], 0.1)
.ys <- rts.stderr(seq(rtm.minmax[1], rtm.minmax[2], 0.1), 20)
plot(x = .xs, y = .ys,
     type = "l", lwd = the.lwd, yaxt="n",
     bty="n", ylab = "", xlab = "Value (x)",
     xlim = rtm.minmax, col = "white"
)
shade(.xs, .ys, rt.mean-2*rt.se, rt.mean+2*rt.se,
      col = lighten(the.lightcols[1], 0.5),
      border = darken(the.lightcols[1], 0.4)
      )
shade(.xs, .ys, rt.mean-rt.se, rt.mean+rt.se,
      col = lighten(the.lightcols[2], 0.5),
      border = darken(the.lightcols[2], 0.4)
      )
text(120, 0.025, "μ±1SE", col = darken(the.lightcols[2], 0.6))
text(113.5, 0.01, "μ±2SE", col = darken(the.lightcols[2], 0.6))
lines(x = .xs, y = .ys,
     type = "l", lwd = the.lwd, col = the.darkgray
)
@

How is this useful?
Think about what it means in terms of probability.
It means that (i)~if the true mean of a variable is $\mu$, (ii)~its true standard deviation is $\sigma$, and (iii)~we draw a sample with $n$, then the probability that the mean of any sample lies in the interval $\mu\pm 1SE$ is $\Sexpr{round(pnorm(1, 0, 1, T)-pnorm(-1, 0, 1, T), 2)}$.
Similarly, the probability that the mean of any sample lies in the interval $\mu\pm 2SE$ is $\Sexpr{round(pnorm(2, 0, 1, T)-pnorm(-2.01, 0, 1, T), 2)}$.
The frequentist interpretation consistently equates the probability of an event occurring with its long-run proportion among repeated repetitions.
In our notation for probabilities from Section~\ref{sec:probability}, for any sample mean \xmean:

\begin{center}
  \begin{math}
    Pr(\mu-SE_{\mu}\leq\xmean\leq\mu+SE_{\mu})\approx\Sexpr{round(pnorm(1, 0, 1, T)-pnorm(-1, 0, 1, T), 2)}
  \end{math}
\end{center}

\begin{center}
  \begin{math}
    Pr(\mu-2SE_{\mu}\leq\xmean\leq\mu+2SE_{\mu})\approx\Sexpr{round(pnorm(2, 0, 1, T)-pnorm(-2.01, 0, 1, T), 2)}
  \end{math}
\end{center}

The standard error is sometimes also just called $\sigma$ as it is a kind of standard deviation.
We find it best to distinguish clearly between the standard deviation of primary data points $\sigma$ and the standard deviation of sample means, the standard error of the mean $\ssigma_{\mu}$.

\subsection{Theoretical Error Intervals}

Given that we can calculate the probability of outcomes that lie within a certain interval, we can also ask the inverse question:
\textit{How many standard errors into each direction (from the mean) define the central 90\%, 95\%, 99\%, etc. sample means under long-run repetition?}
That number of standard errors is often called the \Key{z-value} or just $z$.
One can use the a function (called the \textit{quantile function} of the Normal Distribution) for calculating the z-value, but pre-calculated tables usually suffice.
Table~\ref{tab:ztable} is a very sparse example of such a table.

<<ztable, echo=F, cache=T, results="asis">>=
  ps <- c(0.5,0.8,0.9,0.95,0.99)
  zs <- abs(round(qnorm((1-ps)/2, 0, 1), 2))
  ztable <- data.frame(Pr = ps, Z=zs)

  ztable %>%
    as.data.frame.matrix %>%
    kable(align = c("c", "c"), booktabs = T, linesep = "",
          caption = "Z-Values for some Probabilities",
          label = "ztable") %>%
    row_spec(0, bold=TRUE)
@

Let's go through this again.
Table~\ref{tab:ztable} tells us that 50\% of samples of a normally distributed variable lie within a range around the mean that is 0.67 standard errors wide on both sides of the mean.
Remember that the standard error itself depends only on the variance in the data and the sample size.
Furthermore, 80\% of all samples lie between $\mu-1.28$ standard errors and $\mu+1.28$ standard errors, and so on.
This is formalised in the notion of the \Key{error interval} for the mean $\confint_{\mu}$, where $z_a$ stands for the z-value for a given probability\slash proportion $a$ as illustrated in Table~\ref{tab:ztable}:%
\footnote{We use $a$ because all other variables are taken.}

\begin{equation}
  \confint_{\mu}=\mu\pm z_a\cdot\ssigma_{\mu}
  \label{eq:confintmean}
\end{equation}

<<cisims, cache=T, echo=FALSE, include=FALSE, results='asis'>>=

  set.seed(78)
  # Function for confint simulations.
  ci.sim <- function(n = 100, mu = 0, sigma = 1, z = 1.96, reps = 100) {
    .results <- list()
    for (.i in 1:reps) {
      .sim <- rnorm(n, mu, sigma)
      .var <- var(.sim)
      .mu <- mean(.sim)
      .se <- sqrt(.var/n)
      .lo <- .mu-(.se*z)
      .hi <- .mu+(.se*z)
      .in <- ifelse(mu >= .lo & mu <= .hi, TRUE, FALSE)
      .results[[.i]] <- list(
        "var" = .var,
        "mu" = .mu,
        "se" = .se,
        "low" = .lo,
        "hi" = .hi,
        "in" = .in
      )
    }
    .results
  }

  n.ci <- 20
  z.ci <- 1.96
  reps.ci <- 1000
  ci.sims <- ci.sim(n.ci, rt.mean, rt.sd, z.ci, reps.ci)

  # Get coverage
  cov.ci <- unlist(ci.sims, recursive = F)
  cov.ci <- unname(unlist(cov.ci[which(names(cov.ci)=="in")]))
  cov.ci <- length(which(cov.ci == TRUE))
@

Continuing with the reading time example, we can take the $\ssigma_{\mu}$ calculated above and the mean of $\mu=120$ to find the error interval with $a=0.95$:

\begin{center}
  \begin{math}
    \ssigma_{\mu}=120\pm 1.96\cdot 5=120\pm 9.8=\langle 110.2, 129.8\rangle
  \end{math}
\end{center}

\subsection{Empirical Error Intervals}

But wait!
In normal empirical work, we don't know the true mean and the true standard deviation.
Some readers might have noticed this very early on and gotten frustrated because it appeared as if error intervals were a purely theoretical thing without practical use.
This is not the case, and the argument will finally bring us back to the frequentist logic of inference.
What we do in real life is this:
We calculate an error interval based on the statistics of our concrete empirical sample.
That is, we use \xmean\ instead of $\mu$ and $s_{\xsample}$ instead of $\sigma$.
After all, $\mu$ and $\sigma$ are unknown.
The calculations are the same, only the symbols change:

\begin{center}
  \begin{math}
   \ssigma_{\xmean}=\sqrt{\cfrac{s^2_{\xsample}}{n}}
  \end{math}
\end{center}

\begin{center}
  \begin{math}
    \confint_{\xmean}=\xmean\pm z_a\cdot\ssigma_{\xmean}
  \end{math}
\end{center}

Then, as the inverse of the theoretical error interval, the empirical error interval will contain the true value in $a$ of all cases (with $z_a$ chosen appropriately for $a$).
Hence, we don't already have to know the truth but can still use error intervals.
To make it absolutely clear what an error interval is, we have simulated it for you.
We simulated \Sexpr{reps.ci} samples with sample size $n=\Sexpr{n.ci}$ from the distribution of reading times introduced above.
Then, we used the samples to calculate the empirical error intervals exclusively based on each sample.
Think about it:
We used the simulations of a known fictitious population to generate samples, then we took the samples and calculated the error interval \textit{pretending we didn't know the population} to check whether the error interval works as we think it should.
It's not as good as a mathematical proof, but it really helps in understanding what statistics is about.
Figure~\ref{fig:cisim} shows the true mean and all those empirical error intervals.
The ones that don't include the true mean are coloured red.

<<cisim, fig.cap=paste0("Empirical error intervals from ", reps.ci ," simulated samples; ones that don't contain the true mean marked in red; μ=", rt.mean, ", σ=", rt.sd, ", z=", z.ci, ", n=", n.ci), echo=F, fig.pos="t", cache=T>>=
  par(mar=c(1,2,1,1), family = "Plotfont")
  ylim.ci <- c(90, 150)
  plot(0, xlim=c(0, reps.ci), ylim = ylim.ci, col = "white",
       xaxt = "n", bty = "n", xlab = "", ylab = "")
  lines(c(0, reps.ci), c(rt.mean, rt.mean), col = the.lightcols[2], lwd = the.lwd)
  for (i in 1:length(ci.sims)) {
    .col <- ifelse(ci.sims[[i]][["in"]], alpha(the.cols[1], 0.4), alpha(the.cols[3], 0.9))
    lines(c(i, i), c(ci.sims[[i]][["low"]], ci.sims[[i]][["hi"]]), col = .col)
  }
@

The number of empirical error intervals in these \Sexpr{reps.ci} replications that did include the true mean was \Sexpr{cov.ci}, \ie, \Sexpr{round(cov.ci/reps.ci*100, 1)}\%.
This is called the \Key{coverage}, and it's very close to the expected---or rather \textit{requested}---value of 95\%.
We say \textit{requested} because in deciding to calculate the intervals for a proportion of $a=0.95$ and consequently choosing $z=1.96$, we required that in the long run 95\% of all empirical error intervals should contain the true mean.%
\footnote{Actually, by calculating error intervals with $a=0.95$ for any parameter, our long-run error is 5\%.
It doesn't matter whether we keep taking samples from the same population over and over again or whether we first sample this, then that, and then the other.
It's of no big practical consequence, but a good thing to keep in mind in order not to misinterpret error intervals.}
We've now called it a \textit{proportion}, but think about it again in terms of \textit{probability}.
It's the frequentist probability of drawing a sample whose empirical error interval contains the true mean before we actually draw the sample.
Once it has been drawn, the empirical error interval either contains the true mean or it doesn't, but that's a fact and has no probability attached to it.

The term \textit{error interval} is our invention.
It's usually called the \Key{confidence interval}.
However, there are so many misinterpretations attached to this term that we decided to avoid it.
An error interval with $a=0.95$ is called the \textit{95\% confidence interval} (and similar for other values of $a$).
A very wrong way of interpreting an empirical error interval then goes like this:

\begin{quote} \itshape
  \Argh The 95\% confidence interval for the mean contains the true mean with 95\% certainty.
  (Meaning: We can be 95\% sure that it contains the true mean.)
\end{quote}

What's this even supposed to mean?
Next time you hear someone say something like this about a frequentist error interval, ask them what it means to be \textit{95\% certain of something}.
What it really means is that before taking the sample, there was a probability of 0.95 to draw a sample that contains the true mean (given the chosen $n$ and $z=1.96$).
Now, all we know is that the interval either contains the true mean or a rare event occurred.
Please go back to Section~\ref{sec:pandsig} to convince yourself that an event with a probability of 0.05 is not that rare after all.

\subsection{Varying Z and N}

We use error intervals to get an estimate of the true mean with a known probability of being wrong.
We call this the \Key{safety of the estimate}:
The lower the error probability, the safer the estimate.
If we set $z=1.96$, we will estimate 95\% of all intervals correctly in the long run.
Setting $z$ higher (for example, $z=2.58$) gives us even better long-run chances of getting our estimates right.
So why don't we always set $z=5$ or something really high?
Look at Figure~\ref{fig:cisim}.
If we increased $z$, the samples themselves wouldn't change.
However, we'd require that more intervals should include the true mean, and the only way to do that is to make the intervals wider.
This leads to a higher rate of correct estimates, but the \Key{precision of the estimate} decreases.
We call precision the narrowness of the error interval:
The narrower the interval, the higher the precision.
But be aware that the precision is a value set by us.
Once again, it does not change the sample itself, and hence it does not change the fact that the sample represents the population (parameter) either well or not.

First, let's calculate an empirical error interval with $z=1.96$ and $z=2.58$ before looking at some more illustrative plots.
A single example replications of the simulation led to the following statistics:
$\xmean=\Sexpr{round(ci.sims[[333]][["mu"]], 2)}$ and $s^2_{\xsample}=\Sexpr{round(ci.sims[[333]][["var"]], 2)}$.
The $\ssigma_{\mu}$ is calculated as follows:

\begin{center}
  \begin{math}
    \ssigma_{\xmean}=\sqrt{\cfrac{\Sexpr{round(ci.sims[[333]][["var"]], 2)}}{\Sexpr{n.ci}}}=\Sexpr{round(sqrt(round(ci.sims[[333]][["var"]], 2)/n.ci), 2)}
  \end{math}
\end{center}

The 95\% error interval (\ie, $z=1.96$) and the 99\% error interval (\ie, $z=2.58$) are:

\begin{center}
  \begin{math}
    EI_{\xmean,0.95}=\Sexpr{round(ci.sims[[333]][["mu"]], 2)}\pm 1.96\cdot\Sexpr{round(sqrt(round(ci.sims[[333]][["var"]], 2)/n.ci), 2)}=\langle\Sexpr{round(round(ci.sims[[333]][["mu"]], 2)-1.96*round(sqrt(round(ci.sims[[333]][["var"]], 2)/n.ci), 2), 2)}, \Sexpr{round(round(ci.sims[[333]][["mu"]], 2)+1.96*round(sqrt(round(ci.sims[[333]][["var"]], 2)/n.ci), 2), 2)} \rangle
  \end{math}

  \begin{math}
    EI_{\xmean,0.99}=\Sexpr{round(ci.sims[[333]][["mu"]], 2)}\pm 2.58\cdot\Sexpr{round(sqrt(round(ci.sims[[333]][["var"]], 2)/n.ci), 2)}=\langle\Sexpr{round(round(ci.sims[[333]][["mu"]], 2)-2.58*round(sqrt(round(ci.sims[[333]][["var"]], 2)/n.ci), 2), 2)}, \Sexpr{round(round(ci.sims[[333]][["mu"]], 2)+2.58*round(sqrt(round(ci.sims[[333]][["var"]], 2)/n.ci), 2), 2)} \rangle
  \end{math}
\end{center}

\noindent The interval is roughly 6ms larger for the added 4\% success rate.
For a more comprehensive impression of the effect of manipulating $z$ and the effect of the sample size $n$, we provide Figure~\ref{fig:fourcisims}.

<<fourcisims, fig.cap=paste0("Simulations of empirical error intervals for different z and n with 100 replications each"), echo=F, fig.pos="t", cache=T>>=

  par(mfrow = c(2,2), mar=c(1,2,3,1), family = "Plotfont")

  set.seed(92)
  n.ci <- 20
  z.ci <- 1.96
  reps.ci <- 100

  ci.sims <- ci.sim(n.ci, rt.mean, rt.sd, z.ci, reps.ci)
  cov.ci <- unlist(ci.sims, recursive = F)
  cov.ci <- unname(unlist(cov.ci[which(names(cov.ci)=="in")]))
  cov.ci <- length(which(cov.ci == TRUE))

  ylim.ci <- c(100, 140)
  plot(0, xlim=c(0, reps.ci), ylim = ylim.ci, col = "white",
       xaxt = "n", bty = "n", xlab = "", ylab = "", main = paste0("n=", n.ci, " z=", z.ci, "\nCoverage ", cov.ci, "%"))
  lines(c(0, reps.ci), c(rt.mean, rt.mean), col = the.lightcols[2], lwd = the.lwd)
  for (i in 1:length(ci.sims)) {
    .col <- ifelse(ci.sims[[i]][["in"]], alpha(the.cols[1], 0.4), alpha(the.cols[3], 0.9))
    lines(c(i, i), c(ci.sims[[i]][["low"]], ci.sims[[i]][["hi"]]), col = .col, lwd = the.lwd)
  }

  n.ci <- 100

  # Duplicated code!!!
  ci.sims <- ci.sim(n.ci, rt.mean, rt.sd, z.ci, reps.ci)
  cov.ci <- unlist(ci.sims, recursive = F)
  cov.ci <- unname(unlist(cov.ci[which(names(cov.ci)=="in")]))
  cov.ci <- length(which(cov.ci == TRUE))
  plot(0, xlim=c(0, reps.ci), ylim = ylim.ci, col = "white",
       xaxt = "n", bty = "n", xlab = "", ylab = "", main = paste0("n=", n.ci, " z=", z.ci, "\nCoverage ", cov.ci, "%"))
  lines(c(0, reps.ci), c(rt.mean, rt.mean), col = the.lightcols[2], lwd = the.lwd)
  for (i in 1:length(ci.sims)) {
    .col <- ifelse(ci.sims[[i]][["in"]], alpha(the.cols[1], 0.4), alpha(the.cols[3], 0.9))
    lines(c(i, i), c(ci.sims[[i]][["low"]], ci.sims[[i]][["hi"]]), col = .col, lwd = the.lwd)
  }

  n.ci <- 20
  z.ci <- 2.58

  # Duplicated code!!!
  ci.sims <- ci.sim(n.ci, rt.mean, rt.sd, z.ci, reps.ci)
  cov.ci <- unlist(ci.sims, recursive = F)
  cov.ci <- unname(unlist(cov.ci[which(names(cov.ci)=="in")]))
  cov.ci <- length(which(cov.ci == TRUE))
  plot(0, xlim=c(0, reps.ci), ylim = ylim.ci, col = "white",
       xaxt = "n", bty = "n", xlab = "", ylab = "", main = paste0("n=", n.ci, " z=", z.ci, "\nCoverage ", cov.ci, "%"))
  lines(c(0, reps.ci), c(rt.mean, rt.mean), col = the.lightcols[2], lwd = the.lwd)
  for (i in 1:length(ci.sims)) {
    .col <- ifelse(ci.sims[[i]][["in"]], alpha(the.cols[1], 0.4), alpha(the.cols[3], 0.9))
    lines(c(i, i), c(ci.sims[[i]][["low"]], ci.sims[[i]][["hi"]]), col = .col, lwd = the.lwd)
  }

  n.ci <- 100

  # Duplicated code!!!
  ci.sims <- ci.sim(n.ci, rt.mean, rt.sd, z.ci, reps.ci)
  cov.ci <- unlist(ci.sims, recursive = F)
  cov.ci <- unname(unlist(cov.ci[which(names(cov.ci)=="in")]))
  cov.ci <- length(which(cov.ci == TRUE))
  plot(0, xlim=c(0, reps.ci), ylim = ylim.ci, col = "white",
       xaxt = "n", bty = "n", xlab = "", ylab = "", main = paste0("n=", n.ci, " z=", z.ci, "\nCoverage ", cov.ci, "%"))
  lines(c(0, reps.ci), c(rt.mean, rt.mean), col = the.lightcols[2], lwd = the.lwd)
  for (i in 1:length(ci.sims)) {
    .col <- ifelse(ci.sims[[i]][["in"]], alpha(the.cols[1], 0.4), alpha(the.cols[3], 0.9))
    lines(c(i, i), c(ci.sims[[i]][["low"]], ci.sims[[i]][["hi"]]), col = .col, lwd = the.lwd)
  }

  par.defaults()
@

As expected, a larger $n$ makes the intervals smaller (increased precision) because it makes the standard error smaller (see Section~\ref{sec:milli}).
A larger $z$, on the other hand, make the intervals larger because we require that the probability of the interval including the true mean be higher (increased safety).
By the way, the fact that the coverage does not always land exactly at 95\% or 99\% is not surprising.
A perfect match between the theoretical expectation and the empirical reality is guaranteed only in the limit, \ie, if we take infinitely many samples.
In the remainder of the chapter, we will introduce error intervals for proportions before returning briefly to the Normal Distribution in the in-depth Section~\ref{sec:gaussian}).

\Bigpoint{Interpretation of Error Intervals}{%
An error interval for the mean is an attempt to estimate the true mean.
It provides an interval which covers the true mean with a specified frequentist pre-sample probability.
For example, a 95\% error interval ($a=0.95$) means that before the sample is drawn, it has a probability of 0.95 of actually including the true mean.
Once it is drawn, it either includes the true mean, or a (relatively) rare event has occurred.
In the long run, it is guaranteed that a proportion of $a$ intervals thus calculated ($a\cdot 100$\%) include the true mean.
An error interval does not quantify any justifiable degree of \textit{confidence} we should have in the estimate, which is why we don't call it \textit{confidence interval}.
The term \textit{error interval} reflects the fact that we merely control the error rate of the estimation.

If safe estimates are of the essence, one should increase $q$ (for example, $q=0.99$ for the 99\% error interval).
This lowers the error rate in the long run and makes the estimation \textit{safer}.
A safer estimates increase the size of the intervals and makes it less \textit{precise} (larger intervals).
Changing the sample size does not change the safety of the estimate.
Hence, if \textit{both a precise and a safe} estimation is required, one should increase $q$ and also increase the sample size $n$.
Finally, error intervals (also called confidence intervals) have nothing to do with specifying the degree of certainty that the true mean falls within the interval.
}

\section{Error Intervals for Binary Measurements}\label{sec:variabilitysampleproportions}

The idea behind error intervals for proportions is essentially the same as for means.
Look back at Figures~\ref{fig:berryone}, \ref{fig:berrythreetoten}, and~\ref{fig:berrybig}.
The distributions of the proportions from samples with very low sample size show obvious banding and look irregular.
However, at least starting with $n=20$, the plots look very similar to the plots of means in Figure~\ref{fig:rtsmall} and Figure~\ref{fig:rtsbig}.
Compare the density plots in Figure~\ref{fig:histcompare}.
Keep in mind:
These are plots of the distribution of proportions and means calculated from a lot of (simulated) samples, not plots of raw individual measurements.

<<histcompare, fig.cap=paste0("Histograms of 1000 proportions measured in samples (left) and 1000 sample means (right) at the same sample sizes"), echo=F, fig.pos="t", cache=T>>=
  par(mfrow=c(3,2), mar=c(5,5,1.5,1), family = "Plotfont")

  plot(density(unlist(berry.sims["5"]), kernel = "cosine"),
       main = "n=5", xlab = "", ylab = "Density", bty="n", yaxt = "n",
       lwd = the.lwd, col = the.darkgray, xlim=c(-0.25, 1))
  plot(density(unlist(rt.sims["5"]), kernel = "cosine"),
       main = "n=5", xlab = "", ylab = "", bty="n", yaxt = "n",
       lwd = the.lwd, col = the.darkgray, xlim=c(90, 150))

  plot(density(unlist(berry.sims["10"]), kernel = "cosine"),
       main = "n=10", xlab = "", ylab = "Density", bty="n", yaxt = "n",
       lwd = the.lwd, col = the.darkgray, xlim=c(-0.25, 1))
  plot(density(unlist(rt.sims["10"]), kernel = "cosine"),
       main = "n=10", xlab = "", ylab = "", bty="n", yaxt = "n",
       lwd = the.lwd, col = the.darkgray, xlim=c(90, 150))

  plot(density(unlist(berry.sims["20"]), kernel = "cosine"),
       main = "n=20", xlab = "Proportions", ylab = "Density", bty="n", yaxt = "n",
       lwd = the.lwd, col = the.darkgray, xlim=c(-0.25, 1))
  plot(density(unlist(rt.sims["20"]), kernel = "cosine"),
       main = "n=20", xlab = "Means", ylab = "", bty="n", yaxt = "n",
       lwd = the.lwd, col = the.darkgray, xlim=c(90, 150))

  par.defaults()
@

The distributions still look bumpy at $n=20$, but that is expected.
Most importantly, however, there is virtually no qualitative distinction anymore between the distributions of proportions and menas at $n=20$, even though proportions reflect counts of a discrete variable and means derive from a continuous numerical variable.
It was also shown beyond such impressionistic comparisons that the sampling distributions of proportions is reasonably well approximated by a normal distribution.
Hence, we can use the normal error interval we introduced for the mean $\mu$ for proportions as well.
The theoretical error interval for the population proportion $\rho$ is:

\begin{equation}
  \confint_{\rho}=\rho\pm z\cdot\ssigma_{\rho}
  \label{eq:confintprop}
\end{equation}

\noindent Again, the empirical error interval based on a single sample is the same, except for a single sample proportion $q$ instead of the population proportion $\rho$:

\begin{equation}
  \confint_{q}=q\pm z\cdot\ssigma_{q}
  \label{eq:confintpropemp}
\end{equation}

But what are $\ssigma_{\rho}$ and $\ssigma_p$?
The raw data points do not have a normal variance associated with them.
They are binary, either and or, 0 or 1.
We need a measure for the sample proportions that is not directly derived from variance in the data points.%
\footnote{Of course, real statisticians have conducted proofs and done all the proper maths.
We skip all of this and try to make things easy to grasp intuitively.}
We expect the standard error for proportions to be smaller for larger samples (see Figure~\ref{fig:histcompare}).
Hence, we divide by $n$.
Additionally, consider that proportions range from 0 to 1 and consider at which values between 0 and 1 the variance in proportions might be higher.
Again, we use lots of simulated samples for illustration, see Figure~\ref{fig:propdist}.

<<propdist, fig.cap=paste0("Proportions from samples with n=20 with different true population proportions ρ"), echo=F, fig.pos="t", cache=T>>=
  par(mfrow=c(1,2), family = "Plotfont")
  berry.prob <- 0.1
  n <- 20
  berry.reps <- 1000
  .sample <- unname(unlist(lapply(1:berry.reps, function(x) {berries(n)})))
  plot(.sample, pch = 19, col = alpha("white", 0), cex = 1.5, ylim=c(0,1),
       xaxt = "n", bty = "n", ylab = "Proportion",
       xlab = "Individual Samples", main = "ρ=0.1 and ρ=0.9")
  points(.sample,
         pch = 19, col = alpha(the.cols[1], 0.5), cex = 1.5)

  berry.prob <- 0.9
  .sample <- unname(unlist(lapply(1:berry.reps, function(x) {berries(n)})))
  points(.sample,
         pch = 19, col = alpha(the.midgray, 0.5), cex = 1.5)

  berry.prob <- 0.5
  .sample <- unname(unlist(lapply(1:berry.reps, function(x) {berries(n)})))
  plot(.sample, pch = 19, col = alpha("white", 0), cex = 1.5, ylim=c(0,1),
       xaxt = "n", bty = "n", ylab = "Proportion",
       xlab = "Individual Samples", main = "ρ=0.5")
  points(.sample,
         pch = 19, col = alpha(the.cols[3], 0.5), cex = 1.5)

  par.defaults()
@

We hope you noticed what's going on there.
At $\rho=0.5$, the sample proportion deviates from the true proportion into the negative and the positive direction.
The farther towards 0 or 1 the true proportion is moved, the more restricted the possible variation is in one of the two directions.
As proportions can't be negative or larger than 1, the variance in the sample proportions hits a ceiling of 1 and a floor of 0.
Hence, the variance is largest at $\rho=0.5$ and smallest at $\rho=0$ and $\rho=1$.

The standard error for proportions $\ssigma_{\rho}$ (theoretical) and $\ssigma_q$ (empirical) reflects this:

\begin{equation}
  \ssigma_{\rho}=\sqrt{\cfrac{\rho\cdot(1-\rho)}{n}}
  \label{eq:seprop}
\end{equation}

\begin{equation}
  \ssigma_{q}=\sqrt{\cfrac{q\cdot(1-q)}{n}}
  \label{eq:sepropemp}
\end{equation}

We multiply the proportion $\rho$ by $1-\rho$ (or $q$ by $q-1$).
Do you see why that is appropriate (at least what's below the square root)?
The simplest illustration is the comparison between, for example, $0.9\cdot 0.1=0.09$ compared to the much larger $0.5\cdot 0.5=0.25$.

Figure~\ref{fig:qnotq} illustrates it for all values of $\rho$ or $q$.
Multiplying $\rho$ by $1-\rho$ (or $q$ by $q-1$) gives us exactly what we need:
a function that starts at the function value 0 for the input 0, then increases as we move towards 0.5 (with a function value of 0.25).
Then, the function value returns to 0 as we move towards the input value 1.
The square root conserves this property.
While it's good that mathematical proofs have led to the development of the maths, we can also make sense of the formulas and understand them from the viewpoint of a mere practitioner.

<<qnotq, fig.cap=paste0("Proportions q (x-axis) plotted against q⋅(1-q) (y-axis) and its square root"), echo=F, fig.pos="t", cache=T>>=
  par.defaults()
  .qnotq <- seq(0, 1, 0.01)
  plot(.qnotq, .qnotq*(1-.qnotq), type = "l", lwd = the.lwd, ylim = c(0, 0.55),
       col = the.darkgray, bty = "n", ylab = "", xlab = "q (or ρ)")
  lines(.qnotq, sqrt(.qnotq*(1-.qnotq)), type = "l", lwd = the.lwd,
       col = the.cols[[1]], bty = "n", ylab = "q⋅(1-q)", xlab = "q")
  legend("topleft", legend = c("q⋅(1-q)", "√[q⋅(1-q)]"), bty = "n",
         col = c(the.darkgray, the.cols[[1]]),
         lwd = the.lwd)
@

Therefore, the width of the confidence intervals has the characteristics observable in Figure~\ref{fig:cioverview}.
It's largest for centre values and very narrow for extreme proportions.
This concludes our discussion of estimation and sample variance.
There is one final word of warning:
The error interval discussed here is the so-called \Key{Wald interval} (named after Abraham Wald).
It's very simple and it has some suboptimal properties due to its relying on the distribution of sample proportions being \textit{approximately normal}.
For example, close to the extremes, the intervals can extend to minimally below 0 and above 1, which is nonsensical for proportions.%
\footnote{However, anyone who thinks that this (of all things) is a strong argument against frequentism has no idea what they're talking about.}
In general, it's not very exact.
There are other frequentist alternatives available which are more exact but less suitable for a conceptual introduction like ours.
If you ever have to calculate error intervals for proportions, look for the \Key{Wilson score interval} or our favourite, the \Key{Clopper-Pearson interval}.
The idea behind all of them is the same, the alternatives just use different maths.

<<cioverview, fig.cap=paste0("Width of error intervals for proportions at three sample sizes, three safety levels, and for proportions from 0 to 1"), echo=F, fig.pos="t", cache=T>>=

  par.defaults()

  ci.prop <- function(p = 0.5, n = 100, sig = 0.95, width = T) {
    .se <- sqrt((p*(1-p)/n))
    .z <- qnorm((1-sig)/2, lower.tail = F)
    if (width) 2 * .se * .z
    else .se * .z
  }
  ci.80.10 <- function(p) ci.prop(p = p, n = 10, sig = 0.8)
  ci.80.100 <- function(p) ci.prop(p = p, n = 100, sig = 0.8)
  ci.80.1000 <- function(p) ci.prop(p = p, n = 1000, sig = 0.8)

  par(mfrow=c(1,3))

  plot(ci.80.10(seq(0, 1, 0.01)), type = "l", col = the.cols[1], lwd = 3,
       main = "80%", ylim = c(0, 1), bty = "n", xaxt = "n", cex = 1.2,
       cex.axis = 1.2, cex.main = 1.2, cex.lab = 1.2, xlab = "", ylab = "")
  axis(1, at = seq(1, 101, 10), labels = seq(0, 1, 0.1), cex = 1.2,
       cex.axis = 1.2, las = 2)
  lines(ci.80.100(seq(0, 1, 0.01)), col = the.darkgray, lwd = 3, lty = 1)
  lines(ci.80.1000(seq(0, 1, 0.01)), col = the.cols[3], lwd = 3, lty = 1)
  legend("topleft", legend = c("n=10", "n=100", "n=1000"), lwd = 3, col = the.cols,
         bty = "n", cex = 1.2)

  ci.95.10 <- function(p) ci.prop(p = p, n = 10)
  ci.95.100 <- function(p) ci.prop(p = p, n = 100)
  ci.95.1000 <- function(p) ci.prop(p = p, n = 1000)
  plot(ci.95.10(seq(0, 1, 0.01)), ty = "l", col = the.cols[1], lwd = 3,
       main = "95%", ylim = c(0, 0.9), bty = "n", cex = 1.2, cex.axis = 1.2,
       cex.main = 1.2, cex.lab = 1.2, yaxt = "n", xaxt = "n", xlab = "",
       ylab = "")
  axis(1, at = seq(1, 101, 10), labels = seq(0, 1, 0.1), cex = 1.2,
       cex.axis = 1.2, las = 2)
  mtext("Proportion", side = 1, line = 4)
  lines(ci.95.100(seq(0, 1, 0.01)), col = the.darkgray, lwd = 3, lty = 1)
  lines(ci.95.1000(seq(0, 1, 0.01)), col = the.cols[3], lwd = 3, lty = 1)

  ci.99.10 <- function(p) ci.prop(p = p, n = 10, sig = 0.99)
  ci.99.100 <- function(p) ci.prop(p = p, n = 100, sig = 0.99)
  ci.99.1000 <- function(p) ci.prop(p = p, n = 1000, sig = 0.99)
  plot(ci.99.10(seq(0, 1, 0.01)), ty = "l", col = the.cols[1], lwd = 3,
       main = "99%", ylim = c(0, 0.9), bty = "n", cex = 1.2, cex.axis = 1.2,
       cex.main = 1.2, cex.lab = 1.2, yaxt = "n", xaxt = "n", ylab = "",
       xlab = "")
  axis(1, at = seq(1, 101, 10), labels = seq(0, 1, 0.1), cex = 1.2,
       cex.axis = 1.2, las = 2)
  lines(ci.99.100(seq(0, 1, 0.01)), col = the.darkgray, lwd = 3, lty = 1)
  lines(ci.99.1000(seq(0, 1, 0.01)), col = the.cols[3], lwd = 3, lty = 1)
@


\section{\Indepth\ The Normal Distribution}\label{sec:gaussian}

In this short section, we show the density function of the Normal Distribution as an added bonus for the curious.
The purpose isn't to turn you into mathematicians or to prove anything.
We just would like to show you that sometimes intimidating functions and equations can be understood almost intuitively with minimal effort in taking them apart.
This is the general density function of the Normal Distribution:

\begin{equation}
  \frac{1}{\sqrt{2\pi\sigma^2}}\cdot e^{-\frac{(x - \mu)^2}{2\sigma^2}}
  \label{eq:normal}
\end{equation}

<<plotnormalone, fig.cap=paste0("Dissecting the density function of the Normal Distribution"), echo=F, fig.pos="t", cache=T>>=
  mmu <- 120
  ssigma <- 20

  par.defaults()

  par(mar=c(2,3,2,2), mfrow=c(2,2), family = "Plotfont")

  xs <- seq(-300, 300, 0.01)

  plot(xs, xs^2, type = "l",
       xlim = c(-20, 20), ylim = c(0, 200), bty ="n", xlab = "x", ylab = "",
       main = "(1)",
       lwd = the.lwd, col = the.darkgray)
  plot(xs, exp(-xs^2), type = "l",
       xlim = c(-3, 3), ylim = c(0, 1), bty ="n", xlab = "", ylab = "",
       lwd = the.lwd, col = the.darkgray,
       main = "(2)")
  plot(xs, exp(-(xs^2/(2*ssigma^2))), type = "l",
       xlim = c(-80, 80), ylim = c(0, 1), bty ="n", xlab = "", ylab = "",
       lwd = the.lwd, col = the.darkgray, xaxt = "n",
       main = "(3)")
  axis(1, seq(-80, 80, 20), seq(-80, 80, 20))
  plot(xs, exp(-((xs-mmu)^2/(2*ssigma^2))), type = "l",
       xlim = c(40, 200), ylim = c(0, 1), bty ="n", xlab = "", ylab = "",
       lwd = the.lwd, col = the.darkgray, xaxt = "n",
       main = "(4)")
  axis(1, seq(40, 200, 20), seq(40, 200, 20))

  par.defaults()
@

Remember that $\mu$ and $\sigma$ are parameters.
They define the precise shape of the distribution, but they do not vary with the actual input values $x$.
Imagine a parametrisation for a given sampling distribution of means---which, as we've shown follows a Normal Distribution---, where $\mu=120$ and $\sigma=\ssigma_{\mu}=20$ as in our example above.
These parameters determine the centre x-value (mean) and slope (standard deviation) of the distribution, but the input values correspond to means for which the density function gives us the probability.
As $x$ does not occur in the first multiplicand, it is a constant in the parametrised function, and we ignore it for the time being.
We're left with:

\begin{center}
  \begin{math}
    e^{-\frac{(x-\mu)^2}{2\sigma^2}}
  \end{math}
\end{center}

This is an exponential function, a function with the Euler number ($e\approx 2.72$) as its base and \textit{something} in the exponent.
In this case, this \textit{something} is first and foremost $x^2$, which gives us a simple parabola, see Figure~\ref{fig:plotnormalone} panel~(1):

\begin{center}
  \begin{math}
    \text{(1)\ }x^2
  \end{math}
\end{center}

The exponential function changes its shape (making it flatter around its minimum), and $e^{-x^2}$ is just the reciprocal of $e^{x^2}$, which means it inverts the curve, which will then also be squashed between 0 and 1.
We get something the shape of which already looks exactly like a normal density function, see Figure~\ref{fig:plotnormalone} panel~(2).

\begin{center}
  \begin{math}
    \text{(2)\ }e^{-x^2}
  \end{math}
\end{center}

Dividing the exponent by $2\sigma^2$, which is twice the variance, simply scales the curve horizontally.
It spreads it out with increasing variance.
Figure~\ref{fig:plotnormalone} panel (3) is exactly the same as Figure~\ref{fig:plotnormalone} panel~(2), except the x-axis is scaled up:

\begin{center}
  \begin{math}
    \text{(3)\ }e^{-\frac{x^2}{2\sigma^2}}
  \end{math}
\end{center}

<<plotnormaltwo, fig.cap=paste0("Dissecting the density function of the Normal Distribution; final version"), echo=F, fig.height=5, fig.pos="t", cache=T>>=

  par(mfrow=c(1,1), mar=c(3,2,2,2), family = "Plotfont")
  plot(xs, (1/sqrt(2*pi*ssigma^2)) * 1/exp((xs-mmu)^2 / (2*ssigma^2)), type = "l",
       xlim = c(60, 180), ylim = c(0, 0.02), bty ="n", xlab = "", ylab = "",
       main = "",
       lwd = 0.5, col = the.cols[2])
  arrows(125, 0.005, 150, 0.005, 0.075, col = the.cols[3], lwd = the.lwd, code = 2)
  arrows(115, 0.005, 90, 0.005, 0.075, col = the.cols[3], lwd = the.lwd, code = 2)
  text(137.5, 0.0055, "σ", col = the.cols[3], 0.5)
  text(102.5, 0.0055, "σ", col = the.cols[3], 0.5)

  lines(c(120, 120), c(0, 0.0199), col = the.cols[2], lwd = the.lwd)
  arrows(70, 0, 115, 0, 0.075, col = the.cols[2], lwd = the.lwd)
  text(90, 0.0005, "μ", col = the.cols[2], 0.5)

  lines(xs, (1/sqrt(2*pi*ssigma^2)) * 1/exp((xs-mmu)^2 / (2*ssigma^2)),
       lwd = the.lwd, col = the.darkgray)
  par.defaults()
@

Subtracting $\mu$ from the numerator in the exponent just centres it around the mean $\mu$.
This happens independently of the exponential function.
Try removing the exponential, and it will still move the curve (which would still look like a scaled parabola by itself).
Figure~\ref{fig:plotnormalone} panel~(4) corresponds to:

\begin{center}
  \begin{math}
    \text{(4)\ }e^{\frac{(x-\mu)^2}{2\sigma^2}}
  \end{math}
\end{center}

It's still exactly the same as Figure~\ref{fig:plotnormalone} panel~(2), just scaled on both axes and moved horizontally.
Finally, the denominator of the constant term is equivalent to the area under the curve plotted in Figure~\ref{fig:plotnormalone} panel~(4):

\begin{center}
  \begin{math}
    \frac{1}{\sqrt{2\pi\sigma^2}}
  \end{math}
\end{center}

Hence, its reciprocal as a constant factor scales the function again to ensure that the area under its curve is 1, which is a requirement for any probability density function, see Figure~\ref{fig:plotnormaltwo}.
That's it.
It's not rocket science to wrap one's mind around why the function looks the way it does and how the parameters $\mu$ and $\sigma$ shape the curve.
It's indeed rocket science to come up with (\ie, invent) useful probability distributions, work out their properties, and show why they have something to do with how the world works.
To dig at least a bit deeper while still being entertained, two videos by Grant Sanderson (3Blue1Brown) are highly recommended:
\url{https://youtu.be/zeJD6dqJ5lo} and \url{https://youtu.be/cy8r7WSuT1I}.
For non-mathematicians, that's already quite sophisticated material.


\begin{exercises}

\exercise{confidence1} One Two Three

\end{exercises}
