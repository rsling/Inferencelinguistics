% !Rnw root = ../main.Rnw
<<setupconfidence, cache=FALSE, echo=FALSE, include=FALSE, results='asis'>>=
opts_knit$set(self.contained=FALSE)

set.seed(524)

### Binary variable ###

rt.mean <- 120
rt.sd <- 20
rt.reps <- 100

rts <- function(n) {
  .t <- rnorm(n, rt.mean, rt.sd)
  mean(.t)
}

rt.sims <- list()
rt.ns <- c(1,2,3,4,5,10,20,50,100,1000,2000,5000,10000)

for (n in rt.ns) {
  .sample <- unname(unlist(lapply(1:rt.reps, function(x) {rts(n)})))
  rt.sims[[as.character(n)]] <- .sample
}

# Single sample for illu plot.
.normsample <- rnorm(100, rt.mean, rt.sd)

### Numeric variable ###

berry.prob <- 0.3
berry.reps <- 100

berries <- function(n) {
  .t <- rbinom(n, size = 1., prob = berry.prob)
  .t <- length(which(.t == 1))
  .t/n
}

berry.sims <- list()
berry.ns <- c(1,2,3,4,5,10,20,50,100,1000,2000,5000,10000)

for (n in berry.ns) {
  .sample <- unname(unlist(lapply(1:berry.reps, function(x) {berries(n)})))
  berry.sims[[as.character(n)]] <- .sample
}


@

\chapter{Sampling Accuracy and Parameter Estimation}
\label{sec:confidence}

\section*{Overview}

\Problem{How Reliable Is Your Sample?}{%
Have you ever tried Haribo Berries?
They come in packs containing two delicious flavours: raspberry and blackberry.
The two flavours are clearly distinct, and you should really prefer blackberry flavour.
However, many people feel that there are on average less blackberries than raspberries per pack.
Before complaining to Haribo, you decide to take a sample and buy a pack containing 100g.
You count them and find that there are 18 raspberries and 12 blackberries.
Hm.
Is this a precise estimate of the distribution of berries in the average pack?
You consider buying a 3kg pack and counting the 900 berries in it.
But how much better would the resulting estimate be compared to the 100g bag?
Would it be 30 times better?
What does this even mean?

Later that day (feeling a tad queasy because you've destroyed the evidence), you go back to ``the lab'' to do a self-paced reading experiment in order to find out the average per-word reading speed of adult Japanese speakers in a pre-study.
It's an exploratory study, and you don't have a hypothesis for the average.
Of the 30 invited participants, 4 show up, each reading 10 words.
Adjusted for word-length, it took them a mean 100ms per word.
A colleague from the theory department pops over for a coffee, looks at the results, and recommends increasing your sample size to get a better estimate.
She says you should have at least 100 participants read 1000 words each.
How much better would your estimate be?
Would it be 2500 times better?
What does this even mean?
}

\section{Sampling From a Population}

\subsection{Data Generating Processes}

Before we discuss sampling accuracy, we need to clarify what we mean by a population.
In Section~\ref{sec:populationssamples} we colloquially compared three populations: (i)~all galaxies in the Laniakea Supercluster, (ii)~all adult Tories in Buckinghamshire, and (iii)~all sentences of contemporary German.
Clearly, these are very different in type and count.
There are (according to Wikipedia) roughly 100,000 galaxies in Laniakea.
The population in Buckinghamshire is roughly 850,000, and it's a very conservative county.
Let's estimate 70\% of the population are Tory supporters.%
\footnote{According to the Buckinghamshire Council website, there are 105 conservative councillors, 15 are independent, 15 are liberal democrats, 6 belong to Labour, and 5 have other affiliations as of 25 February 2025.
That's approximately 70\% Tories.
However, given the British voting system, the council is a bad estimator of the composition of the electorate in terms of political affiliation.
(\url{https://buckinghamshire.moderngov.co.uk/mgMemberIndex.aspx})}
The population of Tory supporters in Buckinghamshire probably consists of some 400,000 to 450,000 humans (having subtracted 20\% of the total population to account for minors and apolitical people).
How many \textit{sentences of contemporary German} are there?
That's a much more difficult question.
What counts as German?
What counts as a sentence?
Do sentences spoken and written by L2 learners count?
How advanced do these L2 learners have to be?
Is CEFR level B2 good enough, or do we require C1?
These questions can't be answered in a statistics text book.%
\footnote{They should have been answered by linguists (at least those who rely on empirical data) before they started to do research on German per se (as opposed to research on the linguistic behaviour of samples of German-speaking subjects).
If you're a corpus linguist who believes that your corpus of choice \textit{represents} a population, you should have a very good answer to these questions.
In this respect, the most important property of works such as \citet{Biber1993} is that they're over 30 years old.}
What matters is that none of these populations is static.
Over the lifespan of the universe, the number of galaxies in Laniakea went from 0 to 100,000, and it will return to 0.
Galaxies form or are drawn into superclusters all the time, and galaxies are torn apart and will eventually fall apart (in layman's terms) in the distant future.
People in Buckinghamshire are born and die, and people change their political affiliation all the time.
German sentences are produced at a breathtaking rate, and even the population of speakers of German changes every minute.
If the population really mattered as a fixed construct of a well-defined (even if unknown) size, we'd have to repeat all empirical work every day, second, or even millisecond.

While it's sometimes stressed that the population needs to be infinite or at least significantly larger than the sample, we see this rarely ever playing an important role in empirical science.
What's more, the whole idea of a fixed and huge population from which we draw a sample is not very helpful.
Why does any linguist draw a sample of German sentences?
In the post-structuralist era, it's not because we want to find out properties of a massive collection of sentences but because we're interested in the mechanism that generates such sentences.
It doesn't matter which model of grammar a linguist believes in:
When they look at a sample of sentences they ask what is the grammar like that produces such sentences, be it a formal or a cognitive concept of grammar.
Hence, we consider it much more appropriate and intuitive to speak of the \Key{data-generating process} (DGP) rather than the population.
The process can be conceived of as cognitive, social, or even purely formal.
In fact, our simulations (see Section~\ref{sec:pdist}) simulate exactly such processes.
Since any scientifically interesting process can, in principle, generate ever new data points, the population of those data points is conceptually infinite.

In this chapter, we ask a very crucial question.
How reliable is a sample in representing the DGP (or, in old-school terms, the population)?
Imagine English speakers were programmed (through cognitive constraints) to produce only 2\% passives of \textit{sleep}.
In this case, the true parameter with which the DGP was set up is 2\% (a proportion of 0.02).
How well can you expect to approximate this percentage\slash proportion with a sample of size $n=1$, $n=10$, $n=100$, and so on?
Do you see how this question is related to the logic of testing introduced in Chapter~\ref{sec:fisher}?
However, this chapter is not about testing but rather about the \Key{estimation} a parameter of the population from a sample.
In order to get there, we need to talk about the distribution of results in repeated experiments in relation to true values of the DGP.
We'll use the results from this chapter to develop a general framework for inferential testing in Chapter~\ref{sec:zandt}.

\subsection{Sampling Berries: One, Two, Three, Many}

We begin with a simulation of a known situation.
As explained in Section~\ref{sec:pdist}, this is never the case in actual empirical work.
We do empirical research to find out what reality is like, precisely because we don't know what it's like.
However, simulating a known reality allows us to explore and illustrate what happens in actual empirical work.
Hence, we now simulate a berry-generating Haribo process that produces blackberries at a rate of $\Sexpr{berry.prob}$ (or $\Sexpr{berry.prob*100}$\%).
In the long run, bags of Haribo Berries produced by this process should contain a proportion of $q\Sub{blackberry}=\Sexpr{berry.prob}$ on average.
That does not mean, however, that \textit{each} bag will contain \textit{exactly} $\Sexpr{berry.prob*100}$\% blackberries.
Also, we expect larger samples to better approximate the true proportion of blackberries.
We've been re-iterating this point and variations of it throughout the previous chapters.

The simulated process allows us to pretend that we have a machine that randomly fills bags with berries from a production line that produces $\Sexpr{berry.prob*100}$\% blackberries and $\Sexpr{(1-berry.prob)*100}$\% raspberries.
We call each such bag a \textit{simulation run} or a \Key{replication}.
In Figure~\ref{fig:berryone}, we plot the results for \Sexpr{berry.reps} replications at a sample size of $n=1$.
Each dot represents one bag containing a single berry filled by a machine that produces $\Sexpr{berry.prob*100}$\% blackberries by design.
We plot the proportion of blackberries in the sample.
As a sample of 1 berry contains either a blackberry or a raspberry and nothing else, the proportion of blackberries can only be either 0 or 1.

<<berryone, fig.cap=paste0("Proportions measured in 100 replications at n=1 with a true proportion of 0.3"), echo=F, fig.pos="t", out.width="85%", cache=T>>=
  p <- plot(unlist(berry.sims["1"]),
            pch = 19, col = alpha("white", 0), cex = 1.5, ylim=c(0,1),
            xaxt = "n", bty = "n", ylab = "Proportion of Blackberries",
            xlab = "Individual Simulated Samples at n=1"
            )
  lines(c(0,100), rep(berry.prob, 2), col = the.lightcols[2], lwd = the.lwd)
  points(unlist(berry.sims["1"]),
            pch = 19, col = alpha(the.cols[1], 0.4), cex = 1.5)
  text(x = 70, y = 0.35, paste0("True proportion: ", berry.prob), col = the.cols[2])
@

That's \Sexpr{length(which(unlist(berry.sims["1"])==0))} samples of size $n=1$ containing no blackberry (proportion $q=0$) and \Sexpr{berry.reps-length(which(unlist(berry.sims["1"])==0))} containing a blackberry (proportion $q=1$).
The individual samples do the best they can to approximate the true proportion of $\Sexpr{berry.prob}$, but at $n=1$, the options are limited.
In this extreme case, however, the \textit{proportion of samples} where there was a blackberry (\Sexpr{(berry.reps-length(which(unlist(berry.sims["1"])==0)))/berry.reps}) approximates the true value reasonably well.

We now increase the sample size step by step.
At each increment, we simulate \Sexpr{berry.reps} replications and plot the proportion of blackberries per replication in Figure~\ref{fig:berrythreetoten}.

<<berrythreetoten, fig.cap=paste0("Proportions measured in 100 replications at n=3, n=4, n=5, and n=10 with a true proportion of 0.3"), echo=F, fig.pos="t", out.width="85%", cache=T>>=
  par(mfrow=c(2,2), mar = c(2,2,2,2))
  for (i in c(3,4,5,10)) {
    p <- plot(unlist(berry.sims[as.character(i)]),
              pch = 19, col = alpha("white", 0), cex = 1.5, ylim=c(0,1),
              xaxt = "n", bty = "n", ylab = "",
              xlab = "", main = paste0("n=", i)
              )
    lines(c(0,100), rep(berry.prob, 2), col = the.lightcols[2], lwd = the.lwd)
    points(unlist(berry.sims[as.character(i)]),
              pch = 19, col = alpha(the.cols[1], 0.4), cex = 1.5)
  }
 par.defaults()
@

We don't need to analyse these results numerically.
It should be immediately obvious that the samples approximate the true value more reliably.
With very small samples (up to $n=5$ in this example), results are still very limited in their possible outcomes, and it's impossible to hit the true proportion of \Sexpr{berry.prob} precisely.
But most of the samples drift towards the best approximation possible.
(Keep in mind that each dot represents ons sample at the respective sample size.)
With $n=10$, the sample proportions start to form a clearly distinguishable cloud around the true value.
Let's see what happens with samples of substantial size in Figure~\ref{fig:berrybig}.

<<berrybig, fig.cap=paste0("Proportions measured in 100 replications at n=20, n=50, n=100, and n=1000 with a true proportion of 0.3"), echo=F, fig.pos="t", out.width="85%", cache=T>>=
  par(mfrow=c(2,2), mar = c(2,2,2,2))
  for (i in c(20, 50, 100, 1000)) {
    p <- plot(unlist(berry.sims[as.character(i)]),
              pch = 19, col = alpha("white", 0), cex = 1.5, ylim=c(0,1),
              xaxt = "n", bty = "n", ylab = "",
              xlab = "", main = paste0("n=", i)
              )
    lines(c(0,100), rep(berry.prob, 2), col = the.lightcols[2], lwd = the.lwd)
    points(unlist(berry.sims[as.character(i)]),
              pch = 19, col = alpha(the.cols[1], 0.4), cex = 1.5)
  }
 par.defaults()
@

While it's not at all impossible to sample 0 or 1000 blackberries if the true proportion of blackberries is \Sexpr{berry.prob}, such a result becomes extremely rare event at this sample size.
We see in the lower-right panel of Figure~\ref{fig:berrybig} that even results lower than 0.2 or higher than 0.4 are so rare that none occurred in the $\Sexpr{berry.reps}$ replications.
For a binary (or nominal or ordinal) variable, larger samples approximate the true proportion more reliably.
In the next section, we try the same with a numeric variable.

\subsection{Sampling Milliseconds: One, Two, Three, Many}\label{sec:milli}

We now turn to the per-word reading speed from the Problem Statement.
Before we run the simulation, we have to set the parameters that define the simulated reality.
Let's assume that the real word-length-adjusted reading speed is at $\mu=\Sexpr{rt.mean}$ (in milliseconds) per word, and the standard deviation is $\sigma=\Sexpr{rt.sd}$.
With the simulations, we can now ask what will the outcomes be if we take many samples of size $n=1$, $n=2$, etc.
First take a look at Figure~\ref{fig:rts}.

<<rts, fig.cap=paste0("Raw data plot of a single random sample with n=100 from a DGP that has μ=", rt.mean, " and σ=", rt.sd), echo=F, out.width="85%", fig.pos="t", cache=F>>=

# par(mfrow=c(2,1), mar = c(4,3,4,2))
#
# rt.minmax <- c(40, 200)
# rt.dnorm <- function(x) {dnorm(x, mean = rt.mean, sd = rt.sd)}
#
# plot(x = seq(rt.minmax[1], rt.minmax[2], 0.1),
#      y = rt.dnorm(seq(rt.minmax[1], rt.minmax[2], 0.1)),
#      type = "l", lwd = 0.5, col = "darkgray", main = "Probability Density",
#      bty="n", xaxt="n", yaxt = "n", xlab = "Value (in ms)", ylab = "Density",
#      xlim = c(rt.minmax[1], rt.minmax[2])
# )
# axis(1, at = seq(rt.minmax[1], rt.minmax[2], 20))
# lines(x = c(rt.mean, rt.mean), y = c(0, rt.dnorm(rt.mean)),
#       lwd = the.lwd, col = the.lightcols[2])
# lines(x = c(rt.mean-rt.sd, rt.mean-rt.sd), y = c(0, rt.dnorm(rt.mean-rt.sd)),
#       lwd = the.lwd, col = the.lightcols[1])
# lines(x = c(rt.mean+rt.sd, rt.mean+rt.sd), y = c(0, rt.dnorm(rt.mean+rt.sd)),
#       lwd = the.lwd, col = the.lightcols[1])
# lines(x = seq(rt.minmax[1], rt.minmax[2], 0.1),
#      y = rt.dnorm(seq(rt.minmax[1], rt.minmax[2], 0.1)),
#      lwd = the.lwd, col = "darkgray"
#      )
#
# text(x = rt.mean+5, y = rt.dnorm(rt.mean)/2, labels = "μ", col = the.cols[2])
# text(x = (rt.mean-rt.sd)+7.5, y = rt.dnorm(rt.mean-rt.sd)/3, labels = "μ-σ",
#      col = the.cols[1])
# text(x = (rt.mean+rt.sd)+7.5, y = rt.dnorm(rt.mean+rt.sd)/3, labels = "μ+σ",
#      col = the.cols[1])

p <- plot(.normsample,
          pch = 19, col = alpha("white", 0), cex = 1.5, ylim=c(60,170),
          xaxt = "n", bty = "n",
          xlab = "Individual data points", main = "",
          ylab = "Value (in ms)"
          )
lines(c(0,100), rep(rt.mean, 2), col = the.lightcols[2], lwd = the.lwd)
points(.normsample,
          pch = 19, col = alpha(the.cols[1], 0.4), cex = 1.5)
@

It shows a single sample with $n=100$ generated by a DGP that is set to $\mu=\Sexpr{rt.mean}$ and $\sigma=\Sexpr{rt.sd}$.
The dots are single data points.
Values close to $\mu=\Sexpr{rt.mean}$ have the highest probability (and hence the highest frequency in repeated sampling).
The average deviation of the values is at $\mu-\sigma=\Sexpr{rt.mean-rt.sd}$ and at $\mu+\sigma=\Sexpr{rt.mean+rt.sd}$.
Since this is just the average deviation, we do not expect there to be a higher number of measurements around these two points, of course.
They're just the centre points of the negative and positive deviations, respectively.
Figure~\ref{fig:rtsmall} shows what happens in \Sexpr{rt.reps} simulated samples at sizes of $n=1$, $n=3$, $n=5$, and $n=10$.

<<rtsmall, fig.cap=paste0("Means measured in 100 replications at n=1, n=3, n=5, and n=10 with a true mean of σ=120 and a true standard deviation of μ=20"), echo=F, fig.pos="t", out.width="85%", cache=T>>=
  par(mfrow=c(2,2), mar = c(2,2,2,2))
  for (i in c(1,3,5,10)) {
    p <- plot(unlist(rt.sims[as.character(i)]),
              pch = 19, col = alpha("white", 0), cex = 1.5, ylim=c(60,160),
              xaxt = "n", bty = "n", ylab = "",
              xlab = "", main = paste0("n=", i)
              )
    lines(c(0,100), rep(rt.mean, 2), col = the.lightcols[2], lwd = the.lwd)
    points(unlist(rt.sims[as.character(i)]),
              pch = 19, col = alpha(the.cols[1], 0.4), cex = 1.5)
  }
 par.defaults()
@

We see a very similar effect as in the case of the binary variable.
At a sample size of $n=1$, the mean is virtually identical to a single sampled value.
Therefore, the distribution of sample means in the upper left panel of Figure~\ref{fig:rtsmall} looks like a distribution of individual data points in Figure~\ref{fig:rts}.%
\footnote{Keep in mind: The dots in Figure~\ref{fig:rts} represent individual data points.
The dots in Figure~\ref{fig:rtsmall} represent individual sample means calculated from samples (of different sizes per panel).}
The larger the sample, the closer the sample means move (on average) towards the real value as the variance between single data points plays an ever smaller role.
This trend continues with much larger samples, see Figure~\ref{fig:rtsbig}.

<<rtsbig, fig.cap=paste0("Means measured in 100 replications at n=20, n=50, n=100, and n=1000 with a true mean of σ=120 and a true standard deviation of μ=20"), echo=F, fig.pos="t", out.width="85%", cache=T>>=
  par(mfrow=c(2,2), mar = c(2,2,2,2))
  for (i in c(20, 50, 100, 1000)) {
    p <- plot(unlist(rt.sims[as.character(i)]),
              pch = 19, col = alpha("white", 0), cex = 1.5, ylim=c(60,160),
              xaxt = "n", bty = "n", ylab = "",
              xlab = "", main = paste0("n=", i)
              )
    lines(c(0,100), rep(rt.mean, 2), col = the.lightcols[2], lwd = the.lwd)
    points(unlist(rt.sims[as.character(i)]),
              pch = 19, col = alpha(the.cols[1], 0.4), cex = 1.5)
  }
 par.defaults()
@

Why did we begin with samples of size $n=1$?
After all, it's a ridiculous sample size.
We did it because it allowed us to illustrate how a sample approximates parameters like the mean increasingly better than a single measurement.
A single value (equivalently a sample with $n=1$) approximates a parameter of the DGP with a certain reliability, and the reliability is higher the lower the variance in the DGP is.
The variance determines how far single data points bounce around the actual parameter, so with larger variance comes higher uncertainty.
However, the more such single data points you have in your sample, the more the bouncing around averages out.
It's still possible to draw a very extreme sample, but the probability of drawing such a sample gets lower and lower with larger samples.

We hope that our illustration was intuitive enough and laid the foundations for the statistics introduced in Section~\ref{sec:variancesamplemeans} and \ref{sec:variabilitysampleproportions}.
The way we introduced the idea of sampling variation---we hope---has made it clear that it's a frequentist concept.
If we took a lot of samples (100 in the examples above), then we'd see the patterns that we saw in the plots.
Nothing can be known for certain or with a quantifiable reliability from a single sample.
Please keep this in mind.
The Statistics Wars were fought about misunderstandings of this simple fact (see Chapter~\ref{sec:powerseverity}).

\Bigpoint{Sample Size in Parameter Estimation}{%
When we use a sample to estimate a population parameter, we can rely on some simple facts about variance and sample size.
A larger variance in the data-generating process (DGP) makes estimates less accurate.
On the other hand, larger samples average over the variance that makes the individual data points bounce around the true parameter.
Hence, parameter estimates from larger samples have a better chance of estimating the true parameter more accurately.
However, no amount of quantifiable certainty about the true parameter can ever be gained from any sample!
}

\section{Variance of Sample Means}\label{sec:variancesamplemeans}

\subsection{Sampling Intervals for Normal Numeric Measurements}

\subsubsection{The Variance of Sample Means}

Now we know that each increment in sample size counteracts the variance that's inherent in the data points.
In other words, samples have (on average, \ie, under repeated sampling) the same or less variance than the raw data points.
They only have the same variance if $n=1$, and even with $n=2$, the variance already decreases.
As it happens, we can express this mathematically and at the same time very intuitively for means like so:

\begin{center}
  \begin{math}
    \text{variance of sample means at sample size }n=\cfrac{s^2}{n}
  \end{math}
\end{center}

The variance of the sample means is the variance of the individual measurements $s^2$ divided by the sample size $n$.
Hence, when the variance among the individual measurements goes up, the variance among the sample means goes up.
On the other hand, when the sample size goes up, the variance among the sample means goes down.%
\footnote{Notice that, from this angle, we can answer \textit{yes} to the questions from the Problem Statement asking something like:
\textit{Would a sample 10 times larger be 10 times more accurate?}}
In order to get the \textit{average deviation of sample means from the true mean at sample size $n$}, we need to take the square root.
In Section~\ref{sec:quantifyvariance}, we did the same to calculate the standard deviation from the variance.
The result is called the \Key{standard error} of the mean $\ssigma_{\mu}$.
It's simply the standard deviation of sample means at a given sample size $n$ and given the variance among the individual data points $s^2$.
The formula in Equation~\ref{eq:semean} is often expressed by the equivalent Equation~\ref{eq:semeanalt}, but we consider the first variant more transparent and easier to memorise.

\begin{equation}
   \ssigma_{\mu}=\sqrt{\cfrac{\sigma^2}{n}}
   \label{eq:semean}
\end{equation}

\begin{equation}
   \ssigma_{\mu}=\cfrac{\sigma}{\sqrt{n}}
   \label{eq:semeanalt}
\end{equation}

We have omitted one important detail.
The above is only perfectly true for data that follow the \Key{Normal Distribution} or \Key{Gaussian Distribution}.
A normally distributed random variable has a characteristic probability density function.
It has two parameters: the mean $\mu$ and the standard deviation $\sigma$.
Figure~\ref{fig:normal} shows a plot of the normal probability density with $\mu=0$ and $\sigma=1$.

<<normal, fig.cap=paste0("Theoretical population distribution for a normal distribution with μ=0 and σ=1"), echo=F, out.width="85%", fig.pos="t", cache=F>>=

normal.mu <- 0
normal.sigma <- 1
normal.minmax <- c(-3, 3)
normal.dnorm <- function(x) {
  dnorm(x, mean = normal.mu, sd = normal.sigma)
}

plot(x = seq(normal.minmax[1], normal.minmax[2], 0.1),
     y = normal.dnorm(seq(normal.minmax[1], normal.minmax[2], 0.1)),
     type = "l", lwd = the.lwd,
     bty="n", ylab = "Probability Density", xlab = "Value (x)",
     xlim = normal.minmax, col = "white"
)
lines(x = c(normal.mu, normal.mu), y = c(0, normal.dnorm(normal.mu)),
      lwd = the.lwd, col = the.lightcols[2])
lines(x = c(normal.mu-normal.sigma, normal.mu-normal.sigma), y = c(0, normal.dnorm(normal.mu-normal.sigma)),
      lwd = the.lwd, col = the.lightcols[1])
lines(x = c(normal.mu+normal.sigma, normal.mu+normal.sigma), y = c(0, normal.dnorm(normal.mu+normal.sigma)),
      lwd = the.lwd, col = the.lightcols[1])
lines(x = seq(normal.minmax[1], normal.minmax[2], 0.1),
     y = normal.dnorm(seq(normal.minmax[1], normal.minmax[2], 0.1)),
     type = "l", lwd = the.lwd, col = "darkgray"
)
text(x = normal.mu+0.2, y = normal.dnorm(normal.mu)/2, labels = "μ", col = the.cols[2])
text(x = (normal.mu-normal.sigma)+0.3, y = normal.dnorm(normal.mu-normal.sigma)/3, labels = "μ-σ",
     col = the.cols[1])
text(x = (normal.mu+normal.sigma)+0.3, y = normal.dnorm(normal.mu+normal.sigma)/3, labels = "μ+σ",
     col = the.cols[1])
@

You've probably seen the Normal Distribution before with the characteristic bell-shaped curve of its density function.
Not only do many numeric measurements in nature follow this distribution, it also pops up in an almost creepy way in a family of fundamental proofs in statistics, the so-called \textit{Central Limit Theorem} (which has a lot to do with what we're in the middle of introducing here).
Be that as it may, for samples from normally distributed data, the simple formula for the standard error of the mean given above is provably correct.
Notice that the density curve is fully defined once the mean and the standard deviation are given (see also the in-depth Section~\ref{sec:gaussian}).
This means that we can calculate the probability for each value of $x$ (or rather each interval on the x-axis).
For example, we can calculate the probability of $x\ge 1$, written: $Pr(x\ge 1)$.
As the area under any probability density curve is 1, $Pr(x\ge 1)$ is the integral of the normal density from 1 to infinity.
Piece of cake.
Let's have that piece of cake now (step by step) and call it the Sampling Interval.

<<rtsnormal, fig.cap=paste0("Density of a normal distribution with μ=", rt.mean, " and σ=", rt.sd), echo=F, out.width="85%", fig.pos="t", cache=F>>=

rt.minmax <- c(60, 180)
rts.dnorm <- function(x) {
  dnorm(x, mean = rt.mean, sd = rt.sd)
}
rts.stderr <- function(x, n) {
  dnorm(x, mean = rt.mean, sd = rt.sd/sqrt(n))
}

plot(x = seq(rt.minmax[1], rt.minmax[2], 0.1), ylim = c(0, 0.1),
     y = rts.dnorm(seq(rt.minmax[1], rt.minmax[2], 0.1)),
     type = "l", lwd = the.lwd, yaxt="n",
     bty="n", ylab = "", xlab = "Value (x)",
     xlim = rt.minmax, col = "white"
)
lines(x = seq(rt.minmax[1], rt.minmax[2], 0.1),
     y = rts.dnorm(seq(rt.minmax[1], rt.minmax[2], 0.1)),
     type = "l", lwd = the.lwd, col = "darkgray"
)

# StdErrs

.c <- 0
for (n in c(2, 10, 20)) {
  lines(x = seq(rt.minmax[1], rt.minmax[2], 0.1),
       y = rts.stderr(seq(rt.minmax[1], rt.minmax[2], 0.1), n),
       type = "l", lwd = the.lwd, col = darken(the.lightcols[2], .c)
  )
  .c <- .c + 0.2
}

legend("topleft", legend = c("Data", "Means at n=2", "Means at n=10", "Means at n=20"), lwd = the.lwd, bty = "n",
       col = c("darkgray", the.lightcols[2], darken(the.lightcols[2], 0.2), darken(the.lightcols[2], 0.4))
)
@

First, Figure~\ref{fig:rtsnormal} shows the theoretical normal density plot of the reading times in milliseconds as introduced in Section~\ref{sec:milli}.%
\footnote{In denisty plots, we omit y axis labels whenever possible as they are rarely interpreted directly.}
We've added three density functions for the distributions of means at sample sizes 2, 10, and 20.
With larger samples, the probability mass piles up around the mean very quickly, and the normal curve narrows sharply.
This graph illustrates again what we've shown before:
The sample means are centred around the true mean, and they become better and better estimators of the mean with increasing sample size.

Let's focus on one of those curves, the density function of the means at $n=10$.
It's shown in Figure~\ref{fig:meandensten}.
In addition, the centre area under the curve that spans from the mean $1SE$ to the left and to the right ($\mu\pm 1SE$) is shaded in blue.
The are covering $\mu\pm 2SE$ is shaded in green.
As the Normal Distribution has known mathematical properties and we know the real mean and the real standard deviation, we can calculate the shaded areas.
The blue area is approximately $\Sexpr{round(pnorm(1, 0, 1, T)-pnorm(-1, 0, 1, T), 2)}$, \ie, $\Sexpr{round(pnorm(1, 0, 1, T)-pnorm(-1.01, 0, 1, T), 2)*100}$\% of the whole area under the curve.
The green area (plus the blue area) is approximately $\Sexpr{round(pnorm(2, 0, 1, T)-pnorm(-2.01, 0, 1, T), 2)}$, \ie, $\Sexpr{round(pnorm(2, 0, 1, T)-pnorm(-2.01, 0, 1, T), 2)*100}$\%.

<<meandensten, fig.cap=paste0("Density of the distribution of the means at n=20 with μ=", rt.mean, " and σ=", rt.sd, "; are at μ±σ is highlighted"), echo=F, out.width="85%", fig.pos="t", cache=F>>=

rt.se <- rt.sd/sqrt(20)

rtm.minmax <- c(105, 135)
.xs <- seq(rtm.minmax[1], rtm.minmax[2], 0.1)
.ys <- rts.stderr(seq(rtm.minmax[1], rtm.minmax[2], 0.1), 20)
plot(x = .xs, y = .ys,
     type = "l", lwd = the.lwd, yaxt="n",
     bty="n", ylab = "", xlab = "Value (x)",
     xlim = rtm.minmax, col = "white"
)
shade(.xs, .ys, rt.mean-2*rt.se, rt.mean+2*rt.se,
      col = lighten(the.lightcols[1], 0.5),
      border = darken(the.lightcols[1], 0.4)
      )
shade(.xs, .ys, rt.mean-rt.se, rt.mean+rt.se,
      col = lighten(the.lightcols[2], 0.5),
      border = darken(the.lightcols[2], 0.4)
      )
text(120, 0.025, "μ±1SE", col = darken(the.lightcols[2], 0.6))
text(113.5, 0.01, "μ±2SE", col = darken(the.lightcols[2], 0.6))
lines(x = .xs, y = .ys,
     type = "l", lwd = the.lwd, col = darken(the.lightcols[2], 0.4)
)
@

How is this useful?
Think about what it means in terms of probability.
It means that we know for sure that (i)~if the true mean of a variable is $\mu$, (ii)~its true standard deviation is $\sigma$, and (iii)~we draw a sample with $n$, then the probability that the mean of any sample lies in the interval $\mu\pm 1SE$ is $\Sexpr{round(pnorm(1, 0, 1, T)-pnorm(-1.01, 0, 1, T), 2)}$.
Similarly, the probability that the mean of any sample lies in the interval $\mu\pm 2SE$ is $\Sexpr{round(pnorm(2, 0, 1, T)-pnorm(-2.01, 0, 1, T), 2)}$.
The frequentist interpretation consistently equates the probability of an event occurring with the long-run proportion it has in repeated repetitions.
In our notation for probabilities from Section~\ref{sec:probability}, for any sample mean \xmean:

\begin{center}
  \begin{math}
    Pr(\mu-SE_{\mu}\leq\xmean\leq\mu+SE_{\mu})\approx\Sexpr{round(pnorm(1, 0, 1, T)-pnorm(-1, 0, 1, T), 2)}
  \end{math}
\end{center}

\begin{center}
  \begin{math}
    Pr(\mu-2SE_{\mu}\leq\xmean\leq\mu+2SE_{\mu})\approx\Sexpr{round(pnorm(2, 0, 1, T)-pnorm(-2.01, 0, 1, T), 2)}
  \end{math}
\end{center}

The standard error is sometimes also just called $\sigma$ as it is a kind of standard deviation.
We find it best to distinguish clearly between the standard deviation of primary data points $\sigma$ and the standard deviation of sample means, the standard error of the mean $\ssigma_{\mu}$.

\subsubsection{Theoretical Sampling Intervals}

Given that we can calculate the probability of outcomes that lie within a certain interval, we can also ask the inverse question:
\textit{How many standard errors into each direction (from the mean) define the central 90\%, 95\%, 99\%, etc. sample means under long-run repetition?}
That number of standard errors is often called the \Key{z-value} or just $z$.
One can use the a function (called the \textit{quantile function} of the Normal Distribution) for calculating the z-value, but pre-calculated tables usually suffice.
Table~\ref{tab:ztable} is a very sparse example of such a table.

<<ztable, echo=F, cache=T, out.width="85%", results="asis">>=
  ps <- c(0.5,0.8,0.9,0.95,0.99)
  zs <- abs(round(qnorm((1-ps)/2, 0, 1), 2))
  ztable <- data.frame(Pr = ps, Z=zs)

  ztable %>%
    as.data.frame.matrix %>%
    kable(align = c("c", "c"), booktabs = T, linesep = "",
          caption = "Z-Values for some Probabilities",
          label = "ztable") %>%
    row_spec(0, bold=TRUE)
@

Let's go through this again.
Table~\ref{tab:ztable} tells us that 50\% of samples of a normally distributed variable lie within a range around the mean that is 0.67 standard errors wide on both sides of the mean.
Remember that the standard error itself depends only on the variance in the data and the sample size.
80\% of all samples lie within $\mu-1.28$ standard errors and $\mu+1.28$ standard errors, and so on.
This is formalised in the notion of the \Key{sampling interval} for the mean $\confint_{\mu}$, where $z_Q$ stands for the z-value for a given probability\slash proportion $q$ as illustrated in Table~\ref{tab:ztable}:

\begin{equation}
  \confint_{\mu}=\mu\pm z_q\cdot\ssigma_{\mu}
  \label{eq:confintmean}
\end{equation}

<<cisims, cache=FALSE, echo=FALSE, include=FALSE, results='asis'>>=

  set.seed(78)
  # Function for confint simulations.
  ci.sim <- function(n = 100, mu = 0, sigma = 1, z = 1.96, reps = 100) {
    .results <- list()
    for (.i in 1:reps) {
      .sim <- rnorm(n, mu, sigma)
      .var <- var(.sim)
      .mu <- mean(.sim)
      .se <- sqrt(.var/n)
      .lo <- .mu-(.se*z)
      .hi <- .mu+(.se*z)
      .in <- ifelse(mu >= .lo & mu <= .hi, TRUE, FALSE)
      .results[[.i]] <- list(
        "var" = .var,
        "mu" = .mu,
        "se" = .se,
        "low" = .lo,
        "hi" = .hi,
        "in" = .in
      )
    }
    .results
  }

  n.ci <- 20
  z.ci <- 1.96
  reps.ci <- 1000
  ci.sims <- ci.sim(n.ci, rt.mean, rt.sd, z.ci, reps.ci)

  # Get coverage
  cov.ci <- unlist(ci.sims, recursive = F)
  cov.ci <- unname(unlist(cov.ci[which(names(cov.ci)=="in")]))
  cov.ci <- length(which(cov.ci == TRUE))
@

\subsubsection{Empirical Sampling Intervals}

But wait!
In normal empirical work, we don't know the true mean and the true standard deviation.
Some readers might have noticed this very early on and gotten frustrated because it appeared as if sampling intervals were a purely theoretical thing without practical use.
This is not the case, and the argument will finally bring us back to the frequentist logic of inference.
What we do in real life is this:
We calculate a sampling interval based on the statistics of our concrete empirical sample.
That is, we use \xmean\ instead of $\mu$ and $s(\xsample)$ instead of $\sigma$.
After all, $\mu$ and $\sigma$ are unknown.
The calculations are the same, only the symbols change:

\begin{center}
  \begin{math}
   \ssigma_{\xmean}=\sqrt{\cfrac{s^2(\xsample)}{n}}
  \end{math}
\end{center}

\begin{center}
  \begin{math}
    \confint_{\xmean}=\xmean\pm z_q\cdot\ssigma_{\xmean}
  \end{math}
\end{center}

Then, as the inverse of the theoretical sampling interval, the empirical sampling interval contains the true value with the proportion ($q$) corresponding to $z_q$.
We simulated this.
We simulated \Sexpr{reps.ci} samples with sample size $n=\Sexpr{n.ci}$ from the distribution of reading times.
Then, we used the samples to calculate the empirical sampling intervals exclusively based on each sample.
Figure~\ref{fig:cisim} shows the true mean and all those empirical sampling intervals.
The ones that don't include the true mean are coloured orange.

<<cisim, fig.cap=paste0("Empirical sampling intervals from ", reps.ci ," simulated samples; ones that don't contain the true mean marked in orange; μ=", rt.mean, ", σ=", rt.sd, ", z=", z.ci, ", n=", n.ci), echo=F, out.width="85%", fig.pos="t", cache=F>>=
  par(mar=c(1,2,1,1))
  ylim.ci <- c(90, 150)
  plot(0, xlim=c(0, reps.ci), ylim = ylim.ci, col = "white",
       xaxt = "n", bty = "n", xlab = "", ylab = "")
  lines(c(0, reps.ci), c(rt.mean, rt.mean), col = the.lightcols[2], lwd = the.lwd)
  for (i in 1:length(ci.sims)) {
    .col <- ifelse(ci.sims[[i]][["in"]], alpha(the.cols[1], 0.4), alpha(the.cols[3], 0.9))
    lines(c(i, i), c(ci.sims[[i]][["low"]], ci.sims[[i]][["hi"]]), col = .col)
  }
@

The number of empirical sampling intervals in these \Sexpr{reps.ci} replications was \Sexpr{cov.ci}, \ie, \Sexpr{round(cov.ci/reps.ci*100, 1)}\%.
This is called the \Key{coverage}, and it's very close to the expected---or rather \textit{requested}---value of 95\%.
We say \textit{requested} because in deciding to calculate the intervals for a proportion of $q=0.95$ and consequently choosing $z=1.96$, we required that in the long run 95\% of all empirical sampling intervals should contain the true mean.
We've now called it a \textit{proportion}, but think about it again in terms of \textit{probability}.
It's the frequentist probability of drawing a sample whose empirical sampling interval contains the true mean before we actually draw the sample.
Once it has been drawn, the empirical sampling interval either contains the true mean or it doesn't, but that's a fact and has no probability attached to it.

The term \textit{sampling interval} is our invention.
It's usually called the \Key{confidence interval}.
However, there are so many misinterpretations attached to this term that we decided to avoid it.
A sampling interval with $q=0.95$ is called the \textit{95\% confidence interval} (and similar for other values of $q$).
A very wrong way of interpreting an empirical sampling interval then goes like this:

\begin{quote} \itshape
  \Argh The 95\% confidence interval for the mean contains the true mean with 95\% certainty.
  (That means we can be 95\% sure that it contains the true mean.)
\end{quote}

What's this even supposed to mean?
Next time you hear someone say something like this about a frequentist sampling interval, ask them what it means to be \textit{95\% certain of something}.
What it really means is that before taking the sample, there was a probability of 0.95 to draw a sample that contains the true mean (given the chosen $n$ and $z=1.96$).
Now, all we know is that the interval either contains the true mean or a rare event occurred.
Please go back to Section~\ref{sec:pandsig} to convince yourself that an event with a probability of 0.05 is not that rare after all.

\subsubsection{Varying Z and N}

We use sampling intervals to get an estimate of the true mean with a known probability of being wrong.
If we set $z=1.96$, we will estimate 95\% of all intervals correctly in the long run.
Setting $z=2.58$ gives us even better chances of getting the estimate right.
So why don't we always set $z=5$ or something really high?
Look at Figure~\ref{fig:cisim}.
If we increased $z$, the samples themselves wouldn't change.
However, we'd require that more intervals should include the true mean, and the only way to do that is to make the intervals wider.
This leads to a higher rate of correct estimates, but the estimates become more imprecise.

First, let's calculate empirical sampling intervals with $z=1.96$ and $z=2.58$, before looking at some more illustrative plots.
A random replications of the simulation led to the following statistics:
$\xmean=\Sexpr{round(ci.sims[[333]][["mu"]], 2)}$ and $s^2(\xsample)=\Sexpr{round(ci.sims[[333]][["var"]], 2)}$.
The $\ssigma_{\mu}$ is calculated as follows:

\begin{center}
  \begin{math}
    \ssigma_{\xmean}=\sqrt{\cfrac{\Sexpr{round(ci.sims[[333]][["var"]], 2)}}{\Sexpr{n.ci}}}=\Sexpr{round(sqrt(round(ci.sims[[333]][["var"]], 2)/n.ci), 2)}
  \end{math}
\end{center}

The 95\% sampling interval (\ie, $z=1.96$) and the 99\% sampling interval (\ie, $z=2.58$) are:

\begin{center}
  \begin{math}
    SI_{\xmean,0.95}=\Sexpr{round(ci.sims[[333]][["mu"]], 2)}\pm 1.96\cdot\Sexpr{round(sqrt(round(ci.sims[[333]][["var"]], 2)/n.ci), 2)}=\langle\Sexpr{round(round(ci.sims[[333]][["mu"]], 2)-1.96*round(sqrt(round(ci.sims[[333]][["var"]], 2)/n.ci), 2), 2)}, \Sexpr{round(round(ci.sims[[333]][["mu"]], 2)+1.96*round(sqrt(round(ci.sims[[333]][["var"]], 2)/n.ci), 2), 2)} \rangle
  \end{math}

  \begin{math}
    SI_{\xmean,0.99}=\Sexpr{round(ci.sims[[333]][["mu"]], 2)}\pm 2.58\cdot\Sexpr{round(sqrt(round(ci.sims[[333]][["var"]], 2)/n.ci), 2)}=\langle\Sexpr{round(round(ci.sims[[333]][["mu"]], 2)-2.58*round(sqrt(round(ci.sims[[333]][["var"]], 2)/n.ci), 2), 2)}, \Sexpr{round(round(ci.sims[[333]][["mu"]], 2)+2.58*round(sqrt(round(ci.sims[[333]][["var"]], 2)/n.ci), 2), 2)} \rangle
  \end{math}
\end{center}

The interval is roughly 6ms larger for the added 4\% success rate.
For a more comprehensive impression of the effect of manipulating $z$ and the effect of the sample size $n$, we provide Figure~\ref{fig:fourcisims}.

<<fourcisims, fig.cap=paste0("Simulations of empirical sampling intervals for different z and n with 100 replications each"), echo=F, out.width="85%", fig.pos="t", cache=F>>=

  par(mfrow = c(2,2), mar=c(1,2,3,1))

  set.seed(92)
  n.ci <- 20
  z.ci <- 1.96
  reps.ci <- 100

  ci.sims <- ci.sim(n.ci, rt.mean, rt.sd, z.ci, reps.ci)
  cov.ci <- unlist(ci.sims, recursive = F)
  cov.ci <- unname(unlist(cov.ci[which(names(cov.ci)=="in")]))
  cov.ci <- length(which(cov.ci == TRUE))

  ylim.ci <- c(100, 140)
  plot(0, xlim=c(0, reps.ci), ylim = ylim.ci, col = "white",
       xaxt = "n", bty = "n", xlab = "", ylab = "", main = paste0("n=", n.ci, " z=", z.ci, "\nCoverage ", cov.ci, "%"))
  lines(c(0, reps.ci), c(rt.mean, rt.mean), col = the.lightcols[2], lwd = the.lwd)
  for (i in 1:length(ci.sims)) {
    .col <- ifelse(ci.sims[[i]][["in"]], alpha(the.cols[1], 0.4), alpha(the.cols[3], 0.9))
    lines(c(i, i), c(ci.sims[[i]][["low"]], ci.sims[[i]][["hi"]]), col = .col, lwd = the.lwd)
  }

  n.ci <- 100

  # Duplicated code!!!
  ci.sims <- ci.sim(n.ci, rt.mean, rt.sd, z.ci, reps.ci)
  cov.ci <- unlist(ci.sims, recursive = F)
  cov.ci <- unname(unlist(cov.ci[which(names(cov.ci)=="in")]))
  cov.ci <- length(which(cov.ci == TRUE))
  plot(0, xlim=c(0, reps.ci), ylim = ylim.ci, col = "white",
       xaxt = "n", bty = "n", xlab = "", ylab = "", main = paste0("n=", n.ci, " z=", z.ci, "\nCoverage ", cov.ci, "%"))
  lines(c(0, reps.ci), c(rt.mean, rt.mean), col = the.lightcols[2], lwd = the.lwd)
  for (i in 1:length(ci.sims)) {
    .col <- ifelse(ci.sims[[i]][["in"]], alpha(the.cols[1], 0.4), alpha(the.cols[3], 0.9))
    lines(c(i, i), c(ci.sims[[i]][["low"]], ci.sims[[i]][["hi"]]), col = .col, lwd = the.lwd)
  }

  n.ci <- 20
  z.ci <- 2.58

  # Duplicated code!!!
  ci.sims <- ci.sim(n.ci, rt.mean, rt.sd, z.ci, reps.ci)
  cov.ci <- unlist(ci.sims, recursive = F)
  cov.ci <- unname(unlist(cov.ci[which(names(cov.ci)=="in")]))
  cov.ci <- length(which(cov.ci == TRUE))
  plot(0, xlim=c(0, reps.ci), ylim = ylim.ci, col = "white",
       xaxt = "n", bty = "n", xlab = "", ylab = "", main = paste0("n=", n.ci, " z=", z.ci, "\nCoverage ", cov.ci, "%"))
  lines(c(0, reps.ci), c(rt.mean, rt.mean), col = the.lightcols[2], lwd = the.lwd)
  for (i in 1:length(ci.sims)) {
    .col <- ifelse(ci.sims[[i]][["in"]], alpha(the.cols[1], 0.4), alpha(the.cols[3], 0.9))
    lines(c(i, i), c(ci.sims[[i]][["low"]], ci.sims[[i]][["hi"]]), col = .col, lwd = the.lwd)
  }

  n.ci <- 100

  # Duplicated code!!!
  ci.sims <- ci.sim(n.ci, rt.mean, rt.sd, z.ci, reps.ci)
  cov.ci <- unlist(ci.sims, recursive = F)
  cov.ci <- unname(unlist(cov.ci[which(names(cov.ci)=="in")]))
  cov.ci <- length(which(cov.ci == TRUE))
  plot(0, xlim=c(0, reps.ci), ylim = ylim.ci, col = "white",
       xaxt = "n", bty = "n", xlab = "", ylab = "", main = paste0("n=", n.ci, " z=", z.ci, "\nCoverage ", cov.ci, "%"))
  lines(c(0, reps.ci), c(rt.mean, rt.mean), col = the.lightcols[2], lwd = the.lwd)
  for (i in 1:length(ci.sims)) {
    .col <- ifelse(ci.sims[[i]][["in"]], alpha(the.cols[1], 0.4), alpha(the.cols[3], 0.9))
    lines(c(i, i), c(ci.sims[[i]][["low"]], ci.sims[[i]][["hi"]]), col = .col, lwd = the.lwd)
  }

  par.defaults()
@

As expected, a larger $n$ makes the intervals smaller because it makes the standard error smaller (see Section~\ref{sec:milli}).
A larger $z$, on the other hand, make the intervals larger because we require that the probability of the interval including the true mean be higher.
The fact that the coverage does not always land exactly at 95\% or 99\% is not surprising.
A perfect match between the theoretical expectation and the empirical reality is guaranteed only in the limit, \ie, if we take infinitely many samples.
After a short discussion of the Normal Distribution, we will briefly demonstrate how sampling intervals for proportions work.
The idea is essentially the same.

\Bigpoint{Interpretation of Sampling Intervals}{%
A sampling interval for the mean is an attempt to estimate the true mean.
It provides an interval which covers the true mean with a specified frequentist pre-sample probability.
For example, a 95\% sampling interval ($q=0.95$) means that before the sample is drawn, it has a probability of 0.95 to actually include the true mean.
Once it is drawn, it either includes the true mean, or a (relatively) rare event has occurred.
If safe estimates are of the essence, one should increase $q$ (for example, $q=0.99$ for the 99\% sampling interval).
This lowers the error rate in the long run and makes the estimation \textit{safer}.
A safer estimates increase the size of the intervals and makes it less \textit{precise} (larger intervals).
Changing the sample size does not change the safety of the estimate.
Hence, if \textit{both a precise and a safe} estimation is required, one should increase $q$ and also increase the sample size $n$.
Finally, sampling intervals (also called confidence intervals) have nothing to do with specifying the degree of certainty that the true mean falls within the interval.
}

\subsection{\Indepth\ The Normal Distribution}\label{sec:gaussian}

\begin{equation}
  c\cdot e^{-\cfrac{(x - \mu)^2}{2\sigma^2}}
  \label{eq:normal}
\end{equation}

\begin{equation}
  c=\frac{1}{\sqrt{2\pi\sigma^2}}
  \label{eq:normalscaler}
\end{equation}

\section{Sampling Intervals for Binary Measurements}\label{sec:variabilitysampleproportions}

\begin{equation}
   \ssigma_{\rho}=\sqrt{\cfrac{\rho\cdot(1-\rho)}{n}}
   \label{eq:seprop}
\end{equation}

\begin{equation}
  \confint_{\rho}=\rho\pm z\cdot\ssigma_{\rho}
  \label{eq:confintprop}
\end{equation}
