% !Rnw root = ../main.Rnw
<<setupdescribing, cache=T, echo=FALSE, include=FALSE, results='asis'>>=
opts_knit$set(self.contained=FALSE)
@

\chapter{Data: Central Tendency and Variance}
\label{sec:describing}

\section*{Overview}

\enlargethispage{2\baselineskip}

Chapter~\ref{sec:fisher} introduced the logic of frequentist inference using the relatively simple Fisher Exact Test as an example.
In order to extend the same logic to other use cases and develop more advanced versions of the same logic, we need to take a step back in this Chapter.
Many other tests will require knowledge of measures of central tendency like the mean (or average) and the variance.
Variance is a statistic that quantifies how strongly single values deviate from the mean.
It helps us to deal with a central reason we need inferential statistics in the first place:
Not all measured values are the same.
People don't all have the same weight, sentences don't all have the same length, etc.
Hence, variance is one of the most crucial concepts in statistics, and it's one of the reasons that making inferences about virtually infinite populations based on finite or even small samples is hard.

While many statistics text books make a huge brouhaha about so-called \textit{descriptive statistics} and \textit{levels of measurement}, we focus those two core concepts (the central tendency and the variance of measurements).
We begin by characterising different types of measurements (which are often called \alert{levels of measurement}).
We discuss ways of finding the most characteristic measurements in a sample or a population by finding the \alert{mode}, \alert{median}, \alert{quartiles} and \alert{percentiles}, and the \alert{mean} for different types of measurements.
Finally, we quantify how much single measurements deviate from the most characteristic measurements in the form of the \alert{variance} and the \alert{standard deviation}.
Along the way, some types of plots are introduced which provide insight into the structure of data.
These are \alert{raw data plots}, \alert{histograms}, \alert{box plots}, \alert{density plots}, and \alert{violinplots}.

\Problem{Show Me Your Sample!}{%
Imagine you've collected some data.
It might be counts of words and constructions from a corpus, acceptability judgements for relative clause constructions on a five-point-scale, word or sentence lengths, reaction times in milliseconds.
Ultimately, you want to make an inference, but first you want to get an overview of your data set and explore it in order to see whether it roughly meets your expectations.
Furthermore, you want to publish the results and provide your readers with an overview of the results, as most of them will not look at your raw data.
Besides all kinds of plots that aggregate your data visually, you might want to characterise your sample numerically.
What's the best numerical characterisation for your kind of data.}

\section{Central Tendency and Typical Values}\label{sec:centraltendency}

\subsection{Binary Measurements}\label{sec:binary}

If you go through a corpus of English and look at each noun, it could be a singular or a plural form.
We use a paragraph from the Wikipedia article on Critical Rationalism as an example.%
\footnote{Sourced from \url{https://en.wikipedia.org/wiki/Critical_rationalism} on 19 February 2025.}

\begin{quote}
  \itshape Critical \alert{rationalists} hold that scientific \alert{theories} and any other \alert{claims} to \alert{knowledge} can and should be rationally criticized, and (if they have empirical \alert{content}) can and should be subjected to \alert{tests} which may falsify them. Thus \alert{claims} to \alert{knowledge} may be contrastingly and normatively evaluated. They are either falsifiable and thus empirical (in a very broad \alert{sense}), or not falsifiable and thus non-empirical.
\end{quote}

\noindent In such a case, the measurement consists of a sequence of two possible measurements: singular or plural.
The sequence for the highlighted nouns in the sample is:

\begin{center}
  \itshape plural, plural, plural, singular, singular, plural, plural, singular, singular
\end{center}

A main point of this chapter is to introduce the lingo used to describe the properties of such observations in experimental design and statistics.
A measurement in an experiment is always characterised by a well-defined \Key{variable}.
A variable doesn't always take the form of a numerical measurement (although it can).
In any case, it's a description of some property of events.
Turning to the example, if a noun occurs, the noun's grammatical number can be characterised as either singular or plural, the two possible \alert{values} or \Key{levels} of the variable \textit{Number}.
In other words, all events of noun occurrence in English fall into one of two categories: singular or plural.
We need some type of formal representation for a series of measurements of such values.
We name it \xsample\ in boldprint and regard it as a \Key{tuple}, customarily specified in angled brackets $\langle~\rangle$:

\begin{quote}
  $\xsample=\langle$\textit{plural, plural, plural, singular, singular, plural, plural, singular, singular}$\rangle$
\end{quote}

A tuple is a set-theoretic structure that (i)~has an order, (ii)~allows for the same element to occur more than once.
Think of it as a list of items where the same item can appear as many times as necessary.
It would be perfectly okay to disregard the order of the events because we (albeit often incorrectly, see page~\pageref{abs:independent}) assume that the events are independent.
From that perspective, we could use a \Key{set} (which is unordered by definition) instead of a tuple, symbolised by curly brackets $\{~\}$ instead of angled brackets $\langle~\rangle$.
By definition, however, sets don't allow the same element to be added to them twice, which would clearly be inadequate for series of data points such as \xsample.
We call the individual measurements in \xsample\ $x_i$, where $i$ is an \Key{index variable} that takes on integer values.
The first element in \xsample\ is denoted $x_1$, the second element $x_2$, and the last element is always $x_n$, where $n$ is the sample size.
In our example, $n=9$ and hence $x_9$ is the last element in \xsample.
Without digging any deeper into the murky waters of set theory and adjacent areas, we leave it at that.
Tuples are called \textit{lists} in Python, \textit{vectors} in R, \textit{arrays} in more substantive programming languages like C++ or Ada, etc.

How are the corresponding results best summarised?
Mathematically, one of the few things we can do with these occurrences is to count them, which is exactly what we did with the variables \textit{Bristow} and \textit{Reality} in Chapter~\ref{sec:fisher}, both having the possible values \textit{tea-first} and \textit{milk-first} (\eg, Table~\ref{tab:teamarathon}).
For completeness, we show the very obvious summarisation of these counts in tabular form (Table~\ref{tab:counts}).
However, we have already shown the more complex type of contingency tables, where the co-occurrences of the levels of two two-level variables per event are tabulated.
Hence, Table~\ref{tab:counts} should seem quite unsophisticated to most readers.

<<counts, echo=F, cache=T, results="asis">>=
  counts <- matrix(c(4,5), nrow = 1)
  colnames(counts) <- c("Singular", "Plural")
  rownames(counts) <- c("Count")

  counts %>%
    as.data.frame.matrix %>%
    kable(align = c("c", "c"), booktabs = T, linesep = "",
          caption = "A tabulation of counts of a binary variable",
          label = "counts") %>%
    column_spec(1, bold = T) %>%
    row_spec(0, bold=TRUE)
@

Obviously, the variable \textit{Number} is two-level for languages without duals, trials, etc.\ such as English.
It is called a \Key{binary variable} or \textit{dichotomous variable}.
In order to summarise binary data in a single number, we can calculate a proportion, an operation which we assume is well-known (but see below for the general formula).
The proportion of singular noun forms in the above sample is $4\div (4+5)=4\div 9\approx 0.4444$.
Note that proportions always lie between 0 and 1.
A \Key{percentage} is simply a proportion times 100, hence the percentage of singular forms is $0.4444\cdot 100=44.44\%$.
Consequently, a percentage always lies between 0 and 100.
As we'll demonstrate very clearly in Chapter~\ref{sec:confidence}, proportions and percentages are fine, but they can be meaningless if the \Key{sample size} is unknown.
For binary variables, the sample size $n$ is simply the number of events, hence $n=9$ in the example.
Using the symbol $q$ for proportions (as $p$ is already taken and proportions are always a kind of \textit{quotient}), it should be obvious that for any count $c_l$ of a level $l$ of a binary variable (such as $c_{\text{singular}}=4$ for \textit{singular} in the example) and it's corresponding proportion $q_l$ (such as $q_{\text{singular}}\approx 0.44$):

\begin{center}
\begin{math}
  c_l=q_l\cdot n
\end{math}
\end{center}

\noindent This is because the general formula for calculating \Key{proportions} is:

\begin{equation}
  q_l=\cfrac{c_l}n
  \label{eq:proportion}
\end{equation}

\noindent For the count of the other level $c_m$ and its corresponding proportion $q_m$ ($c_{\text{plural}}=5$ and $q_{\text{plural}}\approx 0.56$ in the example), then, necessarily $c_m=(1-q_l)\cdot n$.
Also, $q_m=(n-c_l)\div n$.
This means that counts of a binary variable are exhaustively summarised by two numbers.
Neither can we adequately summarise it by one number (just one count, just one proportion, or just the sample size), nor do we absolutely need to provide more than two numbers (provided it's two well-chosen numbers).%
\footnote{However, we would like to point out that providing the absolute minimum information is not always the nicest thing to do to your readers.
Usually, the more relevant information you provide in your publications, the better it is for your readers' understanding of your point.}
Either we specify the sample size $n$ and at least one proportion or one count.
Or we specify the raw counts for both levels of the variable.
Furthermore, not much is gained by converting counts to proportions, except that some people find proportions more intuitive.
We point this out because aggregated numbers for other measurements (to be discussed below) serve a much stronger purpose and are much more informative.
For counts of binary variables, any aggregation serves no or just a very limited purpose.

Finally, we can ask what is the \Key{central tendency} of a binary variable.
The purpose of finding the central tendency is to find a typical value in some sense.
The term \textit{central} derives from more complex measurements (see Section~\ref{sec:ordinal} and below), and it's not very intuitive for binary measurements.
It will become clearer as we progress through this chapter.
In the binary case, the only measure of central tendency is its modal category or just \Key{mode}, which is the level that occurs more often than the other.
If $c_l>c_m$, then $c_l$ is the modal category and vice versa.
In general, the central tendency is important as the \Key{expected value} of a variable, and the mode of a binary variable is no exception.
Without any further information, it's the value that we would predict for any event.
If all you knew about English were the short paragraph from Wikipedia quoted above (annotated only for part-of-speech and number, with no information about the syntax, the meaning, the register, etc.), and you were asked to predict the number of the next noun in the text, any sane person would predict \textit{plural} because it appears to be the modal (\ie, more frequent) category judging from the sample.
Having much more information about the English language, we are aware that this would probably not lead to a prediction accuracy of 56\% in real life.
There are two reasons for this:
First, producing English nouns is not a mere lottery, and our analysis should reflect this.
Second, the sample is very small ($n=9$) and from a very specific type of text, problems we'll deal with in Chapter~\ref{sec:confidence} and throughout the book.
However, if we knew for sure that among the totality of English noun tokens, 5 out of 9 were in the plural, and you had to predict the number of a noun without information about the lexeme, syntactic context, or register of the text, we would always predict plural and be right in 56\% of all cases in the long run.
In this sense, it would be the \textit{expected value} or \textit{expectation}.%
\footnote{The difference between an expected value estimated from a sample and one that's true for the population is the subject of Chapter~\ref{sec:confidence}.}

<<binaryhist, fig.cap=paste0("Histogram (or bar plot) of counts of a binary variable with the mode highlighted in colour"), echo=F, fig.pos="t", cache=T>>=
  par.defaults()
  ex.number <- c(4,5)
  h <- barplot(ex.number, xaxt = "n", xlab = "", ylab = "Frequency", main ="", border=F, col = c(the.midgray, the.lightcols[2]), ylim=c(0, 6))
  text(h[,1], ex.number+0.5, labels = ex.number)
  axis(1, h[,1], c("Singular", "Plural"), tick = F)
@

Finally, Figure~\ref{fig:binaryhist} shows a histogram of the sample of the \textit{Number} variable from the illustrative example discussed above.
A histogram for a variable with discrete levels is usually displayed with spaces between the bars (compare Figure~\ref{fig:hist}, where there isn't any space between the bars), and it's often called a \Key{bar plot} instead of a histogram.
It's still just a special type of histogram.
For the benefit of the reader, we have highlighted the mode in colour.
This is the only reasonable way to plot such results.
We strictly recommend to label the individual bars with the raw counts, not proportions or percentages.
Since the bars already provide a good visual impression of the proportions, the relevant numerical information are the counts.
At the same time, we can't think of a situation where such a plot by itself isn't an insult to the reader.
Looking at just two counts (except maybe very high counts in the millions and above), humans are able to grasp the relative magnitude of those counts easily without visualisation.

\subsection{Multi-Valued Measurements}\label{sec:nominal}

In this chapter, we continually progress from simple to more complex variables (\ie, measurements).
After binary variables, the next more complex thing is simply a more general variant of a binary variable.
Let's have a look at a short paragraph from the German Wikipedia article about Critical Rationalism and annotate all lexical nouns with their grammatical case:%
\footnote{Sourced from \url{https://de.wikipedia.org/wiki/Kritischer_Rationalismus} on 20 February 2025.}

\begin{quote}
  \itshape Der Realismus\Nom\ ist die dem subjektiven Idealismus\Dat\ widers\-prechende metaphysische Theorie\Nom, dass eine vom Menschen\Dat\ unabhängige Wirk\-lich\-keit\Nom\ existiert. Während der naive Realismus\Nom\ davon ausgeht, dass die Welt\Nom\ so ist, wie der Mensch\Nom\ sie wahr\-nimmt, vertritt der kri\-ti\-sche Realismus\Nom\ die Auffassung, dass Vorstellungen\Nom\ von ihr durch subjektive Elemente\Akk, die in der Wahrnehmung\Dat\ und im Denken\Dat\ liegen, mehr oder weniger stark beeinflusst werden. Weil die Sinne\Nom\ und die Ver\-arbei\-tungs\-pro\-zesse\Nom\ im Gehirn der angenommenen Außenwelt\Dat\ und der Vor\-stellung\Dat\ zwischen\-geschaltet sind, kann man auch vom indirekten Rea\-lis\-mus\Dat\ sprechen. Dieser Vermit\-tlungs\-vorgang\Nom\ schließt eine \textit{reine Wahr\-nehmung}\Nom\ aus, denn es kann sich um Täuschungen\Nom\ handeln.
\end{quote}

There are four grammatical cases in German: nominative, accusative, dative, and genitive.
They occur at 13, 1, 7, and 0 noun events in the sample, respectively.
The sample of measurements looks as follows:%
\footnote{We reuse the symbol \xsample\ for any tuple containing measurements from an experiment.
If we need to talk about more than one such tuple, we call the others \ysample\ and \zsample.}

\begin{quote}
  $\xsample=\langle$\textit{nominative, dative, nominative, dative, nominative, nominative, nominative, nominative, nominative, nominative, accusative, dative, dative, nominative, nominative, dative, dative, dative, nominative, nominative, nominative}$\rangle$
\end{quote}

The variable \textit{Case} measuring grammatical case in German has four levels which have no empirically significant order.
Instead of the order nominative, accusative, dative, genitive, many grammars and text books order the four cases according to the Latin tradition: nominative, genitive, dative, accusative, which is perfectly fine, too.
At this point, some linguists might object and point out that we should order them by increasing obliqueness or in a way such that syncretisms are analysed better (\eg, \citealt{Eisenberg2020a}).
However, such ordering preferences are not empirical observables.
They are theoretical results rather than measurements.
We specifically chose this example in order to point out this difference.
A variable which has more than two discrete levels with no measurable ordering is called a \Key{nominal variable}.

<<nominal, echo=F, cache=T, results="asis">>=
  counts <- matrix(c(13,1,7,0), nrow = 1)
  colnames(counts) <- c("Nominative", "Accusative", "Dative", "Genitive")
  rownames(counts) <- c("Count")

  counts %>%
    as.data.frame.matrix %>%
    kable(align = c("c", "c"), booktabs = T, linesep = "",
          caption = "A tabulation of counts of a nominal (four-level) variable",
          label = "nominal") %>%
    column_spec(1, bold = T) %>%
    row_spec(0, bold=TRUE)
@

Binary variables are really just the minimal case of a nominal variable.
Not surprisingly, we can treat them essentially the same, for example by tabulating the counts as in Table~\ref{tab:nominal}.
The only reasonable aggregations are conversion of raw counts to proportions as in Equation~\ref{eq:proportion}.
The \Key{mode} is just the most frequent value, much like in the binary case, and we consider it trivial to find it for smaller samples.
We leave it to the reader to find the mode and calculate the proportions and percentages for each of the four grammatical cases in the sample.
Two histograms (more specifically bar plots) for the example are shown in Figure~\ref{fig:nominalhist}.
Since there is no measurable order in grammatical cases, both plots are equivalent, and you could choose the one that you think is more adequate for your purpose.

<<nominalhist, fig.cap=paste0("Two equivalent histograms of counts of a nominal four-level variable (grammatical case in German) with the mode highlighted in colour"), echo=F, fig.pos="t", cache=T>>=
  par(mfrow=c(1,2), family = "Plotfont")

  ex.case <- c(13,1,7,0)
  h <- barplot(ex.case, xaxt = "n", xlab = "", ylab = "Frequency", main ="", border=F, col = c(the.lightcols[2], the.midgray, the.midgray, the.midgray), ylim = c(0, 14))
  text(h[,1], ex.case+0.5, labels = ex.case)
  axis(1, h[,1], c("Nom", "Acc", "Dat", "Gen"), tick = F)

  ex.case <- c(13,7,1,0)
  h <- barplot(ex.case, xaxt = "n", xlab = "", ylab = "Frequency", main ="", border=F, col = c(the.lightcols[2], the.midgray, the.midgray, the.midgray), ylim = c(0, 14))
  text(h[,1], ex.case+0.5, labels = ex.case)
  axis(1, h[,1], c("Nom", "Dat", "Acc", "Gen"), tick = F)

  par(mfrow=c(1,1))
@

\subsection{Ordered but Discrete Measurements}\label{sec:ordinal}

<<ordinalsetup, cache=T, echo=FALSE, include=FALSE, results='asis'>>=
  set.seed(75)
  levs <- rev(c("very high", "high", "medium", "low", "very low"))
  ratings <- scale(rnorm(10, 100, sd = 25))
  ratings <- ifelse(ratings >= 1.5, 5,
                 ifelse(ratings >= 0.7, 4,
                        ifelse(ratings >= 0, 3,
                               ifelse(ratings >= -1, 2, 1))))
  nice.rating <- factor(ratings)
  levels(nice.rating) <- levs
  ex.rating <- table(ratings)
@

Moving on to the next more complex type of measurement and variable, we use acceptability ratings as an example.
Such ratings are often made on a scale with 5 or 7 points, for example \textit{very high}, \textit{high}, \textit{medium}, \textit{low}, \textit{very low}.
For illustration purposes, assume that a rating experiment was conducted where participants were asked to rate the sentence: \textit{It's a mission to boldly go where no man has gone before.}
The study had \Sexpr{length(ratings)} participants, hence $n=\Sexpr{length(ratings)}$.
The raw data are, using \xsample\ again to denote the tuple of measurements:

\begin{quote}\itshape
  $\xsample=\langle$\Sexpr{paste0(as.character(nice.rating), collapse = ", ")}$\rangle$
\end{quote}

We hope that the difference between such a variable---let's call this one \textit{Rating}---and a nominal variable like \textit{Case} from Section~\ref{sec:nominal} is obvious.
The ratings have an intrinsic order, and each level of the variable has a rank:
\textit{very high} is higher than \textit{high}, \textit{high} is higher than \textit{medium}, and so forth.
A variable like \textit{Rating} is called an \Key{ordinal variable}.
All possible ways of aggregating nominal variables (including binary ones) apply to ordinal variables as well:
counting and tabulation, calculation of the proportions or percentages of the different levels, the determination of the mode, and bar plots (histograms).
The only difference is that tables and plots should respect the intrinsic order of the ordinal variable, as can be observed in Table~\ref{tab:ordinal} and Figure~\ref{fig:ordinalhist}.

<<ordinal, echo=F, cache=T, results="asis">>=
  counts <- t(matrix(ex.rating))
  colnames(counts) <- str_to_title(levs)
  rownames(counts) <- c("Count")

  counts %>%
    as.data.frame.matrix %>%
    kable(align = rep("c", 5), booktabs = T, linesep = "",
          caption = "A tabulation of counts of an ordinal variable",
          label = "ordinal") %>%
    column_spec(1, bold = T) %>%
    row_spec(0, bold=TRUE)
@

<<ordinalhist, fig.cap=paste0("Histograms of counts of an ordinal five-level variable (acceptability ratings) with the mode highlighted in colour"), echo=F, fig.pos="t", cache=T>>=
  par.defaults()
  h <- barplot(ex.rating, xaxt = "n", xlab = "", ylab = "Frequency", main ="", border=F, ylim = c(0, 6),
               col = c(rep(the.midgray, 2), the.lightcols[2], rep(the.midgray, 2)))
  text(h[,1], unname(ex.rating)+0.5, labels = unname(ex.rating))
  axis(1, h[,1], str_to_title(levs), tick = F)
@

Thanks to the higher complexity of this variable (the intrinsic order of its levels), we have an additional measure of central tendency available.
To find this measure called the \textit{median}, we first need to sort the raw measurements (not the tabulation of the counts) from the lowest rank to the highest rank, which is something we couldn't do with a nominal variable due to the lack of an intrinsic order of its levels.
The sorted sample \xsample\prm\ looks like this:

<<ratingsmedian, cache=T, echo=FALSE, include=FALSE, results='asis'>>=
  ratings.ordered <- factor(ratings)
  levels(ratings.ordered) <- levs
  ratings.ordered <- sort(ratings.ordered)
@

\begin{quote}\itshape
  $\xsample\prm=\langle$\Sexpr{paste0(as.character(ratings.ordered), collapse = ", ")}$\rangle$
\end{quote}

Plotting these single observations from left to right on the x-axis and putting dots on the y-dimension corresponding to the respective ratings, we get Figure~\ref{fig:ordinalsort}, a \Key{raw data plot}.
The grey line is just a helper line for better visual orientation.
The coloured line marks the middle point of the ordered sample in the sense that half the ordered sample lies to its left and the other half to its right.

<<ordinalsort, fig.cap=paste0("Raw data plot of a sorted sample of an ordinal value; the coloured line marks the position of the median"), echo=F, fig.pos="t", cache=T>>=
  par.defaults()
  p <- plot(as.numeric(ratings.ordered), xaxt = "n", xlab = "Sorted Data Points", yaxt = "n", ylab = "Level", main ="", ylim = c(1, 5), frame.plot = F)

  lines(c(5.5, 5.5), c(1, 3), lty = 1, lwd = the.lwd, col = the.cols[2])

  lines(1:10, as.numeric(ratings.ordered), lwd = the.lwd, col = the.lightgray)
  points(1:10, as.numeric(ratings.ordered), pch = the.pchs[1], cex = 2)

  axis(2, 1:5, levs, tick = T)
  axis(1, 1:10, 1:10, tick = T)
@

The value that lies in the middle of the ordered sample (marked by the coloured line in Figure~\ref{fig:ordinalsort}) is the \Key{median}.
In the example, the median technically lies between two data points (the 5\Up{th} and the 6\Up{th} one) as $n=10$ is an even number.
Fortunately, the data points to left and right of the median point have the same rating value, so we can say without further ado that $\xmedian=$\,\textit{medium}, where we denote the median of \xsample\ by \xmedian.
If, on the other hand, the median point lied between two data points with different values (such as \textit{low} and \textit{medium}), there is no clean way to name the median.
One solution would be to say truthfully that the median lies between \textit{low} and \textit{medium} or that it lies at \textit{low$+$} (as in the American educational grading system with A$-$, B$+$, etc.).
Another solution would be to convert the levels from \textit{very low} to \textit{very high} to the numbers between 1 and 5 and state that $\xmedian=2.5$.
For ordinal scales, we strongly advise against the latter solution (conversion to numbers) as it creates the false impression that the ranks correspond to numerical values (see Section~\ref{sec:numeric}).

Starting with the median, we can get a clearer idea of why we speak of measure of central tendency.
In a well-defined way, the median position is at the \textit{centre} of the sample.
As the expected value of an ordinal variable, however, the mode is probably still the best metric.
If \textit{medium} actually is the rating assigned most often, then guessing \textit{medium} would lead to the highest possible success rate.
However, median is of course an informative measure, and we'll see in the next section that it's very handy if combined with even more informative measures like the mean.

We've got one final word of warning before moving on to numeric measurements in the next section.
Rating scales with 5, 7, 12, or any number of levels are often called \Key{Likert scales} in research papers, text books, and so on.
Authors often use the opportunity to point out rather smugly that the correct pronunciation is [ˈlɪkət] (or some transatlantic variant) and not [ˈlʌ͡ɪkət].
For example, the Wikipedia article on ordinal data states incorrectly (after providing the correct IPA transcription of the name):%
\footnote{Sourced from \url{https://en.wikipedia.org/wiki/Ordinal_data} on 22 February 2025.}

\begin{quote}\itshape
  \Argh A well-known example of ordinal data is the Likert scale. {\normalfont [\dots]}
  Examples of ordinal data are often found in questionnaires:
  for example, the survey question ``Is your general health poor, reasonable, good, or excellent?'' may have those answers coded respectively as 1, 2, 3, and 4.
\end{quote}

\noindent It is not true that any 5-point-scale is a Likert scale.
A Likert scale is a complex psychometric construct invented by a person called Likert and operationalised through a set of responses to Likert items.
The responses are measured on a 5-point agreement scale, but they are aggregated into the underlying Likert scale.
Since the Likert scale cannot be measured but is merely reconstructed from measured responses, we recommend to avoid the phrase \textit{Likert scale} entirely and just call 5-point scales \textit{5-point scales}, etc.

\subsection{Numeric Measurements}\label{sec:numeric}

<<numericsetup, cache=T, echo=FALSE, include=FALSE, results='asis'>>=
  set.seed(2893)
  pots <- round(rnorm(10, 250, sd = 78), 0) %>% sort
  potsx <- round(rnorm(100, 250, sd = 38), 0) %>% sort
@

Finally, we turn to numeric measurements.
As an example, we use fictitious EEG measurements from an impressive cap-and-cable experiment.
We made peoples brains react to split infinitives and measured some negative or positive change in some potential in milliseconds.
The 10 measurements ($n=10$) in our now familiar tuple \xsample\ are as follows:%
\footnote{Yes, we reuse the symbol \xsample\ every time.
In real data analysis, one should always use fresh and informative names, of course.
Never call real data \xsample, \ysample, etc.}

\begin{quote} \itshape
  $\xsample=\langle$\Sexpr{paste0(pots, collapse = ", ")}\,$\rangle$
\end{quote}

Such numeric measurements have a very clear natural order as they are measured in floating point numbers or integers.
It is mathematically and conceptually absolutely clear that 153 is a shorter reaction time than 172, etc.
This is why we took the liberty of pre-sorting \xsample\ from lowest to highest.
While such a numeric measurement defines a ranking like an ordinal measurement, more information is contained in numbers than merely a ranking.
Most importantly, the \alert{distance between two measurements} can be quantified beyond mere ranks.
We could say that \textit{high} is 2 ranks above \textit{low} in the example from Section~\ref{sec:ordinal}.
However, saying that \Sexpr{pots[2]}~ms is \Sexpr{pots[2]-pots[1]}~ms longer than \Sexpr{pots[1]}~ms is much more informative.
It cannot (and need not) be determined whether \textit{good} is twice or three times as good as \textit{bad}, but 2~ms are definitely twice as long as 1~ms, and so forth.

Mathematically speaking, \Key{numeric variables} allow us to use standard arithmetic (adding, subtracting, multiplying, dividing), which we cannot do with any of the less complex variables.
Sometimes, a distinction is made within the numeric measurements:
Variables on a \Key{ratio scale} are those with a defined zero measurement where no measurement below 0 exists.
Reaction times or word lengths are good examples as it's impossible to measure negative reaction times or words that are $-1$ phonemes or graphemes long.
The other type of numeric variable is measured on a so-called \Key{interval scale}, which does not have a zero floor and allows measurements towards negative infinity.
As such scales never occur in linguistics (and, we think, most empirical sciences) and the distinctions between ratio and interval scales can otherwise be safely neglected, we simply speak of numeric variables.

Regarding the central tendency of numeric measurements and the applicable plots, numeric measurements offer many more options and introduce some complications.
Let's begin with a raw data plot of the sorted sample in Figure~\ref{fig:numericraw}.

<<numericraw, fig.cap=paste0("Raw data plot of a sorted sample of a numeric variable; the coloured line marks the position of the median"), echo=F, fig.pos="t", cache=T>>=
  par.defaults()
  p <- plot(pots, xaxt = "n", xlab = "Sorted Data Points", ylab = "Milliseconds", main ="", ylim = c(150, 400), frame.plot = F)
  axis(1, 1:10, 1:10, tick = T)
  lines(c(5.5, 5.5), c(150, median(pots)), lty = 1, lwd = the.lwd, col = the.cols[2])
  lines(1:10, pots, lwd = the.lwd, col = the.lightgray)
  points(1:10, pots, pch = the.pchs[1], cex = 2)
@

The mode is difficult to determine because we defined it as the most frequent value.
However, each measurement is unique, which is intuitively a likely outcome for an experiment measuring anything in millisecods.
We'll return to the concept of the mode later.
The \Key{median} point is between the the 5\Up{th} and the 6\Up{th} data point.
The respective measurements are $x_5=\Sexpr{pots[5]}$~ms and $x_6=\Sexpr{pots[6]}$~ms.
It is customary to calculate the arithmetic middle of the two values and declare it the median: $\xmedian=(\Sexpr{pots[5]}+\Sexpr{pots[6]})\div 2=\Sexpr{(pots[5]+pots[6])/2}$~ms.
Hence, half of the participants had a faster reaction than $\Sexpr{(pots[5]+pots[6])/2}$~ms, and half of the participants had a slower reaction.

While the mean is informative for numeric variables, most readers probably already know that there is one additional operation we can perform to find the central tendency: the \Key{mean} (colloquially called the \textit{average}).
It is obtained by adding all measurements and dividing them by the number of measurements (\ie, the sample size):

\begin{center}
  \begin{math}
    \cfrac{\Sexpr{paste0(pots, collapse="+")}}{\Sexpr{length(pots)}}=\cfrac{\Sexpr{sum(pots)}}{\Sexpr{length(pots)}}\approx\Sexpr{round(sum(pots)/length(pots), 0)}
  \end{math}
\end{center}

\noindent In this example, the mean is virtually identical to the median.
This is typical of symmetric samples.
If you look at Figure~\ref{fig:numericraw}, the data points are arranged almost along a straight line from the lower left to the upper right, and the sample is symmetric in this sense.
We'll encounter cases where the mean and the median are not the same.

There is a convenient notation used in the general definition of the mean.
It's the \Key{sum operator} $\Sigma$, which is used like this:

\begin{center}
  \begin{math}
    \sum\limits_{i=1}^{n}x_i
  \end{math}
\end{center}

\noindent We use $n$ consistently to denote the sample size ($n=10$ in the example).
The symbol $x$ is used to denote individual values from the tuple \xsample, \ie, the measurements from the experiment.
The index $i$ is a counter and tells us which concrete $x_i$ we pull from \xsample.
The limits of the sum operator use these symbols to go through the whole tuple \xsample.
Starting from $i=1$ (the lower limit) and counting all the way up to $n$ (the upper limit), it sums up all $x_i$.
Since $x_1$ is the first element of the tuple and $x_n$ ($x_10$ in the example), all values from \xsample\ are added up.
Writing the general form of the sum operator more explicitly, we get:

\begin{center}
  \begin{math}
    \sum\limits_{i=1}^{n}x_i=x_1+\dots+x_n
  \end{math}
\end{center}

We'll encounter sum operators with more complex expressions than $x_i$, but the logic remains the same.
It's really just the instruction to add some numbers.
The general equation to calculate the mean \xmean\ of a tuple \xsample\ is thus:

\begin{equation}
  \xmean=\cfrac{\sum\limits_{i=1}^{n}x_i}{n}
  \label{eq:mean}
\end{equation}

\noindent As abbreviated variants of the sum operator, we allow the following to denote the sum of all values from \xsample.
Either the upper limit $n$ is implied (second variant), or we just pass the symbol for the whole tuple to the sum operator (third variant), in which case no index variable ($i$) is required:

\begin{center}
  \begin{math}
    \sum\limits_{i=1}^{n}x_i=\sum\limits_ix_i=\sum\xsample
  \end{math}
\end{center}

\noindent Therefore, alternative notations for the mean are:

\begin{center}
  \begin{math}
    \xmean=\cfrac{\sum\limits_{i=1}^{n}x_i}{n}=\cfrac{\sum\limits_ix_i}{n}=\cfrac{\sum\xsample}{n}
  \end{math}
\end{center}

The mean and the median are quite informative measures of the central tendency.
Finally, we'll now generalise the concept of the median, and we'll re-introduce the mode for numeric variables (which we glossed over at the beginning of this section).
Figure~\ref{fig:numericrawq} repeats Figure~\ref{fig:numericraw} with some additional vertical lines.

<<numericrawq, fig.cap=paste0("Raw data plot of a sorted sample of a numeric variable; the coloured lines mark the position of the median, the lower and upper quartile, as well as the minimum and the maximum"), echo=F, fig.pos="t", cache=T>>=
  par.defaults()
  p <- plot(pots, xaxt = "n", xlab = "Sorted Data Points", ylab = "Milliseconds", main ="", ylim = c(150, 400), frame.plot = F)
  axis(1, 1:10, 1:10, tick = T)
  lines(c(5.5, 5.5), c(150, median(pots)), lty = 1, lwd = the.lwd, col = the.cols[2])
  lines(c(3, 3), c(150, pots[3]), lty = 1, lwd = the.lwd, col = the.cols[1])
  lines(c(8, 8), c(150, pots[8]), lty = 1, lwd = the.lwd, col = the.cols[1])
  lines(c(1, 1), c(150, pots[1]), lty = 1, lwd = the.lwd, col = the.cols[3])
  lines(c(10, 10), c(150, pots[10]), lty = 1, lwd = the.lwd, col = the.cols[3])
  lines(1:10, pots, lwd = the.lwd, col = the.lightgray)
  points(1:10, pots, pch = the.pchs[1], cex = 2)
@

The lines mark the aforementioned generalisations of the median.
Going outward from the median, the lines at data point 3 and 8 define the lower and upper \Key{quartile}.
With the median, they split the sorted sample in four parts called the first (lower), second, third, and fourth (upper) quartile.
For numeric samples, it's also informative to inform readers about the minimal and maximal value, which are also marked in the plot.
Contrary to an ordinal variable, a numeric variable does not have a pre-defined minimal and maximal value such as \textit{very low} and \textit{very high}.
Table~\ref{tab:numericsummary} shows all the information discussed so far that can be used to summarise a numeric variable.%
\footnote{Notice that statistics software packages don't always calculate the quartiles in the same naïve way we did.
There is a number of algorithms available that adjusts the quartiles based on the overall distribution of values in the sample.
Please consider the documentation.
It's particularly embarrassing if you first notice the resulting discrepancies while teaching with R.\label{fn:quartiles}}

<<numericsummary, echo=F, cache=T, results="asis">>=
  numsum <- matrix(
    round(c(pots[1], pots[3], median(pots), mean(pots), pots[8], pots[10], length(pots)), 0),
    nrow = 7)
  rownames(numsum) <- c("Minimum", "First Quartile", "Median", "Mean", "Third Quartile", "Maximum", "Sample size")
  colnames(numsum) <- c("Value")

  numsum %>%
    as.data.frame.matrix %>%
    kable(align = "r", booktabs = T, linesep = "",
          caption = "Summary of a numeric variable with n=10",
          label = "numericsummary") %>%
    column_spec(1, bold = T) %>%
    row_spec(6, extra_latex_after = "\\midrule") %>%
    row_spec(0, bold=TRUE)
@

For a sample with 10 data points, providing a summary in 7 values might not appear to be much of a summary.
When samples get bigger, however, this changes very quickly.
Table~\ref{tab:numericsummaryx} shows the same for a very similar sample but with $n=100$.

<<numericsummaryx, echo=F, cache=T, results="asis">>=
  sumsum <- summary(potsx)
  numsumx <- matrix(
    round(c(sumsum, length(potsx)), 0),
    nrow = 7)
  rownames(numsumx) <- c("Minimum", "First Quartile", "Median", "Mean", "Third Quartile", "Maximum", "Sample size")
  colnames(numsumx) <- c("Value")

  numsumx %>%
    as.data.frame.matrix %>%
    kable(align = "r", booktabs = T, linesep = "",
          caption = "Summary of a numeric variable with n=100",
          label = "numericsummaryx") %>%
    column_spec(1, bold = T) %>%
    row_spec(6, extra_latex_after = "\\midrule") %>%
    row_spec(0, bold=TRUE)
@

The summary characterised a sample of 100 values in 7 values, which is much more economical than just showing all 100 values.
Also, most humans aren't very good at processing 100 raw values and get an idea of what the tendencies and trends in the data are.
A histogram as in Figure~\ref{fig:numhist} (left panel) is a helpful companion to the table.
Like in Chapter~\ref{sec:fisher}, it's a histogram of a continuous variable, and the individual bars represent counts of occurrences of values from a certain range:%
<<numhist, fig.cap=paste0("Histogram of a sample of a numeric variable with n=100 and a corresponding density plot based on an estimated probability density function"), echo=F, fig.pos="t", cache=T>>=
  par(mfrow = c(1,2), family = "Plotfont")

  h <- hist(potsx, breaks = 10, xlab = "Milliseconds", ylab = "Frequency", main ="", border=F, ylim = c(0, 25), col = the.midgray)

  densplo <- density(potsx, kernel = "cosine")
  qi <- which.min(abs(densplo$x - summary(potsx)[2]))
  qii <- which.min(abs(densplo$x - summary(potsx)[3]))
  qiii <- which.min(abs(densplo$x - summary(potsx)[5]))
  plot(densplo, xlab = "Milliseconds", main ="",  bty ="n", ylim = c(0, 0.012), lwd = 1)
  lines(densplo, lwd = the.lwd, col= the.darkgray)
  par(mfrow = c(1,1))
@ %
the first bar counts the values that lie between \Sexpr{h$breaks[1]}~ms and \Sexpr{h$breaks[2]}~ms, and so on.
Histograms of continuous variables are plotted without spaces between the bars, unlike the bar plots used for discrete variables.

Another plot with a similar informative value is shown in the right panel of Figure~\ref{fig:numhist}.
It shows an estimated density function for the sample.
Probability density functions were introduced in Section~\ref{sec:hypergeometric} for a known \Key{theoretical probability distribution}, namely the Hypergeometric Distribution.
While the plots look conceptually very similar, there are major differences between the plots of the Hypergeometric Distribution and Figure~\ref{fig:numhist}.
First, the Hypergeometric Distribution is the distribution of a discrete variable ($k$ successes), and the estimate in Figure~\ref{fig:numhist} is for a continuous variable.
While $k$ in \textit{$k$ successes} is always an integer and there is no such thing as 3.84 successes, time can in principle be measured with arbitrary precision.
Second, Figure~\ref{fig:numhist} shows an \Key{empirical distribution}.
In other words, it's just a curve guessed from a set of measurements, while the Hypergeometric Distribution is a mathematically well-defined known distribution.%
\footnote{Chapter~\ref{sec:confidence} deals with this difference in some detail.}

However, Figure~\ref{fig:numhist} is still a plot of a probability density function.
Instead of the individual values adding up to 1 (as with discrete distributions), the area under the curve (the integral of the density function on the interval from $-\infty$ to $+\infty$) is 1.
When a statistics software estimates such a curve, it applies a process of inter- and extrapolation from the measured values in order to arrive at a smooth curve, which is also rescaled in order to fulfil the requirement that the area under it is 1.
Once such a function has been determined, it is possible to look up the hypothetical proportion (equivalent to the estimated probability) of some value, even if that precise value was not in the original sample.
For example, a measurement of \Sexpr{round(densplo$x[82], 2)}~ms occurs in a hypothetical proportion of \Sexpr{format(round(densplo$y[82], 5), scientific = F)} of all cases (\ie, has the estimated probability of \Sexpr{format(round(densplo$y[82], 5), scientific = F)}).
%
%\footnote{While such a statement is not false, it blurs one peculiar property of any continuous probability density function.
%As the x-axis is densely populated with real numbers, for each specific point value the probability is 0.
%This is necessarily so because a point on the curve has no width, and integrating a function on a point always results in 0 as an area with a %width of 0 also has a size of 0.
%Technically, we're therefore always talking about probabilities of an interval of values.
%n the example \Sexpr{format(round(densplo$y[82], 5), scientific = F)} is thus the probability of some interval around \Sexpr{round(densplo$x[82], 2)}~ms.
%In practice, this distinction doesn't play a huge role very often.}
Finally, the density estimate allows us to define the \Key{mode} of a numeric variable.
It's simply the maximum of the curve (or an interval around that point).
In the example, the highest probability density is around \Sexpr{round(densplo$x[which(densplo$y==max(densplo$y))], 0)}, and it's \Sexpr{round(densplo$y[which(densplo$y==max(densplo$y))], 4)}.
In this almost perfectly symmetric sample, the mean, the median, and the mode are apparently very close to each other.

Whether a plot of the density estimate is more informative than a histogram depends on the type of data and the research question.
Having the two plots side by side in Figure~\ref{fig:numhist} should reveal that they contain virtually the same information.
However, before using density plots one should do some research as to which specific method the chosen piece of software uses in order to check whether it delievers the desired result.%
\footnote{Look under \textit{smoother}, \textit{smoothing}, or \textit{kernel}.
We use a cosine kernel by default.}

<<boxplot, fig.cap=paste0("A box plot and a violin plot for a numeric sample with n=100"), echo=F, fig.pos="t", cache=T>>=
  par(mfrow=c(1,2), mar=c(2,5,1,0), family = "Plotfont")
  boxplot(potsx, frame = F, ylab = "Milliseconds", col = the.lightcols[2],
          notch = T, pch = the.pchs[1], medcol = the.cols[2])
  vioplot(potsx, frame.plot = F, add = F, xlab = "", xaxt = "n", #side = "right", horizontal = T,
          col = the.lightcols[5], colMed = the.cols[2],
          rectCol = the.lightcols[2], lineCol = the.lightcols[5])
  par(mfrow=c(1,1), mar = rep(4, 4))
@

We need the above information about density estimates to understand another type of useful plot.
Figure~\ref{fig:boxplot} shows a box plot (left) and a violin plot (right) of the 100 measurements of milliseconds.
A \Key{box plot}---more specifically a box-and-whiskers plot---summarises the distribution of values in sample vertically, and it contains information roughly equivalent to Table~\ref{tab:numericsummaryx}.
The line in the middle is at the median position (here \Sexpr{numsumx[3]}), and the box around it spans the range from the first quartile (here \Sexpr{numsumx[2]}) to the third quartile (here \Sexpr{numsumx[5]}).
While the box shows in which range the central half of the sample values lie, the whiskers are supposed to show the range in which most of the sample values lie.
It's usually based on the \Key{inter-quartile range} or IQR.
The IQR is the distance between the third and the first quartile (here $\Sexpr{numsumx[5]}-\Sexpr{numsumx[2]}=\Sexpr{numsumx[5]-numsumx[2]}$).
The IQR is multiplied by 1.5 and subtracted from the first quartile (hence $\Sexpr{numsumx[2]}-(\Sexpr{numsumx[5]}-\Sexpr{numsumx[2]})\cdot 1.5=\Sexpr{numsumx[2]-(numsumx[5]-numsumx[2])*1.5}$) as well as added to the third quartile (hence $\Sexpr{numsumx[5]}+(\Sexpr{numsumx[5]}-\Sexpr{numsumx[2]})\cdot 1.5=\Sexpr{numsumx[5]+(numsumx[5]-numsumx[2])*1.5}$) to define the lower and upper bound of  the whiskers interval.
The concrete length of the whiskers goes up to the closest data points within this interval.
Finally, the dots show individual data points that lie outside of the whiskers interval.
The idea behind a box plot is to give a straightforward visual impression of where the centre of the sample, the middle 50\%, and the majority of the data points lie, and how many data points are so-called \Key{outliers} far away from the majority range.

The violin plot is similar but offers additional information.
The dot denotes the median, the box the middle two quartiles, and the body adds a density estimate around those.
Arguably, this provides more detailed information than a box-and-whiskers plot.%
\footnote{This is the most basic version of a violin plot.
There are variants which also add whiskers, outliers, and other information.}
It is definitely useful when the sample is very irregular and has a bumpy distribution.
Under such circumstances, the quartiles and the mean tend to create an impression of homogeneity and cleanliness that is unwarranted.

However, both box plots and violin plots are not \textit{innocent} in the sense that there isn't one natural way to plot them.
When you use these plots, you're already doing data analysis.
Even quartiles---as we mentioned above---can be calculated in different ways, and the decision of what counts as an outlier definitely isn't an automatic one.
Some box plots use larger or smaller intervals than $\pm 1.5\cdot IQR$ for the whiskers, which increases or decreases the number of data points plotted as outliers.
Maybe that's warranted for some data sets, but maybe it hides properties of the data set that you and your readers should be aware of.
In violin plots, different algorithms used for the density estimate can change the appearance of the plot significantly.
Furthermore, symmetric violin plots could be criticised for visually doubling the area under the curve.
This can lead readers to overestimate the central areas and underestimate the marginal areas.
While many people seem to be enthusiastic about violin plots, at least some of us have very lukewarm feelings about them.

All in all, we advise anyone to use such plots carefully and wisely.
Always do the appropriate research to find out what your statistics package actually does when creating the plots.
When reading and critiquing published research, always ask whether you understand what was plotted and whether the authors are sufficiently transparent about their plotting methods.
Above all, next time somebody shows you a plot that aggregates data in any way, ask persistent questions, especially if you get the feeling there is relevant information missing regarding the genesis of the plots.
To our embarrassment, we have (in the distant past) also fallen into the trap of presenting graphics that we didn't fully understand, and we now know that these graphics were suboptimal and potentially misleading.
Strangely enough, nobody ever asked us any persistent questions.

\section{Populations, Samples, and Variance}

\subsection{Populations and Samples}
\label{sec:populationssamples}

In Section~\ref{sec:centraltendency} we introduced statistics for the central tendency in a sample.
Before we move on to discussing the spread of a sample and it variance, we need to mention an important point regarding samples and their populations.
As we explained in Chapter~\ref{sec:inference}, the goal of scientific statistical inference is to find out something about a population based on a small sample from that population.
The population is the totality of the objects of the type in which we've got some scientific interest.
It can be anything from all galaxies in the Laniakea Supercluster to all adult Tories in Buckinghamshire to all sentences of contemporary German.
Often, the population is itself a proxy for some bigger research question.
Observations of galaxies in the Laniakea Supercluster might be used to test a theory of gravity, a study of Buckinghamshire Tories could be used to test a theory from Social Psychology, and maybe a close examination of all sentences of German will reveal how the brain processes Split Mood Phrases (SplitMP).

While we illustrate statistics such as the mean using small samples in this book, they are all defined for populations, too.
At least conceptually, for each variable that describes a property of all objects in the population we can calculate the applicable central tendency, the variance (see immediately below), and so forth.
The galaxies in Laniakea have a mean total mass, Buckinghamshire Tories have a mean income, and German sentences have a mean length in phonemes, words, or what not.
Why is everything so sample-centric in this chapter, then?
First, it's much easier to do exercises where you have to calculate the mean of 10 values rather than the mean of infinitely many (or even hundreds of thousands) values.
Second, we don't know all values.
The whole point of statistical inference is to generalise from a relatively small sample to a population, simply because we can't look at every single object in the population.

The story should go like this:
Our theory makes a prediction about some parameter of a population (say, German sentences in a certain register have a mean length o $6.8$ words), and we try to test this prediction with a sample.
However, for this logic to apply, population values must have that parameter (such as a mean) in the first place.
Keep this in mind whenever we introduce any statistic for a sample.
The same statistics should always be defined (as a parameter) for the population as well.

\subsection{Spread and Shape of a Distribution}

<<variancesetup, cache=T, echo=FALSE, include=FALSE, results='asis'>>=
  set.seed(47)
  potsy <- c(potsy1 <- round(rnorm(50, 200, sd = 45), 0),
             potsy2 <- round(rnorm(50, 300, sd = 45), 0)) %>% sort
@

So far, we've discussed how to summarise the central tendency of a sample and how to plot it.
We've also argued that the same measures that we use to describe samples also describe populations.
As we'll discover throughout the book, it's equally important to quantify how spread out the values from a sample or a population are.
Let's illustrate this using two samples.
One is the larger sample of reaction times from Section~\ref{sec:numeric} (called \xsample\ here), the other one is a sample that is superficially similar (called \ysample).
Compare the summary of both samples in Table~\ref{tab:varxandy}.

<<varxandy, echo=F, cache=T, results="asis">>=
  numsumxy <- matrix(
    round(c(summary(potsx), length(potsx), summary(potsy), length(potsy)), 0),
    nrow = 7)
  rownames(numsumxy) <- c("Minimum", "First Quartile", "Median", "Mean", "Third Quartile", "Maximum", "Sample size")
  colnames(numsumxy) <- c("x", "y")

  numsumxy %>%
    as.data.frame.matrix %>%
    kable(align = "rr", booktabs = T, linesep = "",
          caption = "Summary of two numeric samples with n=100",
          label = "varxandy") %>%
    column_spec(1, bold = T) %>%
    row_spec(6, extra_latex_after = "\\midrule") %>%
    row_spec(0, bold=TRUE)
@

The two samples look similar in some ways.
The means are almost identical (\Sexpr{numsumxy[4,1]} and \Sexpr{numsumxy[4,2]}) and the medians are also close (\Sexpr{numsumxy[3,1]} and \Sexpr{numsumxy[3,2]}).
However, the minima and maxima as well as the first and third quartiles paint an interesting picture.
The extremes appear to be spreading out much more in sample \ysample\ (ranging from \Sexpr{numsumxy[1,2]} to \Sexpr{numsumxy[6,2]}) compared to sample \xsample\ (ranging from \Sexpr{numsumxy[1,1]} to \Sexpr{numsumxy[6,1]}).
This tendency is also visible with the first and third quartiles, which are much further apart in sample \xsample\ (ranging from \Sexpr{numsumxy[2,2]} to \Sexpr{numsumxy[5,2]}) than in sample \xsample\ (ranging from \Sexpr{numsumxy[2,1]} to \Sexpr{numsumxy[5,1]}).
These numbers indicate that the values of the sample are on average somewhat farther away from the mean in \ysample.
The \Key{spread} of \ysample\ is larger than the spread of \xsample.
Both a histogram and a density plot should be used to corroborate this and to find out what the shapes of the distributions are.
Figure~\ref{fig:xyhist} shows both types of plots for both samples.

<<xyhist, fig.cap=paste0("Histograms and density plots of two different samples of a numeric variable with n=100"), echo=F, fig.pos="t", cache=T>>=
  par(mfrow = c(2, 2), mar = c(4, 4, 2, 2), family = "Plotfont")

  h1 <- hist(potsx, breaks = 10, xlab = "Milliseconds", ylab = "Frequency", main ="", border=F, ylim = c(0, 27), xlim = c(0,500), col = the.midgray)
  h2 <- hist(potsy, breaks = 15, xlab = "Milliseconds", ylab = "Frequency", main ="", border=F, ylim = c(0, 27), xlim = c(0,500), col = the.midgray)

  densplo1 <- density(potsx, kernel = "cosine")
  qi <- which.min(abs(densplo1$x - summary(potsx)[2]))
  qii <- which.min(abs(densplo1$x - summary(potsx)[3]))
  qiii <- which.min(abs(densplo1$x - summary(potsx)[5]))
  plot(densplo1, xlab = "Milliseconds", main ="x", ylab = "Density",
       bty ="n", ylim = c(0, 0.012), xlim = c(0, 500), lwd = 1)
  lines(densplo1, lwd = the.lwd, col= the.darkgray)

  densplo2 <- density(potsy, kernel = "cosine")
  qi <- which.min(abs(densplo2$x - summary(potsy)[2]))
  qii <- which.min(abs(densplo2$x - summary(potsy)[3]))
  qiii <- which.min(abs(densplo2$x - summary(potsy)[5]))
  plot(densplo2, xlab = "Milliseconds", main ="y",  ylab = "Density",
       bty ="n", ylim = c(0, 0.012), xlim = c(0, 500), lwd = 1)
  lines(densplo2, lwd = the.lwd, col= the.darkgray)

  par(mfrow = c(1,1), mar = c(5.1, 4.1, 4.1, 2.1))
@ %

While the distribution of \xsample\ quickly rises and falls in on neat spike, the distribution of \ysample\ rises more slowly, then drops again, only to rise again.
By the way, notice the effect of the density estimator:
It averages over larger intervals.
The second spike around 350, which is clearly visible in the histogram, is so narrow that the averaging going on in the density estimate caps it significantly.
However, in both types of plots it looks like \ysample\ has two local maxima, which accounts for the larger spread.
Handling data that are distributed like this requires much more care compared to data that looks like \xsample, and we'll return to this later in the book.

As we've already pointed out, the data points in \ysample\ are \alert{on average farther away from the mean} than the ones in \xsample, although the means of both samples is virtually the same.
While the specific shapes of the distributions can't be summarised in one handy statistic, we have one statistic for the overall spread of a sample: the \textit{variance} (and the \textit{standard deviation} derived from it).

\subsection{Quantifiying Variance}\label{sec:quantifyvariance}

In order to illustrate the calculation of the variance, we return to the smaller sample from Section~\ref{sec:numeric} and call it \xsample.
We repeat the raw data here for convenience:

\begin{quote} \itshape
  $\xsample=\langle$\Sexpr{paste0(pots, collapse = ", ")}\,$\rangle$
\end{quote}

Its mean is \Sexpr{round(mean(pots, 0))}.
As we've argued that the variance is related to the distances of the individual points around the mean, we begin by plotting the data points (x-axis) and their measured reaction times (y-axis) in Figure~\ref{fig:meandist}.
We also add the mean as a horizontal line plus lines measuring the distance from the mean to the individual data points (using colour to differentiate between negative and positive distances).

<<meandist, fig.cap=paste0("Raw data plot of a sample with n=10; distances to mean plotted as vertical lines"), echo=F, fig.pos="t", cache=T>>=
  par.defaults()
  p <- plot(pots, xaxt = "n", xlab = "Sorted Data Points", ylab = "Milliseconds", main ="", ylim = c(150, 400), frame.plot = F)
  .mean <- round(mean(pots), 0)
  axis(1, 1:10, 1:10, tick = T)
  .devnull <- lapply(1:10, function(i) {
    .diff <- pots[i]-mean(pots)
    lines(c(i, i), c(pots[i], .mean), lwd = the.lwd,
          col = ifelse(.diff < 0, the.cols[3], the.cols[1]))
  })
  lines(c(1, 10), rep(round(mean(pots, 0)), 2), lty = 1, lwd = the.lwd+1, col = the.cols[2])
  points(1:10, pots, pch = the.pchs[1], cex = 2)
@

Obviously, 5 data points deviate negatively from the mean, and 5 deviate in the positive direction.
The way to calculate those distances is $x_i-\xsample$ for each point $x_i$
Table~\ref{tab:xdistances} show the results.%
\footnote{The mean is actually $\xmean=\Sexpr{mean(pots)}$.
We took the liberty of rounding it to an integer for clarity.}

<<xdistances, echo=F, cache=T, results="asis">>=
  distxy <- matrix(
    c(pots, paste0("$", pots, "-", round(.mean, 0), "=", round(pots-.mean, 0), "$")),
    nrow = 10)
  rownames(distxy) <- paste0("Data point ", 1:10)
  colnames(distxy) <- c("Measurement", "Distance")

  distxy %>%
    as.data.frame.matrix %>%
    kable(align = "cl", booktabs = T, linesep = "",
          caption = "Distances of individual data points to the mean (rounded to integers)",
          label = "xdistances", escape = F) %>%
    column_spec(1, bold = T) %>%
    row_spec(0, bold=TRUE)
@

Simply adding those distances wouldn't be very helpful.
Do you see why?
The problem is that negative and positive deviations cancel each other out:

\begin{center}
  \begin{math}
    \sum\limits_{i=1}^{n}(x_i-\xmean)=\sum\limits_{i=1}^{n}(x_i-\cfrac{\sum\limits_{j=1}^{n}x_j}{n})=0
  \end{math}
\end{center}

It would be best to make all values positive.
One option would be to take the \Key{absolute} value, which is achieved by discarding the minus sign.
While this is an option, another one is more appropriate for many further calculations in statistics: taking the square of the distance.
This makes their absolute value much bigger, but it also means that negative signs cancel out.
If we then sum up the squared distances, we get the squared total deviation of the data points from the mean.
It's called the \Key{sum of squares} of a sample \xsample, and we abbreviate it as $SQ_{\xsample}$, or simply $SQ$ if there is just one sample.%
\footnote{This is the abbreviation of German \textit{Summe der Quadrate}, but you can also read it as \textit{Sum of sQuares}.
We feel more comfortable with this abbreviation than with \textit{SS}, which is often used in English.}

\begin{equation}
  SQ_{\xsample}=\sum\limits_{i=1}^{n}(x_i-\bar{\xsample})^2
  \label{eq:sq}
\end{equation}

While the $SQ$ is the \textit{total} (summed) squared deviation from the mean, we're rather looking for the \textit{average} deviation per data point.
We can get there by dividing $SQ$ by $n$, which gives us the so-called sample \Key{variance} of \xsample\ denoted by $s^2_{\xsample}$, or simply $s^2$ if it's clear that we're talking about one sample only.
There is a minor catch:
We're going to use the variance from a sample as an estimate of the variance in the corresponding population.
Statisticians have found out that small samples tend to underestimate the variance of the population and that the best practical remedy is to divide by $n-1$ instead of $n$.
The larger $n$, the smaller the effect of this correction, and for populations, the correction is by definition unnecessary.
Thus, the sample variance is customarily computed as according to Equation~\ref{eq:variance} and the population variance (denoted $\sigma^2$ with a population size of $N$ and a mean of $\mu$) is understood to be given by Equation~{eq:variancepop}.

\begin{equation}
  s^2_{\xsample}=\cfrac{\sum\limits_{i=1}^{n}(x_i-\bar{\xsample})^2}{n-1}=\cfrac{SQ_{\xsample}}{n-1}
  \label{eq:variance}
\end{equation}

\begin{equation}
  \sigma^2=\cfrac{\sum\limits_{i=1}^{N}(x_i-\mu)^2}{N}
  \label{eq:variancepop}
\end{equation}

In order to arrive at the \textit{average deviation of the data points from the mean} instead of the \textit{average squared deviation of the data points from the mean}, we need to reverse the upscaling effect of squaring the distances when calculating $SQ$.
We can do this by taking the square root of the variance $s^2$.
The result is called the \Key{standard deviation}, denoted $s_{\xsample}$, or simply $s$ if it's clear that there is only one sample under discussion:

\begin{equation}
  s_{\xsample}=\sqrt{s^2}
  \label{eq:stdev}
\end{equation}

It is customary to switch between variance and standard deviation as the information conveyed by them is equivalent.
Most people will probably call the overall phenomenon \textit{variance}, but \textit{standard deviation} is the measure more frequently used in calculations.
Let's do it for the small sample \xsample, based on the distances to the mean from Table~\ref{tab:xdistances}.
First, we calculate $SQ_{\xsample}$:

\small

\begin{center}
  \begin{math}
    SQ_{\xsample}=%
    \Sexpr{paste("(", round(pots-.mean, 0), collapse = ")^2+")})^2=\Sexpr{paste((round(pots-.mean, 0))^2, collapse = "+")}=\Sexpr{format(sum((round(pots-.mean, 0))^2), scientific = F)}
  \end{math}
\end{center}

\normalsize

\noindent Then, we need to divide $SQ$ by $n-1$ to calculate the variance $s^2$:

\begin{center}
  \begin{math}
    s^2_{\xsample}=\cfrac{\Sexpr{format(sum((round(pots-.mean, 0))^2), scientific = F)}}{\Sexpr{length(pots)-1}}=\Sexpr{round(var(pots), 1)}
  \end{math}
\end{center}

\noindent And finally, we take the square root to calculate the standard deviation:

\begin{center}
  \begin{math}
    s_{\xsample}=\sqrt{\Sexpr{round(var(pots), 1)}}=\Sexpr{round(sqrt(round(var(pots), 1)),1)}
  \end{math}
\end{center}

The same could be done for the large \xsample\ from introduced with Table~\ref{tab:numericsummaryx}, but manually calculating the variance for any sample larger than 10 data points is really tedious (as you'll experience when working on the exercises).
Hence, we did it using software, and Table~\ref{tab:varxandyvar} summarises the larger \xsample\ and \ysample\ for you, including variance and standard deviation.
Finally, Table~\ref{tab:summaries} provides an overview of the statistics that we've introduced in this chapter and to which types of measurements they apply.
In the next chapter, we build upon these statistics and discuss the relation between populations and samples in more detail.
The ultimate goal is a flexible and general approach to statistical inference based on samples in later chapters.

<<varxandyvar, echo=F, cache=T, results="asis">>=
  numsumxy <- matrix(
    c(formatC(as.numeric(round(summary(potsx), 0), format="d")),
      round(var(potsx), 1),
      round(sd(potsx), 1),
      formatC(as.numeric(length(potsx), format="d")),
      formatC(as.numeric(round(summary(potsy), 0), format="d")),
      round(var(potsy),1),
      round(sd(potsy),1),
      formatC(as.numeric(length(potsy)), format="d")),
    nrow = 9)
  rownames(numsumxy) <- c("Minimum", "First Quartile", "Median", "Mean", "Third Quartile", "Maximum", "Variance", "Standard Deviation", "Sample size")
  colnames(numsumxy) <- c("x", "y")

  numsumxy %>%
    as.data.frame.matrix %>%
    kable(align = "rr", booktabs = T, linesep = "",
          caption = "Summary of two numeric samples with n=100 including variance and standard deviation",
          label = "varxandyvar") %>%
    column_spec(1, bold = T) %>%
    row_spec(6, extra_latex_after = "\\midrule") %>%
    row_spec(8, extra_latex_after = "\\midrule") %>%
    row_spec(0, bold=TRUE)
@

\begin{table}
  \caption{\label{tab:summaries}Adequate statistics and plots for summarising types of variables}
  \centering
  \resizebox{\textwidth}{!}{\begin{tabular}{lccccc}
    \toprule
                                   &                 & \multicolumn{3}{c}{\textbf{Discrete}}                 & \textbf{Continuous} \\\cmidrule{3-5}
                                   & \textbf{Symbol} & \textbf{Binary} & \textbf{Nominal} & \textbf{Ordinal} & \textbf{Numeric} \\
    \midrule
    \textbf{Sample Size}           & $n$                    & \multicolumn{4}{c}{\Dim number of data points} \\
    \textbf{Median}                & $\widetilde{\xsample}$ & \multicolumn{4}{c}{\CellBlue middle value of sorted sample} \\
    \textbf{Mode}                  & $\dot{\xsample}$       & \multicolumn{3}{c}{\Dim most frequent value} & \CellGreen density maximum \\
    \textbf{Mean}                  & $\bar{\xsample}$       & \multicolumn{3}{c}{\CellBlue ---} & \Dim Equation~\ref{eq:mean} \\
    \textbf{Variance}              & $s^2$                  & \multicolumn{3}{c}{\Dim ---} & \CellGreen Equation~\ref{eq:variance} \\
    \textbf{Standard Deviation}    & $s$                    & \multicolumn{3}{c}{\CellBlue ---} & \Dim Equation~\ref{eq:stdev} \\
    \textbf{Plot}                  &                        & \multicolumn{3}{c}{\Dim bar plot} & \CellGreen histogram, density plot \\
    \bottomrule
  \end{tabular}}
\end{table}


\begin{exercises}

\exercise{describing1} One Two Three

\end{exercises}
