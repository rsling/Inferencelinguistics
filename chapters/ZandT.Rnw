% !Rnw root = ../main.Rnw

<<setupzandt, cache=FALSE, echo=FALSE, include=FALSE, results='asis'>>=
opts_knit$set(self.contained=FALSE)
@

\chapter{Inference: Mean Differences}
\label{sec:zandt}

\section*{Overview}

In this chapter, we introduce the z-test and the t-test.
We have introduced the logic of frequentist testing (Chapter~\ref{sec:fisher}), shown how samples can be summarised through measures of central tendency (Chapter~\ref{sec:describing}), and we've shown how we can adjust the safety and the precision of means and proportions estimated from samples (Chapter~\ref{sec:confidence}).
In this chapter, we return to inferential tests using the results from the chapters on descriptive statistics and estimation.

The tests introduced in this chapter compare means of samples.
The z-test compares the mean of a sample with the mean of a population for which the true mean and the true variance are known for sure.
The t-test for a single sample does the same but for cases where the true variance isn't known.
Finally, the t-test for two samples compares means of two independent samples.

The substantive hypothesis in such tests is usually that the means differ.
Hence, the Null is that they are the same.
We calculate the frequentist probability of measuring the difference between the means that was actually measured (or a larger difference) if the Null is true.
The frequentist probability is not calculated directly as in our presentation of Fisher's Exact Test.
Instead, we calculate a test statistic ($z$ or $t$), for which the probability density is known (in the form of the Normal Distribution and the very similar t Distribution) and we can use it to look up p-values.

\Problem{Z-Tests}{%
Let's assume you know the mean reaction time for a critical region when native speakers process a certain type of relative clause.
This mean reaction time and the corresponding variance in measurements are extremely well established parameters.
They were predicted by a robust theory of syntactic processing, and this prediction has been corroborated by a large number of diverse experiments.
For an emergent subtype of this kind of relative clause, the theory predicts considerably higher precessing effort and thus longer reaction times.
You conduct an experiment and measure reaction times in the critical region.
\alert{Which outcomes of the experiment would you interpret as indidcating that reaction times are indeed longer for the emergent type of relative clause?}}

\section{Population Means and Sample Means}

\subsection{Introducing the Logic}

The Problem Statement exemplifies a simple question:
Given a known population mean, do means measured under a specific condition diverge from this known mean?
In this section, we show through simple frequentist reasoning how measurements from experiments can provide evidence to answer such questions.%
\footnote{Again, we caution readers that we can neither (by no means!) \textit{decide} the questions nor \textit{provide hard evidence} for any possible answer to them, etc.}
The simplest test for such tasks is the \Key{z-Test}.
Notice that for the z-Test to be applicable, the given population mean (and the corresponding variance) must be \textit{known}!
The test does not take into account any uncertainty in the value for the known population mean, and if you disregard that fact, you will end up in inference hell.
This is why the Problem Statement stresses that the mean was predicted by a robust theory and that the prediction was tested in a long series of experiments.
If these conditions are not met, other tests (such as the t-test for two samples) might apply, and we're going to introduce such tests as we go along.

<<z.setup, echo=FALSE>>=
  set.seed(3689)
  z.n <- 1000
  z.mu <- 120
  z.sigma <- 4
  z.sample <- rnorm(n = z.n, mean = z.mu, sd = z.sigma)
  z.minmax <- c(105, 135)
  z.dnorm <- function(x) {
    dnorm(x, mean = z.mu, sd = z.sigma)
  }

  set.seed(673)
  z2.n <- 16
  z2.mu <- 123
  z2.sample <- rnorm(n = z2.n, mean = z2.mu, sd = z.sigma)
  z2.dnorm <- function(x) {
    dnorm(x, mean = z2.mu, sd = z.sigma)
  }

  z2.samplemean <- mean(z2.sample)

  z2.se <- z.sigma/sqrt(z2.n)
  z2.se.rnd <- round(z2.se, 2)
  z2.dnorm.sample <- function(x) {
    dnorm(x, mean = z.mu, sd = z2.se)
  }

  z2.mu <- mean(z2.sample)
  z2.z <- (mean(z2.sample)-z.mu)/z2.se.rnd
  z2.z.rnd <- round(z2.z, 2)

  z2.p <- 1-pnorm(z2.z)
  z2.p.rnd <- round(z2.p, 3)
@

For the sake of illustration, let's assume that the population mean is $\mu=\Sexpr{z.mu}$ (for example milliseconds) and the population variance is\label{page:varianceisfour} $\sigma^2=\Sexpr{z.sigma^2}$, which corresponds to a standard deviation of $\sigma=\Sexpr{z.sigma}$.
If the population values are generated according to a normal distribution, values are distributed according to the now well-known \Key{Probability Density Function} (PDF) in Figure~\ref{fig:z1}.

<<z1, fig.cap=paste0("Theoretical population distribution for a normal distribution with μ=", z.mu, " and σ=", z.sigma, " and a simulated random sample of n=", z.n, " measurements from the population"), echo=F, out.width="85%", fig.pos="t", cache=F>>=

par(mfrow = c(2,1), mar = c(2,1,1,1))

plot(x = seq(z.minmax[1], z.minmax[2], 0.1),
     y = z.dnorm(seq(z.minmax[1], z.minmax[2], 0.1)),
     type = "l", lwd = the.lwd,
     bty="n", yaxt="n", ylab = "",
     xlim = z.minmax, col = "white"
)
lines(x = c(z.mu, z.mu), y = c(0, z.dnorm(z.mu)),
      lwd = the.lwd, col = the.cols[2])
lines(x = c(z.mu-z.sigma, z.mu-z.sigma), y = c(0, z.dnorm(z.mu-z.sigma)),
      lwd = the.lwd, col = the.cols[1])
lines(x = c(z.mu+z.sigma, z.mu+z.sigma), y = c(0, z.dnorm(z.mu+z.sigma)),
      lwd = the.lwd, col = the.cols[1])
text(x = z.mu+0.6, y = z.dnorm(z.mu)/2, labels = "μ", col = the.cols[2])
text(x = (z.mu-z.sigma)+1.1, y = z.dnorm(z.mu-z.sigma)/3, labels = "μ-σ",
     col = the.cols[1])
text(x = (z.mu+z.sigma)+1.3, y = z.dnorm(z.mu+z.sigma)/3, labels = "μ+σ",
     col = the.cols[1])
lines(x = seq(z.minmax[1], z.minmax[2], 0.1),
     y = z.dnorm(seq(z.minmax[1], z.minmax[2], 0.1)),
     lwd = the.lwd, col = the.darkgray
)

plot(x = z.sample, y = 1:length(z.sample),
     pch = 20,
     bty="n", yaxt="n", ylab = "", cex = 1.5,
     xlim = z.minmax, col = alpha("darkgray", 0.5))

par(mfrow = c(1,1))
@

To recapitulate, the PDF gives for each measurement (x-axis) the probability with which it occurs (y-axis).
Informally speaking, the curve shows that if we measure random values from this population, the probability of measuring a value close to $\mu$ is highest, and measurements deviate on average by the standard deviation $\sigma$ from $\mu$.
The blue line shows the mean, and the green lines show one standard deviation in each direction from the mean.
As a result, a very much expected sample of $n=\Sexpr{z.n}$ measurements is shown in the form of the point cloud below the curve (90° rotated compared to the raw data plots we've shown before).
It's a simulated sample, and the simulation was set up according to the known facts about the overall population ($\mu=\Sexpr{z.mu}$and $\sigma^2=\Sexpr{z.sigma^2}$).
The measurements are indeed centred around the mean, and they seem to follow the normal distribution.

If, however, we draw a sample from a different population where the true mean is slightly higher (for example because we're measuring reaction times under a condition that incurs a processing penalty) we expect samples to turn out differently on average and have a higher sample mean compared to the known population mean.
However, this on-average expectation can be treacherous because individual samples are not in any way \textit{guaranteed} to represent their population well, as we have shown in Chapter~\ref{sec:confidence}.
Very similar to Ronald A.~Fisher in his experiment with Muriel Bristow (see Chapter~\ref{sec:fisher}), we need to ask whether the actual sample warrants any inference regarding the underlying mechanism.
It does that if it's a sufficiently unexpected result under the assumption that the desired inference is not correct, \ie, under the Null.
It's sufficiently unexpected if it had a low pre-experiment probability of occurring.

We'll show how this works out for the given example.
In the case of the reaction times described in the Problem Statement, substantive hypothesis is that reaction times are higher with the emergent subtype of relative clauses because of assumed processing penalties.
However---especially if our sample is small---inferring anything from a specific result is tricky.
Figure~\ref{fig:z2} shows a possible outcome with $n=\Sexpr{z2.n}$ in red, and it should be obvious why it's tricky to infer anything from it.
The mean from the sample is higher than the theoretically known mean, but this might very well be just a random deviation.
In frequentist terms, such a sample is expected under the Null as well.
We need to know the probability of a sample that deviates from the known mean to such a degree (or a stronger degree) in order to proceed to an inference.

<<z2, fig.cap=paste0("Theoretical population distribution for a normal distribution with μ=", z.mu, " and σ=", z.sigma, " and a simulated random sample of n=", z2.n, " measurements from some population"), echo=F, out.width="85%", fig.pos="t", cache=F>>=

par(mfrow = c(2,1), mar = c(2,1,1,1))

plot(x = seq(z.minmax[1], z.minmax[2], 0.1),
     y = z.dnorm(seq(z.minmax[1], z.minmax[2], 0.1)),
     type = "l", lwd = the.lwd,
     bty="n", yaxt="n", ylab = "",
     xlim = z.minmax, col = "white"
)
lines(x = c(z.mu, z.mu), y = c(0, z.dnorm(z.mu)),
      lwd = the.lwd, col = the.cols[2])
lines(x = c(z.mu-z.sigma, z.mu-z.sigma), y = c(0, z.dnorm(z.mu-z.sigma)),
      lwd = the.lwd, col = the.cols[1])
lines(x = c(z.mu+z.sigma, z.mu+z.sigma), y = c(0, z.dnorm(z.mu+z.sigma)),
      lwd = the.lwd, col = the.cols[1])
text(x = z.mu+0.6, y = z.dnorm(z.mu)/2, labels = "μ", col = the.cols[2])
text(x = (z.mu-z.sigma)+1.1, y = z.dnorm(z.mu-z.sigma)/3, labels = "μ-σ",
     col = the.cols[1])
text(x = (z.mu+z.sigma)+1.3, y = z.dnorm(z.mu+z.sigma)/3, labels = "μ+σ",
     col = the.cols[1])
lines(x = seq(z.minmax[1], z.minmax[2], 0.1),
     y = z.dnorm(seq(z.minmax[1], z.minmax[2], 0.1)),
     lwd = the.lwd, col = the.darkgray
)

plot(x = z.sample, y = 1:length(z.sample),
     pch = 20, cex = 1.5,
     bty="n", yaxt="n", ylab = "",
     xlim = z.minmax, col = alpha("darkgray", 0.5))
points(x = z2.sample, y = sample(1:length(z.sample), length(z2.sample)),
     pch = 20, cex = 2,
     bty="n", yaxt="n", ylab = "",
     xlim = z.minmax, col = alpha(the.cols[3], 0.75))
abline(v = rep(mean(z2.sample)),
      lwd = the.lwd, col = the.cols[3])
text(x = mean(z2.sample)+2, y = 300,
     labels = bquote(bar("x")*"="*.(round(mean(z2.sample), 2))),
     col = the.cols[3])

par(mfrow = c(1,1))
@

Let's call the sample plotted in Figure~\ref{fig:z2} \xsample, a tuple of \Sexpr{z2.n} measurements $x_1$ through $x_{\Sexpr{z2.n}}$.
The mean of \xsample\ is $\xmean=\Sexpr{round(mean(z2.sample), 2)}$.
As we've shown, inferences in frequentist logic (see Chapter~\ref{sec:fisher}) are always made by taking into account what the outcome of an experiment could have been under one or several possibly correct hypotheses.
In the case at hand, we're interested in the hypothesis that the true mean under the experimental condition (call it $\mu_1$) is larger than the known mean $\mu$.
In other words, we would like to \alert{gather evidence in support of a substantive hypothesis: $\mu_1>\mu$}.
We use the symbol $\mu_1$ for the hypothesised larger mean as it is the mean of a slightly different theoretical population, and $\mu$ is the symbol reserved for population means.

For several reasons, we cannot gather evidence that supports this hypothesis directly.
First, as a population mean $\mu_1$ is obviously not directly observable.
We can virtually never observe whole populations, all we've got are sample.
Rather, $\mu_1$ is a hypothesised mean in a population that exists as separate from the known population if the substantive hypothesis is correct.
However, if that population is not substantially different from the known one, then we have $\mu_1=\mu$.
While we certainly hope that \xmean\ is a good indicator of the true value $\mu_1$, we have no guarantees whatsoever that it actually is.
Second, as Fisherians we have no formal method of gathering positive evidence.%
\footnote{Only Neyman-Pearson philosophy and the Severity approach will give us this power (pun intended) later in Chapter~\ref{sec:powerseverity}, but with some important caveats.}
Therefore, we can only check \alert{whether the data \xmean\ are in accord with a Null $H_0: \mu_1\not{>}\mu$}, which is equivalent to $H_0: \mu_1\leq\mu$.

Let's recapitulate the frequentist logic of inference and adapt it to the case at hand:
To test this hypothesis, frequentism assigns a certain well-defined kind of probability to (obtaining) the data \xsample\ given that the Null is correct.
This probability can be used to assess whether the data \xsample\ are in accord with the Null.
First, what do we infer from the data \xsample\ (in other words, from our experiment) if they are compatible with the Null?
You're right, absolutely nothing!
If the data are in accord with the Null, we haven't found any evidence that it is not the case that $\mu_1$ is not greater than $\mu$, and that's the end of it.
If that sounds uselessly messed up and disappointing, that's because it is.%
\footnote{Whether you're a linguist or not, please consider that \textit{finding no evidence that A is true} is not the same as \textit{finding evidence that A is false}.}
If you infer anything from such a result, you're not only wrong, but also Bayesians (many of them with Dutch names) will come to haunt you (or at least unfollow you on the messaging platform of your choice)---and they'd be somewhat right to do so.
Second, what do we infer from the data \xsample\ if they are not compatible with the Null?
This is the much more interesting case, but it's difficult to define the admissible inferences without creating false ideas.
Let's say rather informally that in such a case we've found some evidence in support of the substantive hypothesis because it and the Null partition the range of possible values of $\mu_1$:
either it's greater than $\mu$ or it is not greater than (\ie, smaller than or equal to) $\mu$ ($H_0$).
Finding no accordance with the Null despite serious attempts to do so (see Chapter~\ref{sec:powerseverity}) provides at least some indication that the substantive hypothesis might be correct.
However, if you're looking for a \alert{proof} of anything, we recommend that you stick to pure theory, logic, theoretical maths, or pseudoscience.
There is no proof to be found in (non-trivial) experiments, and statistical inferences are weak and fragile.

\subsection{Extreme Means Under the Null}

We have argued that in Fisherian inference, we have to assess whether \xsample\ and its mean \xmean\ are compatible with the Null.
But how do we do this?
Because we're comparing means (\ie, numeric values) and not merely counting occurrences of correct and incorrect tea-first detections, it seems difficult to compute the number of all possible outcomes and the number of outcomes as extreme as or more extreme than the one we actually observed.
After all, we're dealing with numeric measurements (real values), and there's always a result in between two results, \eg, between $\xmean=123.99999$ and $\xmean=124$, there are infinitely many other possible results such as $\xmean=123.999991$, $\xmean=123.9999911$, etc.
Well, it's not totally impossible to calculate exact probabilities, but there's a convenient shortcut.
We'll go through it step by step.

The most naive but not at all wrong thing to do is to calculate the difference between the known population mean $\mu$ and the mean of the obtained sample \xmean.
In our case, this is $\xmean-\mu=\Sexpr{round(mean(z2.sample)-z.mu,2)}$.
Clearly, a minimal requirement for any further calculations is that this difference is positive.
If it were negative, the sample could hardly be interpreted as evidence against N: $\mu_1\leq\mu$.%
\footnote{This way of putting it is slightly sloppy and informal.
We will return to this notion and make it more precise in Chapter~\ref{sec:powerseverity}.
However, in practice it is blatantly obvious that we would never take an experiment that showed lower reaction times as evidence for higher reaction times, etc.}
While this is something one should always do first, it's not suitable for serious inference due to one main reason:
It doesn't take into account how large the sample was.

We now follow a very similar logic as in our introduction to Fisher's Exact Test (Chapter~\ref{sec:fisher}).
The question is:
How often would we expect to see a sample of $\Sexpr{z2.n}$ measurements with a sample mean of $\Sexpr{round(mean(z2.sample),2)}$ or larger if the true mean is that specified by the Null, which is $\mu=\Sexpr{z.mu}$?
Luckily, we have already introduced the tool that we need: the \Key{standard error} of the mean $\ssigma_{\mu}$.
The standard error for $n=\Sexpr{z2.n}$ and the known variance $\sigma=\Sexpr{z.sigma}$ (see p.~\pageref{page:varianceisfour}) tells us how strongly samples of this size deviate from the true mean (on average) in each direction.
The standard error of the mean in this case is (revise Equation~\ref{eq:semean} if necessary):

\begin{center}\begin{math}
  \ssigma_{\mu}=\frac{\Sexpr{z.sigma}}{\sqrt{\Sexpr{z2.n}}}=\sqrt{\frac{\Sexpr{z.sigma}^2}{\Sexpr{z2.n}}}=\Sexpr{z2.se.rnd}
\end{math}\end{center}

<<z3, fig.cap=paste0("Theoretical population distribution for a normal distribution with μ=", z.mu, " and σ=", z.sigma, " (green) the distribution of sample means for samples from this distribution with n=", z2.n, " (black)"), echo=F, fig.pos="t", out.width="85%", cache=F>>=

z.minmax <- c(110, 130)

plot(x = seq(z.minmax[1], z.minmax[2], 0.1),
     y = z2.dnorm.sample(seq(z.minmax[1], z.minmax[2], 0.1)),
     type = "l", lwd = the.lwd,
     bty="n", yaxt="n", ylab = "", xlab = "",
     xlim = z.minmax, col = "white"
)
lines(x = seq(z.minmax[1], z.minmax[2], 0.1),
      y = z.dnorm(seq(z.minmax[1], z.minmax[2], 0.1)),
      col = the.cols[1], lwd = the.lwd)

lines(x = c(z.mu, z.mu), y = c(0, z.dnorm(z.mu)),
      lwd = the.lwd, col = the.cols[2])
lines(x = c(z.mu, z.mu), y = c(z.dnorm(z.mu), z2.dnorm.sample(z.mu)),
      lwd = the.lwd, col = the.cols[2])

lines(x = seq(z.minmax[1], z.minmax[2], 0.1),
     y = z2.dnorm.sample(seq(z.minmax[1], z.minmax[2], 0.1)),
     lwd = the.lwd, col = the.darkgray
)

text(x = z.mu+0.6, y = z.dnorm(z.mu)/2, labels = "μ", col = the.cols[2])
@

Remember what the standard error is all about (Chapter~\ref{sec:confidence}).
If the mean in a population is $\mu$ and the standard deviation is $\sigma$, then the sample means of samples of size $n$ are themselves normally distributed, and the standard error $\ssigma_{\mu}$ is the standard deviation of that normal distribution.
Furthermore, keep in mind that we're talking about the distribution of sample means drawn from a known population.
Under the Null, it is also the distribution from which our small sample was drawn.
Figure~\ref{fig:z3} contrasts the density of the distribution of individual data points (in our example: individual reaction times) with the much narrower distribution of sample means.
Mathematically, it is narrower because the standard error is always smaller or equal to the standard deviation.
Intuitively, it should be narrower because on average sample means from samples with $n>1$ approximate the true mean better than single measurements.%
\footnote{Understanding this argument is crucial. If you're not following it, you should go back to Chapter~\ref{sec:confidence} for an introduction to the distribution of sample means, especially the argument concerning samples of increasing sizes in Sections~\ref{sec:samplinberries} and~\ref{sec:milli}.}

Remember from Chapter~\ref{sec:confidence} that a Normal Distribution is exhaustively defined by the parameters $\mu$ and $\sigma$.
Since (i)~the distribution of sample means is Normal, (ii)~we know its mean $\mu$, (iii)~we know its variance $\sigma^2$ and standard deviation $\sigma$, we can calculate how many samples (in the long run) drawn from the known population would have a mean of $\Sexpr{round(mean(z2.sample),2)}$ or larger.
In other words, we can calculate how many sample means (from samples of size $n=\Sexpr{z2.n}$) would deviate by $\Sexpr{round(mean(z2.sample)-z.mu,2)}$ from the population mean anyway due to expected sampling error.
Instead of calculating the numbers and probabilities of individual events as in the Tea-Tasting Experiment, we use the known functions of the Normal Distribution to look up those values.
This gives us a very precise and well-defined measure of how surprising the obtained result would be if the Null were true.

<<z4, fig.cap=paste0("Left: PDF of the distribution of sample means for mean µ and standard error SE with obtained sample mean $\\xmean$ and corresponding z-value; area under the curve for results equal to or greater than $\\xmean$ is shaded; Right: CDF for the same distribution with obtained sample mean $\\xmean$"), echo=F, fig.pos="t", out.width="100%", cache=F>>=

par(mfrow=c(1,2))

z.minmax <- c(114, 126)
xs <- seq(z.minmax[1], z.minmax[2], 0.1)
ys <- z2.dnorm.sample(seq(z.minmax[1], z.minmax[2], 0.1))

plot(x = xs,
     y = ys,
     type = "l", lwd = the.lwd,
     bty="n", yaxt="n", ylab = "", xlab = "",
     xlim = z.minmax, col = "white"
)

lines(x = c(z.mu, z.mu), y = c(0, z2.dnorm.sample(z.mu)),
      lwd = the.lwd, col = the.cols[2])

shade(xs, ys, 122 , z.minmax[2],
      col = the.lightcols[1], border=the.cols[1], lwd = the.lwd)

text(x = z.mu+3.25, y = z2.dnorm.sample(z.mu)/2.5, col = the.cols[2],
     labels = bquote(μ*"="*.(round(mean(z.mu), 2)))
     )
text(x = mean(z2.sample)+2.25, y = z2.dnorm.sample(mean(z2.sample)),
     labels = bquote(bar("x")*"="*.(round(mean(z2.sample), 2))),
     col = the.cols[1]
     )

lines(x = c(
  z.mu, mean(z2.sample)-0.25,
      y = 0.01+rep(z2.dnorm.sample(z2.mu), 2),
      lwd = the.lwd, col = the.cols[1]
      )
)

#text(x = mean(z2.sample)+1.5, y = z2.dnorm.sample(mean(z2.sample))+0.02,
#     labels=paste0("z=", z2.z.rnd),
#     col = the.cols[1]
#     )

lines(x = xs,
     y = ys,
     lwd = the.lwd, col = the.darkgray
)

# CDF

xs <- seq(z.minmax[1], z.minmax[2], 0.1)
ps <- pnorm(xs, mean = z.mu, sd = z2.se)

plot(xs,
     ps,
     type = "l", lwd = the.lwd,
     bty="n", ylab = "", xlab = "", xaxt = "n",
     xlim = c(114, 126), col = "white")

axis(1, seq(114, 126, 4), seq(114, 126, 4))

lines(x = c(z.mu, z.mu),
      y = c(0, 0.5),
      lwd = the.lwd, col = the.cols[2])
lines(x = c(z.mu, 110),
      y = rep(0.5, 2),
      lwd = the.lwd, col = the.lightcols[2])
text(114.5, 0.45, labels = 0.5, col = the.cols[2])
text(z.mu+0.5, 0.2, "μ", col = the.cols[2])

lines(x = c(z2.samplemean, z2.samplemean),
      y = c(0, ps[which.closest(xs, z2.samplemean)]),
      lwd = the.lwd, col = the.cols[1])
lines(x = c(z2.samplemean, 110),
      y = rep(ps[which.closest(xs, z2.samplemean)], 2),
      lwd = the.lwd, col = the.lightcols[1])
text(115, 0.93, labels = round(ps[which.closest(xs, z2.samplemean)], 2), col = the.cols[1])
text(z2.samplemean+0.5, 0.2, labels = bquote(bar(x)), col = the.cols[1])

lines(x = xs,
     ps,
     lwd = the.lwd, col = the.darkgray
)

par.defaults()

@

The left panel of Figure~\ref{fig:z4} show the PDF of the distribution of sample means, and the highlightes area under the curve corresps to results as extreme or more extreme than \xmean.
The corresponding \Key{cumulative distribution function} (CDF) can be used to calculate the statistics of interest.
The right panel of Figure~\ref{fig:z4} shows the Normal CDF for $\mu=\Sexpr{z.mu}$ and $\sigma=\Sexpr{z2.se}$ (which happens to be $SE_{\mu}$).
For the observed sample mean $\xmean=\Sexpr{round(z2.samplemean,1)}$, the CDF has the value $\Sexpr{round(pnorm(z2.samplemean, z.mu, z2.se), 2)}$, which means that the probability of obtaining this or a more extreme result is $1-\Sexpr{round(pnorm(z2.samplemean, z.mu, z2.se), 2)}=\Sexpr{1-round(pnorm(z2.samplemean, z.mu, z2.se), 2)}$.
Think of the value $\Sexpr{round(pnorm(z2.samplemean, z.mu, z2.se), 2)}$ as indicating that $\Sexpr{round(pnorm(z2.samplemean, z.mu, z2.se), 2)*100}$\% of the probability mass lie to the right of $\xmean=\Sexpr{round(z2.samplemean,1)}$.
If you have access to a software that has built-in functions for such CDFs, this is the most straightforward way to arrive at a p-value for a sample mean and a known Normal Distribution.
In this case, you don't even need to calculate $z$ to perform a z-test.

Otherwise, you can calculate the so-called z-score and look up the p-value in a table.
To to this, we simply count the distance between the distribution mean and the sample mean in multiples of the standard error, which gives us the \Key{z-score}.
It's the same $z$ we introduced in Chapter~\ref{sec:confidence}, except we're using it the other way round.
For estimation, we looked up z-scores corresponding to probabilities, now we're calculating z-scores in order to lookup probabilities.
In this case, it's particularly easy because $SE_{\mu}=1$:

\begin{center}\begin{math}
z=\frac{\xmean-\mu}{SE_{\mu}}=\frac{\Sexpr{round(mean(z2.sample), 2)}-\Sexpr{z.mu}}{\sqrt{\frac{4^2}{16}}}=\frac{\Sexpr{round(mean(z2.sample)-z.mu, 2)}}{\Sexpr{z2.se.rnd}}=\Sexpr{z2.z.rnd}
\end{math}\end{center}

By dividing the distance by the standard error and calculating the z-score, we normalise it and make its interpretation independent of the concrete slope ($\sigma$) of the distribution.
The z-score is the distance from the mean in a Standard Normal Distribution (with $\mu=0$ and $\sigma=1$) that corresponds to the measured distance from the known mean.
Hence, $z=\Sexpr{z2.z.rnd}$ can be interpreted independently of the concrete sample mean and population mean, and it establishes a direct link to the probability we're looking for: another \Key{p-value}.
We can use tables to look up p-values corresponding to z-scores, and for $z=2.1$, we get $Pr=0.02$, which in our application is the p-value $p=0.02$.
Table~\ref{tab:ptable} shows such a table, and it's the reverse of tables such as Table~\ref{tab:ztable}.
The values are those for the Standard Normal Distribution with $\mu=0$ and $\sigma=1$.
Do sou see why?

<<ptable, echo=F, cache=T, out.width="85%", results="asis">>=
  zs <- seq(1.5, 2.5, 0.1)
  ps <- round(pnorm(zs, lower.tail = F), 2)
  ztable <- data.frame(Z=zs, Pr = ps)

  ztable %>%
    as.data.frame.matrix %>%
    kable(align = c("c", "c"), booktabs = T, linesep = "",
          caption = "Probabilities for Some Z-Values",
          label = "ptable") %>%
    row_spec(0, bold=TRUE)
@

Clearly, either the Null is false or the Null is true and a relatively rare event has occurred.
Whether a chance of 1 in 50 (equivalent to $p=0.02$) is rare or unexpected enough to make an inference can only be decided by you based on your knowledge of the field you're working in.
How precise are your measurements?
What magnitude does your theory predict the difference should be?%
\footnote{It doesn't make numeric predictions?
Blimey! Then back to the drawing board.}
How does the measured difference in reading times compare to differences observed for similar processing penalties?
In corpus linguistics, we fail to see how $p=0.02$ would be surprising enough in any situation to warrant substantive inferences.

\subsection{The Difference Between Error Intervals and the Z-Test}

The z-test allows us to

\subsection{Differences Between the Fisher Test and the Z-Test}

\subsection{The Distribution of P Values}

\section{The Undiscovered Population}

\subsection{Accounting for Unknown Variance}

\subsection{Comparing Two Means From Undiscovered Populations}






