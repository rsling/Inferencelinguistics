% !Rnw root = ../main.Rnw

<<setupzandt, cache=T, echo=FALSE, include=FALSE, results='asis'>>=
opts_knit$set(self.contained=FALSE)
@

\chapter{Inference: Mean Differences}
\label{sec:zandt}

\section*{Overview}

In this chapter, we introduce the z-test and the t-test.
We have introduced the logic of frequentist testing (Chapter~\ref{sec:fisher}), shown how samples can be summarised through measures of central tendency (Chapter~\ref{sec:describing}), and we've shown how we can adjust the safety and the precision of means and proportions estimated from samples (Chapter~\ref{sec:confidence}).
In this chapter, we return to inferential tests using the results from the chapters on descriptive statistics and estimation.

The tests introduced in this chapter compare means of samples.
The z-test compares the mean of a sample with the mean of a population for which the true mean and the true variance are known for sure.
The t-test for a single sample does the same but for cases where the true variance isn't known.
Finally, the t-test for two samples compares means of two independent samples.

The substantive hypothesis in such tests is usually that the means differ.
Hence, the Null is that they are the same.
We calculate the frequentist probability of measuring the difference between the means that was actually measured (or a larger difference) if the Null is true.
The frequentist probability is not calculated directly as in our presentation of Fisher's Exact Test.
Instead, we calculate a test statistic ($z$ or $t$), for which the probability density is known (in the form of the Normal Distribution and the very similar t Distribution) and we can use it to look up p-values.

\Problem{Inferences About Means}{%
Let's assume you know the mean reaction time for a critical region when native speakers process a certain type of relative clause.
This mean reaction time and the corresponding variance in measurements are extremely well established parameters.
They were predicted by a robust theory of syntactic processing, and this prediction has been corroborated by a large number of diverse experiments.
For an emergent subtype of this kind of relative clause, the theory predicts considerably higher precessing effort and thus longer reaction times.
You conduct an experiment and measure reaction times in the critical region.
\alert{Which outcomes of the experiment would you interpret as indidcating that reaction times are indeed longer for the emergent type of relative clause?}}

\section{Population Means and Sample Means}

\subsection{Introducing the Logic}

The Problem Statement exemplifies a simple question:
Given a known population mean, do means measured under a specific condition diverge from this known mean?
In this section, we show through simple frequentist reasoning how measurements from experiments can provide evidence to answer such questions.%
\footnote{Again, we caution readers that we can neither (by no means!) \textit{decide} the questions nor \textit{provide hard evidence} for any possible answer to them, etc.}
The simplest test for such tasks is the \Key{z-Test}.
Notice that for the z-Test to be applicable, the given population mean (and the corresponding variance) must be \textit{known}!
The test does not take into account any uncertainty in the value for the known population mean, and if you disregard that fact, you will end up in inference hell.
This is why the Problem Statement stresses that the mean was predicted by a robust theory and that the prediction was tested in a long series of experiments.
If these conditions are not met, other tests (such as the t-test for two samples) might apply, and we're going to introduce such tests as we go along.

<<z.setup, echo=FALSE>>=
  set.seed(3689)
  z.n <- 1000
  z.mu <- 120
  z.sigma <- 4
  z.sample <- rnorm(n = z.n, mean = z.mu, sd = z.sigma)
  z.minmax <- c(105, 135)
  z.dnorm <- function(x) {
    dnorm(x, mean = z.mu, sd = z.sigma)
  }

  set.seed(673)
  z2.n <- 16
  z2.mu <- 123
  z2.sample <- rnorm(n = z2.n, mean = z2.mu, sd = z.sigma)
  z2.dnorm <- function(x) {
    dnorm(x, mean = z2.mu, sd = z.sigma)
  }

  z2.samplemean <- mean(z2.sample)

  z2.se <- z.sigma/sqrt(z2.n)
  z2.se.rnd <- round(z2.se, 2)
  z2.dnorm.sample <- function(x) {
    dnorm(x, mean = z.mu, sd = z2.se)
  }

  z2.mu <- mean(z2.sample)
  z2.z <- (mean(z2.sample)-z.mu)/z2.se.rnd
  z2.z.rnd <- round(z2.z, 2)

  z2.p <- 1-pnorm(z2.z)
  z2.p.rnd <- round(z2.p, 3)
@

For the sake of illustration, let's assume that the population mean is $\mu=\Sexpr{z.mu}$ (for example milliseconds) and the population variance is\label{page:varianceisfour} $\sigma^2=\Sexpr{z.sigma^2}$, which corresponds to a standard deviation of $\sigma=\Sexpr{z.sigma}$.
If the population values are generated according to a normal distribution, values are distributed according to the now well-known \Key{Probability Density Function} (PDF) in Figure~\ref{fig:z1}.

<<z1, fig.cap=paste0("Theoretical population distribution for a normal distribution with μ=", z.mu, " and σ=", z.sigma, " and a simulated random sample of n=", z.n, " measurements from the population"), echo=F, fig.pos="t", cache=T>>=

par(mfrow = c(2,1), mar = c(2,1,1,1), family = "Plotfont")

plot(x = seq(z.minmax[1], z.minmax[2], 0.1),
     y = z.dnorm(seq(z.minmax[1], z.minmax[2], 0.1)),
     type = "l", lwd = the.lwd,
     bty="n", yaxt="n", ylab = "",
     xlim = z.minmax, col = "white"
)
lines(x = c(z.mu, z.mu), y = c(0, z.dnorm(z.mu)),
      lwd = the.lwd, col = the.cols[2])
lines(x = c(z.mu-z.sigma, z.mu-z.sigma), y = c(0, z.dnorm(z.mu-z.sigma)),
      lwd = the.lwd, col = the.cols[1])
lines(x = c(z.mu+z.sigma, z.mu+z.sigma), y = c(0, z.dnorm(z.mu+z.sigma)),
      lwd = the.lwd, col = the.cols[1])
text(x = z.mu+0.6, y = z.dnorm(z.mu)/2, labels = "μ", col = the.cols[2])
text(x = (z.mu-z.sigma)+1.1, y = z.dnorm(z.mu-z.sigma)/3, labels = "μ-σ",
     col = the.cols[1])
text(x = (z.mu+z.sigma)+1.3, y = z.dnorm(z.mu+z.sigma)/3, labels = "μ+σ",
     col = the.cols[1])
lines(x = seq(z.minmax[1], z.minmax[2], 0.1),
     y = z.dnorm(seq(z.minmax[1], z.minmax[2], 0.1)),
     lwd = the.lwd, col = the.darkgray
)

plot(x = z.sample, y = 1:length(z.sample),
     pch = 20,
     bty="n", yaxt="n", ylab = "", cex = 1.5,
     xlim = z.minmax, col = alpha("darkgray", 0.5))

par(mfrow = c(1,1))
@

To recapitulate, the PDF gives for each measurement (x-axis) the probability with which it occurs (y-axis).
Informally speaking, the curve shows that if we measure random values from this population, the probability of measuring a value close to $\mu$ is highest, and measurements deviate on average by the standard deviation $\sigma$ from $\mu$.
The blue line shows the mean, and the green lines show one standard deviation in each direction from the mean.
As a result, a very much expected sample of $n=\Sexpr{z.n}$ measurements is shown in the form of the point cloud below the curve (90° rotated compared to the raw data plots we've shown before).
It's a simulated sample, and the simulation was set up according to the known facts about the overall population ($\mu=\Sexpr{z.mu}$and $\sigma^2=\Sexpr{z.sigma^2}$).
The measurements are indeed centred around the mean, and they seem to follow the normal distribution.

If, however, we draw a sample from a different population where the true mean is slightly higher (for example because we're measuring reaction times under a condition that incurs a processing penalty) we expect samples to turn out differently on average and have a higher sample mean compared to the known population mean.
However, this on-average expectation can be treacherous because individual samples are not in any way \textit{guaranteed} to represent their population well, as we have shown in Chapter~\ref{sec:confidence}.
Very similar to Ronald A.~Fisher in his experiment with Muriel Bristow (see Chapter~\ref{sec:fisher}), we need to ask whether the actual sample warrants any inference regarding the underlying mechanism.
It does that if it's a sufficiently unexpected result under the assumption that the desired inference is not correct, \ie, under the Null.
It's sufficiently unexpected if it had a low pre-experiment probability of occurring.

We'll show how this works out for the given example.
In the case of the reaction times described in the Problem Statement, substantive hypothesis is that reaction times are higher with the emergent subtype of relative clauses because of assumed processing penalties.
However---especially if our sample is small---inferring anything from a specific result is tricky.
Figure~\ref{fig:z2} shows a possible outcome with $n=\Sexpr{z2.n}$ in red, and it should be obvious why it's tricky to infer anything from it.
The mean from the sample is higher than the theoretically known mean, but this might very well be just a random deviation.
In frequentist terms, such a sample is expected under the Null as well.
We need to know the probability of a sample that deviates from the known mean to such a degree (or a stronger degree) in order to proceed to an inference.

<<z2, fig.cap=paste0("Theoretical population distribution for a normal distribution with μ=", z.mu, " and σ=", z.sigma, " and a simulated random sample of n=", z2.n, " measurements from some population"), echo=F, fig.pos="t", cache=T>>=

par(mfrow = c(2,1), mar = c(2,1,1,1), family = "Plotfont")

plot(x = seq(z.minmax[1], z.minmax[2], 0.1),
     y = z.dnorm(seq(z.minmax[1], z.minmax[2], 0.1)),
     type = "l", lwd = the.lwd,
     bty="n", yaxt="n", ylab = "",
     xlim = z.minmax, col = "white"
)
lines(x = c(z.mu, z.mu), y = c(0, z.dnorm(z.mu)),
      lwd = the.lwd, col = the.cols[2])
lines(x = c(z.mu-z.sigma, z.mu-z.sigma), y = c(0, z.dnorm(z.mu-z.sigma)),
      lwd = the.lwd, col = the.cols[1])
lines(x = c(z.mu+z.sigma, z.mu+z.sigma), y = c(0, z.dnorm(z.mu+z.sigma)),
      lwd = the.lwd, col = the.cols[1])
text(x = z.mu+0.6, y = z.dnorm(z.mu)/2, labels = "μ", col = the.cols[2])
text(x = (z.mu-z.sigma)+1.1, y = z.dnorm(z.mu-z.sigma)/3, labels = "μ-σ",
     col = the.cols[1])
text(x = (z.mu+z.sigma)+1.3, y = z.dnorm(z.mu+z.sigma)/3, labels = "μ+σ",
     col = the.cols[1])
lines(x = seq(z.minmax[1], z.minmax[2], 0.1),
     y = z.dnorm(seq(z.minmax[1], z.minmax[2], 0.1)),
     lwd = the.lwd, col = the.darkgray
)

plot(x = z.sample, y = 1:length(z.sample),
     pch = 20, cex = 1.5,
     bty="n", yaxt="n", ylab = "",
     xlim = z.minmax, col = alpha("darkgray", 0.5))
points(x = z2.sample, y = sample(1:length(z.sample), length(z2.sample)),
     pch = 20, cex = 2,
     bty="n", yaxt="n", ylab = "",
     xlim = z.minmax, col = alpha(the.cols[3], 0.75))
abline(v = rep(mean(z2.sample)),
      lwd = the.lwd, col = the.cols[3])
text(x = mean(z2.sample)+2, y = 300,
     labels = bquote(bar("x")*"="*.(round(mean(z2.sample), 2))),
     col = the.cols[3])

par(mfrow = c(1,1))
@

Let's call the sample plotted in Figure~\ref{fig:z2} \xsample, a tuple of \Sexpr{z2.n} measurements $x_1$ through $x_{\Sexpr{z2.n}}$.
The mean of \xsample\ is $\xmean=\Sexpr{round(mean(z2.sample), 2)}$.
As we've shown, inferences in frequentist logic (see Chapter~\ref{sec:fisher}) are always made by taking into account what the outcome of an experiment could have been under one or several possibly correct hypotheses.
In the case at hand, we're interested in the hypothesis that the true mean under the experimental condition (call it $\mu_1$) is larger than the known mean $\mu$.
In other words, we would like to \alert{gather evidence in support of a substantive hypothesis: $\mu_1>\mu$}.
We use the symbol $\mu_1$ for the hypothesised larger mean as it is the mean of a slightly different theoretical population, and $\mu$ is the symbol reserved for population means.

For several reasons, we cannot gather evidence that supports this hypothesis directly.
First, as a population mean $\mu_1$ is obviously not directly observable.
We can virtually never observe whole populations, all we've got are sample.
Rather, $\mu_1$ is a hypothesised mean in a population that exists as separate from the known population if the substantive hypothesis is correct.
However, if that population is not substantially different from the known one, then we have $\mu_1=\mu$.
While we certainly hope that \xmean\ is a good indicator of the true value $\mu_1$, we have no guarantees whatsoever that it actually is.
Second, as Fisherians we have no formal method of gathering positive evidence.%
\footnote{Only Neyman-Pearson philosophy and the Severity approach will give us this power (pun intended) later in Chapter~\ref{sec:powerseverity}, but with some important caveats.}
Therefore, we can only check \alert{whether the data \xmean\ are in accord with a Null $H_0: \mu_1\not{>}\mu$}, which is equivalent to $H_0: \mu_1\leq\mu$.

Let's recapitulate the frequentist logic of inference and adapt it to the case at hand:
To test this hypothesis, frequentism assigns a certain well-defined kind of probability to (obtaining) the data \xsample\ given that the Null is correct.
This probability can be used to assess whether the data \xsample\ are in accord with the Null.
First, what do we infer from the data \xsample\ (in other words, from our experiment) if they are compatible with the Null?
You're right, absolutely nothing!
If the data are in accord with the Null, we haven't found any evidence that it is not the case that $\mu_1$ is not greater than $\mu$, and that's the end of it.
If that sounds uselessly messed up and disappointing, that's because it is.%
\footnote{Whether you're a linguist or not, please consider that \textit{finding no evidence that A is true} is not the same as \textit{finding evidence that A is false}.}
If you infer anything from such a result, you're not only wrong, but also Bayesians (many of them with Dutch names) will come to haunt you (or at least unfollow you on the messaging platform of your choice)---and they'd be somewhat right to do so.
Second, what do we infer from the data \xsample\ if they are not compatible with the Null?
This is the much more interesting case, but it's difficult to define the admissible inferences without creating false ideas.
Let's say rather informally that in such a case we've found some evidence in support of the substantive hypothesis because it and the Null partition the range of possible values of $\mu_1$:
either it's greater than $\mu$ or it is not greater than (\ie, smaller than or equal to) $\mu$ ($H_0$).
Finding no accordance with the Null despite serious attempts to do so (see Chapter~\ref{sec:powerseverity}) provides at least some indication that the substantive hypothesis might be correct.
However, if you're looking for a \alert{proof} of anything, we recommend that you stick to pure theory, logic, theoretical maths, or pseudoscience.
There is no proof to be found in (non-trivial) experiments, and statistical inferences are weak and fragile.

\subsection{Extreme Means Under the Null}

We have argued that in Fisherian inference, we have to assess whether \xsample\ and its mean \xmean\ are compatible with the Null.
But how do we do this?
Because we're comparing means (\ie, numeric values) and not merely counting occurrences of correct and incorrect tea-first detections, it seems difficult to compute the number of all possible outcomes and the number of outcomes as extreme as or more extreme than the one we actually observed.
After all, we're dealing with numeric measurements (real values), and there's always a result in between two results, \eg, between $\xmean=123.99999$ and $\xmean=124$, there are infinitely many other possible results such as $\xmean=123.999991$, $\xmean=123.9999911$, etc.
Well, it's not totally impossible to calculate exact probabilities, but there's a convenient shortcut.
We'll go through it step by step.

The most naive but not at all wrong thing to do is to calculate the difference between the known population mean $\mu$ and the mean of the obtained sample \xmean.
In our case, this is $\xmean-\mu=\Sexpr{round(mean(z2.sample)-z.mu,2)}$.
Clearly, a minimal requirement for any further calculations is that this difference is positive.
If it were negative, the sample could hardly be interpreted as evidence against N: $\mu_1\leq\mu$.%
\footnote{This way of putting it is slightly sloppy and informal.
We will return to this notion and make it more precise in Chapter~\ref{sec:powerseverity}.
However, in practice it is blatantly obvious that we would never take an experiment that showed lower reaction times as evidence for higher reaction times, etc.}
While this is something one should always do first, it's not suitable for serious inference due to one main reason:
It doesn't take into account how large the sample was.

We now follow a very similar logic as in our introduction to Fisher's Exact Test (Chapter~\ref{sec:fisher}).
The question is:
How often would we expect to see a sample of $\Sexpr{z2.n}$ measurements with a sample mean of $\Sexpr{round(mean(z2.sample),2)}$ or larger if the true mean is that specified by the Null, which is $\mu=\Sexpr{z.mu}$?
Luckily, we have already introduced the tool that we need: the \Key{standard error} of the mean $\ssigma_{\mu}$.
The standard error for $n=\Sexpr{z2.n}$ and the known variance $\sigma=\Sexpr{z.sigma}$ (see p.~\pageref{page:varianceisfour}) tells us how strongly samples of this size deviate from the true mean (on average) in each direction.
The standard error of the mean in this case is (revise Equation~\ref{eq:semean} if necessary):

\begin{center}\begin{math}
  \ssigma_{\mu}=\cfrac{\Sexpr{z.sigma}}{\sqrt{\Sexpr{z2.n}}}=\sqrt{\cfrac{\Sexpr{z.sigma}^2}{\Sexpr{z2.n}}}=\Sexpr{z2.se.rnd}
\end{math}\end{center}

<<z3, fig.cap=paste0("Theoretical population distribution for a normal distribution with μ=", z.mu, " and σ=", z.sigma, " (green) the distribution of sample means for samples from this distribution with n=", z2.n, " (black)"), echo=F, fig.pos="t", fig.height=4, cache=T>>=

par(mar=c(3,2,2,2), family = "Plotfont")

z.minmax <- c(110, 130)

plot(x = seq(z.minmax[1], z.minmax[2], 0.1),
     y = z2.dnorm.sample(seq(z.minmax[1], z.minmax[2], 0.1)),
     type = "l", lwd = the.lwd,
     bty="n", yaxt="n", ylab = "", xlab = "",
     xlim = z.minmax, col = "white"
)
lines(x = seq(z.minmax[1], z.minmax[2], 0.1),
      y = z.dnorm(seq(z.minmax[1], z.minmax[2], 0.1)),
      col = the.cols[1], lwd = the.lwd)

lines(x = c(z.mu, z.mu), y = c(0, z.dnorm(z.mu)),
      lwd = the.lwd, col = the.cols[2])
lines(x = c(z.mu, z.mu), y = c(z.dnorm(z.mu), z2.dnorm.sample(z.mu)),
      lwd = the.lwd, col = the.cols[2])

lines(x = seq(z.minmax[1], z.minmax[2], 0.1),
     y = z2.dnorm.sample(seq(z.minmax[1], z.minmax[2], 0.1)),
     lwd = the.lwd, col = the.darkgray
)

text(x = z.mu+0.6, y = z.dnorm(z.mu)/2, labels = "μ", col = the.cols[2])

par.defaults()
@

Remember what the standard error is all about (Chapter~\ref{sec:confidence}).
If the mean in a population is $\mu$ and the standard deviation is $\sigma$, then the sample means of samples of size $n$ are themselves normally distributed, and the standard error $\ssigma_{\mu}$ is the standard deviation of that normal distribution.
Furthermore, keep in mind that we're talking about the distribution of sample means drawn from a known population.
Under the Null, it is also the distribution from which our small sample was drawn.
Figure~\ref{fig:z3} contrasts the density of the distribution of individual data points (in our example: individual reaction times) with the much narrower distribution of sample means.
Mathematically, it is narrower because the standard error is always smaller or equal to the standard deviation.
Intuitively, it should be narrower because on average sample means from samples with $n>1$ approximate the true mean better than single measurements.%
\footnote{Understanding this argument is crucial. If you're not following it, you should go back to Chapter~\ref{sec:confidence} for an introduction to the distribution of sample means, especially the argument concerning samples of increasing sizes in Sections~\ref{sec:samplinberries} and~\ref{sec:milli}.}

Remember from Chapter~\ref{sec:confidence} that a Normal Distribution is exhaustively defined by the parameters $\mu$ and $\sigma$.
Since (i)~the distribution of sample means is Normal, (ii)~we know its mean $\mu$, (iii)~we know its variance $\sigma^2$ and standard deviation $\sigma$, we can calculate how many samples (in the long run) drawn from the known population would have a mean of $\Sexpr{round(mean(z2.sample),2)}$ or larger.
In other words, we can calculate how many sample means (from samples of size $n=\Sexpr{z2.n}$) would deviate by $\Sexpr{round(mean(z2.sample)-z.mu,2)}$ from the population mean anyway due to expected sampling error.
Instead of calculating the numbers and probabilities of individual events as in the Tea-Tasting Experiment, we use the known functions of the Normal Distribution to look up those values.
This gives us a very precise and well-defined measure of how unexpected the obtained result would be if the Null were true.

<<z4, fig.cap=paste0("Left: PDF of the distribution of sample means for mean µ and standard error SE with obtained sample mean $\\xmean$ and corresponding z-value; area under the curve for results equal to or greater than $\\xmean$ is shaded; Right: CDF for the same distribution with obtained sample mean $\\xmean$"), echo=F, fig.pos="t", cache=T, fig.height=4>>=

par(mfrow=c(1,2), mar=c(3,2,2,2), family = "Plotfont")

z.minmax <- c(114, 126)
xs <- seq(z.minmax[1], z.minmax[2], 0.1)
ys <- z2.dnorm.sample(seq(z.minmax[1], z.minmax[2], 0.1))

plot(x = xs,
     y = ys,
     type = "l", lwd = the.lwd,
     bty="n", yaxt="n", ylab = "", xlab = "",
     xlim = z.minmax, col = "white"
)

lines(x = c(z.mu, z.mu), y = c(0, z2.dnorm.sample(z.mu)),
      lwd = the.lwd, col = the.cols[2])

shade(xs, ys, z2.mu, z.minmax[2],
      col = the.lightcols[1], border=the.cols[1], lwd = the.lwd)

text(x = z.mu+3.25, y = z2.dnorm.sample(z.mu)/2.5, col = the.cols[2],
     labels = bquote(μ*"="*.(round(mean(z.mu), 2)))
     )
text(x = mean(z2.sample)+2.25, y = z2.dnorm.sample(mean(z2.sample)),
     labels = bquote(bar("x")*"="*.(round(mean(z2.sample), 2))),
     col = the.cols[1]
     )

lines(x = c(
  z.mu, mean(z2.sample)-0.25,
      y = 0.01+rep(z2.dnorm.sample(z2.mu), 2),
      lwd = the.lwd, col = the.cols[1]
      )
)

#text(x = mean(z2.sample)+1.5, y = z2.dnorm.sample(mean(z2.sample))+0.02,
#     labels=paste0("z=", z2.z.rnd),
#     col = the.cols[1]
#     )

lines(x = xs,
     y = ys,
     lwd = the.lwd, col = the.darkgray
)

# CDF

xs <- seq(z.minmax[1], z.minmax[2], 0.1)
ps <- pnorm(xs, mean = z.mu, sd = z2.se)

plot(xs,
     ps,
     type = "l", lwd = the.lwd,
     bty="n", ylab = "", xlab = "", xaxt = "n",
     xlim = c(114, 126), col = "white")

axis(1, seq(114, 126, 4), seq(114, 126, 4))

lines(x = c(z.mu, z.mu),
      y = c(0, 0.5),
      lwd = the.lwd, col = the.cols[2])
lines(x = c(z.mu, 110),
      y = rep(0.5, 2),
      lwd = the.lwd, col = the.lightcols[2])
text(114.5, 0.45, labels = 0.5, col = the.cols[2])
text(z.mu+0.5, 0.2, "μ", col = the.cols[2])

lines(x = c(z2.samplemean, z2.samplemean),
      y = c(0, ps[which.closest(xs, z2.samplemean)]),
      lwd = the.lwd, col = the.cols[1])
lines(x = c(z2.samplemean, 110),
      y = rep(ps[which.closest(xs, z2.samplemean)], 2),
      lwd = the.lwd, col = the.lightcols[1])
text(115, 0.93, labels = round(ps[which.closest(xs, z2.samplemean)], 2), col = the.cols[1])
text(z2.samplemean+0.5, 0.2, labels = bquote(bar(x)), col = the.cols[1])

lines(x = xs,
     ps,
     lwd = the.lwd, col = the.darkgray
)

par.defaults()

@

The left panel of Figure~\ref{fig:z4} show the PDF of the distribution of sample means, and the highlightes area under the curve corresponds to results as extreme or more extreme than \xmean.
The corresponding \Key{cumulative distribution function} (CDF) can be used to calculate the statistics of interest.
The right panel of Figure~\ref{fig:z4} shows the Normal CDF for $\mu=\Sexpr{z.mu}$ and $\sigma=\Sexpr{z2.se}$ (which happens to be $\ssigma_{\mu}$).
For the observed sample mean $\xmean=\Sexpr{round(z2.samplemean,1)}$, the CDF has the value $\Sexpr{round(pnorm(z2.samplemean, z.mu, z2.se), 2)}$, which means that the probability of obtaining this or a more extreme result is $1-\Sexpr{round(pnorm(z2.samplemean, z.mu, z2.se), 2)}=\Sexpr{1-round(pnorm(z2.samplemean, z.mu, z2.se), 2)}$.
Think of the value $\Sexpr{round(pnorm(z2.samplemean, z.mu, z2.se), 2)}$ as indicating that $\Sexpr{round(pnorm(z2.samplemean, z.mu, z2.se), 2)*100}$\% of the probability mass lie to the right of $\xmean=\Sexpr{round(z2.samplemean,1)}$.
If you have access to a software that has built-in functions for such CDFs, this is the most straightforward way to arrive at a p-value for a sample mean and a known Normal Distribution.
In this case, you don't even need to calculate $z$ to perform a z-test.

Otherwise, you can calculate the so-called z-score and look up the p-value in a table.
To to this, we simply count the distance between the distribution mean and the sample mean in multiples of the standard error, which gives us the \Key{z-score}.
It's the same $z$ we introduced in Chapter~\ref{sec:confidence}, except we're using it the other way round.
For estimation, we looked up z-scores corresponding to probabilities, now we're calculating z-scores in order to lookup probabilities.
In this case, it's particularly easy because $\ssigma_{\mu}=1$:

\begin{center}\begin{math}
z=\cfrac{\xmean-\mu}{\ssigma_{\mu}}=\frac{\Sexpr{round(mean(z2.sample), 2)}-\Sexpr{z.mu}}{\sqrt{\frac{4^2}{16}}}=\frac{\Sexpr{round(mean(z2.sample)-z.mu, 2)}}{\Sexpr{z2.se.rnd}}=\Sexpr{z2.z.rnd}
\end{math}\end{center}

By dividing the distance by the standard error and calculating the z-score, we normalise it and make its interpretation independent of the concrete slope ($\sigma$) of the distribution.
The z-score is the distance from the mean in a Standard Normal Distribution (with $\mu=0$ and $\sigma=1$) that corresponds to the measured distance from the known mean.
Hence, $z=\Sexpr{z2.z.rnd}$ can be interpreted independently of the concrete sample mean and population mean, and it establishes a direct link to the probability we're looking for: another \Key{p-value}.
We can use tables to look up p-values corresponding to z-scores, and for $z=2.1$, we get $Pr=0.02$, which in our application is the p-value $p=0.02$.
Table~\ref{tab:ptable} shows such a table, and it's the reverse of tables such as Table~\ref{tab:ztable}.%
By the way, the values are those for the Standard Normal Distribution with $\mu=0$ and $\sigma=1$.
Do you see why?
We give two columns, one for the single-sided test and one for the two-sided test (see p.~\ref{abs:sided}, immediately below, and Section~\ref{sec:diffciz}).

<<ptable, echo=F, cache=T, results="asis">>=
  zs <- seq(1.5, 2.5, 0.1)
  ps <- round(pnorm(zs, lower.tail = F), 2)
  pstt <- round(2*pnorm(zs, lower.tail = F), 2)
  ztable <- data.frame(Z=zs, "Pr1" = ps, "Pr2" = pstt)

  ztable %>%
    as.data.frame.matrix %>%
    kable(align = c("c", "c"), booktabs = T, linesep = "",
          caption = "Probabilities for Some z-Values",
          label = "ptable",
          col.names = c("Z", "Pr (one-sided)", "Pr (two-sided)")) %>%
    row_spec(0, bold=TRUE)
@

Clearly, either the Null is false or the Null is true and a relatively rare event has occurred.
Whether a chance of 1 in 50 (equivalent to $p=0.02$) is rare or unexpected enough to make an inference can only be decided by you based on your knowledge of the field you're working in.
How precise are your measurements?
What magnitude does your theory predict the difference should be?%
\footnote{It doesn't make numeric predictions?
Blimey! Then back to the drawing board.}
How does the measured difference in reading times compare to differences observed for similar processing penalties?
In modern corpus linguistics, we fail to see how $p=0.02$ would be surprising enough in any situation to warrant substantive inferences.

One final word on the \Key{one-sided test} and the \Key{two-sided test}, also called single-tailed and two-tailed.
In the example in this section, the substantive hypothesis was $\mu_1>\mu$, and the Null was consequently $H_0: \mu_1\leq\mu$.
We were interested in the probability of obtaining a mean as high as $\Sexpr{z2.z.rnd}$ or higher if $\mu_1\leq\mu$, and hence values that deviate negatively (to the left of) $\mu=\Sexpr{z.mu}$ wouldn't be unexpected at all and aren't counted.
There are two other possible scenarios we could have wished to test.
Our substantive hypothesis could have been $\mu_1<\mu$.
Alternatively, it could have been $\mu_1\not =\mu$.
The corresponding Nulls are $H_0: \mu_1\geq\mu$ and $H_0: \mu_1=\mu$.

<<tails, fig.cap=paste0("Single-sided and two-sided tests"), echo=F, fig.pos="t", fig.height=4, cache=T>>=

par(mfrow=c(1,3), mar=c(2,0.5,1.5,0.5), family = "Plotfont")

xs <- seq(-4, 4, 0.01)
ys <- dnorm(xs)
a.cex <- 1.4

plot(x = xs,
     y = ys,
     type = "l", lwd = the.lean.lwd,
     bty="n", yaxt="n", ylab = "", xlab = "",
     xlim = c(-4, 4), col = "white",
     main="Null: μ₁≤μ"
)

lines(c(0, 0), c(0, ys[round(length(xs)/2+0.1, 0)]),
      lwd = the.lean.lwd, col = the.cols[2])

shade(xs, ys, z2.z.rnd, 4,
      col = the.lightcols[1], border=the.cols[1], lwd = the.lean.lwd)

arrows(0.2, 0.025, z2.z.rnd-0.2, 0.025, length = 0.05,
       lwd = the.lean.lwd, col = the.cols[3])
text(z2.z.rnd/2, 0.05, paste0("z=", z2.z.rnd), col = the.cols[3], cex=a.cex)
text(z2.z.rnd+1.2, 0.05, paste0("p=0.02"), col = the.cols[1], cex=a.cex)

lines(x = xs,
     y = ys,
     lwd = the.lean.lwd, col = the.darkgray
)

plot(x = xs,
     y = ys,
     type = "l", lwd = the.lean.lwd,
     bty="n", yaxt="n", ylab = "", xlab = "",
     xlim = c(-4, 4), col = "white",
     main="Null: μ₁≥μ"
)

lines(c(0, 0), c(0, ys[round(length(xs)/2+0.1, 0)]),
      lwd = the.lean.lwd, col = the.cols[2])

shade(xs, ys, -4, -z2.z.rnd,
      col = the.lightcols[1], border=the.cols[1], lwd = the.lean.lwd)

arrows(-0.2, 0.025, -(z2.z.rnd-0.2), 0.025, length = 0.05,
       lwd = the.lean.lwd, col = the.cols[3])
text(-(z2.z.rnd/2), 0.05, paste0("z=-", z2.z.rnd), col = the.cols[3], cex=a.cex)
text(-(z2.z.rnd+1.2), 0.05, paste0("p=0.02"), col = the.cols[1], cex=a.cex)

lines(x = xs,
     y = ys,
     lwd = the.lean.lwd, col = the.darkgray
)


plot(x = xs,
     y = ys,
     type = "l", lwd = the.lean.lwd,
     bty="n", yaxt="n", ylab = "", xlab = "",
     xlim = c(-4, 4), col = "white",
     main="Null: μ₁=μ"
)
lines(c(0, 0), c(0, ys[round(length(xs)/2+0.1, 0)]),
      lwd = the.lean.lwd, col = the.cols[2])

shade(xs, ys, z2.z.rnd, 4,
      col = the.lightcols[1], border=the.cols[1], lwd = the.lean.lwd)
arrows(0.2, 0.025, z2.z.rnd-0.2, 0.025, length = 0.05,
       lwd = the.lean.lwd, col = the.cols[3])
text(z2.z.rnd/2, 0.05, paste0("z=", z2.z.rnd), col = the.cols[3], cex=a.cex)
text(z2.z.rnd+1.2, 0.05, bquote(frac("p", "2")*" =0.02"), col = the.cols[1], cex=a.cex)

shade(xs, ys, -4, -z2.z.rnd,
      col = the.lightcols[1], border=the.cols[1], lwd = the.lean.lwd)
arrows(-0.2, 0.025, -(z2.z.rnd-0.2), 0.025, length = 0.05,
       lwd = the.lean.lwd, col = the.cols[3])
text(-(z2.z.rnd/2), 0.05, paste0("z=-", z2.z.rnd), col = the.cols[3], cex=a.cex)
text(-(z2.z.rnd+1.2), 0.05, bquote(frac("p", "2")*" =0.02"), col = the.cols[1], cex=a.cex)

lines(x = xs,
     y = ys,
     lwd = the.lean.lwd, col = the.darkgray
)

par.defaults()
@

Each of these cases corresponds to a slightly different test, illustrated in Figure~\ref{fig:tails} for the Standard Normal Distribution as the distribution of sample means, but with the z-value from the example above, \ie, $z=\Sexpr{z2.z.rnd}$.
On the left, the test as conducted above is illustrated once again.
We deviate by $z=\Sexpr{z2.z.rnd}$ from the known mean $\mu$, and that defines 2\% of the area under the curve, hence $p=0.02$.
In the middle, the opposite is shown, which is applicable if the Null is: \textit{the unknown mean $\mu_1$ is equal to or larger than the known mean $\mu$}.
In this case, we're only interested in negative deviations from the known mean $\mu$.
The probability corresponding to a negative z-statistic in the one-sided case is the same as the one corresponding to a positive z-statistic, hence $p=0.02$ according to Table~\ref{tab:ptable}.
Finally, if your hypothesis is an \Key{undirected hypothesis} and we only suspect the sample mean \xmean\ represents a mean $\mu_1$ that is different from $\mu$, we have to take into account both tails of the distribution, which simply doubles the p-value to $p=0.04$.
The right panel in Figure~\ref{fig:tails} clearly shows that values as extreme of more extreme than $z=\Sexpr{z2.z.rnd}$ in either direction of $\mu$ correspond to the double area under the curve.

If you think this is trivial, you're lucky.
However, don't be too hard on yourself if you find this difficult to fiddle apart and to memorise.
This is completely normal, and it has nothing to do with you or frequentist statistics.
Fort some reason, many human brains have difficulties of keeping track of positive and negative signs of z-statistics, greater-than and smaller-than relation, while having to flip them around in formulating and evaluating hypotheses.
Just give it time.

\Bigpoint{The Logic of Testing With Normal Means}{%
For understanding the logic of the z-Test, it's important to keep in mind the difference between the underlying distribution of the \textit{data} (for example, reaction times in milliseconds under an experimental condition) and \textit{sample means} (for example, mean reaction times in milliseconds) of samples taken from these data.
Furthermore, since we're arguing in a frequentist context, we ask how the sample means will be distributed if we take a lot or even infinitely many samples.
The sample means from any underlying distribution of data points are normally distributed around the true mean of the data.
The larger the variance in the data, the larger the variance in the sample means, although the sample means vary less and less with increasing sample size.
If we know the true mean and the true variance in the data, we can perform a frequentist test by asking:
\textit{What's the frequentist pre-experiment probability of (i)~obtaining a sample of the given size (ii)~with a mean that diverges from the true mean as extremely as or more extremely than the one we actually obtained (iii)~if the sample comes from the population with the known mean.}
That probability is the p-value of the z-Test.
If the p-value is low, we've either drawn a rare sample (which is an unexpected event), or the sample does \textit{not}, in fact, come from the known population, but from a population with a lower or higher mean.
If the probability is low enough, you might make a careful inference and say that the sample provides some evidence that the mean under the experimental condition differs from the known mean.
Only you and the colleagues in your field can decide what counts as {low enough}.
Merely reaching a generic sig-level of 0.05 is most likely \textit{not} enough.
}

\subsection{The Difference Between Error Intervals and the z-Test}\label{sec:diffciz}

The z-test allows us to check whether a mean measured in an experiment was an unexpected outcome under the Null.
We attach a specific statistic---the p-value---to the outcome.
It's a matter of debate whether the statistic itself should be interpreted directly numerically.
However, you've probably heard of people using the p-value to make decisions when it's smaller than a certain threshold called $\alpha$-level or---as we prefer---$sig$-level
They call results \textit{significant} when they reach a certain level such as $sig=0.05$.
If $p<sig$, \textit{we've reached significance}.
However, when $p=0.009$ in the next experiment, suddenly that result might be called \textit{even more significant}.
While we consider it vital to report the actual p-value and not just $p<sig$, we advise against cascaded conceptions of significance.
Whether an outcome would be unexpected under the Null depends on the phenomenon at hand, the precision of your theory and of your measurement, whether you've ensured that the conditions for the test were met properly, and probably many other factors.
Nobody can tell you or your research community what your threshold needs to be.
For example, you might have seen reports from nuclear physics (such as results from experiments with the Large Hadron Collider of CERN) where it is claimed that $5\sigma$ was reached.
That's just 5 standard deviations from the expected value under the Null ($z=5$), which corresponds to approximately $p=0.00000029$.
In physics, this is not interpreted as a test but rather as a requirement for measurements to be taken seriously at all, to be accepted as measurements of something.
That's because there is large uncertainty in the measurements, and the phenomena of interest are tiny beyond microscopic and can only observed very indirectly.
Do you need that level of unexpectedness?
Think about it.
If you're uncertain and your field has just gotten started (such as linguistics, cognitive science, or social psychology), we recommend you don't dare to be any more lenient than $sig=0.001$.
Honestly, $sig=0.05$ is borderline ridiculous!

In any case, if you reach some sig-level under a testing approach, you might proceed to an inference, a decision of some sort.
Usually, it should be something like this:

\begin{quote} \itshape
  We're not going to discard our substantive hypothesis just yet, but we'll submit it to further error probing.
\end{quote}

\noindent Do you see how this is different from the following statement?

\begin{quote} \itshape
  \Argh We've statistically proven that our substantive hypothesis is correct with a probability of 95\%.
\end{quote}

\noindent If you don't see this, go back to Chapter~\ref{sec:fisher} and start all over again.

While tests and error intervals are related mathematically (see below), they're entirely different in their interpretation.
You can estimate a value without having any hypothesis at all.
For example, if you just want to find out the average rate at which a manufacturing machine produces defective parts, you might take a sample of 1000 parts, count the proportion of defective parts, calculate a safe error interval (at, say, $a=0.99$), and see whether the resulting interval looks acceptable from an economic point of view.
If you do this with all your machines over many years and decades, your estimate will included the true value in close to 99\% of all times.
Depending on the value of the individual parts that you're working with in your business, this might be acceptable.
In Chapter~\ref{sec:powerseverity}, we'll return to this example from toy manufacturing and move it closer to a testing framework.

<<convzci, fig.cap=paste0("Convergence of z-test and error interval: two-sided test"), echo=F, fig.pos="t", cache=T, fig.height=5>>=

par.defaults()
par(family = "Plotfont")

xs <- seq(-4, 4, 0.01)
ys <- dnorm(xs)

plot(x = xs,
     y = ys,
     type = "l", lwd = the.lwd,
     bty="n", yaxt="n", ylab = "", xlab = "",
     xlim = c(-4, 4), col = "white"
)

shade(xs, ys, -14 , -1.96,
      col = the.lightcols[1], border=the.cols[1], lwd = the.lwd)
shade(xs, ys, 1.96 , 4,
      col = the.lightcols[1], border=the.cols[1], lwd = the.lwd)

points(1.96, 0.1,
      col = the.cols[2], pch = 20, cex = 2)
lines(c(0, 2*1.96), rep(0.1, 2),
      col = the.cols[2], lwd = the.lwd)
lines(rep(0, 2),  c(0.11, 0.09),
      col = the.cols[2], lwd = the.lwd)
lines(rep(2*1.96, 2),  c(0.11, 0.09),
      col = the.cols[2], lwd = the.lwd)

lines(x = xs,
     y = ys,
     lwd = the.lwd, col = the.darkgray
)
@

Their interpretations differ, but frequentist error intervals and frequentist tests are related mathematically.
To illustrate, let's use an even simpler example where the sampling distribution is the Standard Normal Distribution.
Hence, $\mu=0$, $\sigma=\ssigma_{\mu}=1$, and we assume that the measured mean was $\xmean=1.96$.
Since the standard error is 1, all calculations are quite simple.
First, for the test, $z=1.96\div 1= 1.96$.
This gives almost exactly $p=0.05$ for the two-sided test, \ie, the test which merely checks whether the measured mean lies in the outermost 5\% of the probability mass.
In Figure~\ref{fig:convzci}, the shaded areas correspond to those five percent, and they begin at a distance of $1.96$ to both sides of the mean.
If we calculate an error interval with $a=0.95$ around the measurement of $\xmean=1.96$, it is exactly $1.96$ wide to both sides of the measured mean.
Hence, it includes $0$, as you can see in Figure~\ref{fig:convzci}, where the error interval is plotted in blue.

<<convzciss, fig.cap=paste0("Convergence of z-test and error interval: single-sided test"), echo=F, fig.pos="t", cache=T, fig.height=5>>=

par.defaults()
par(family = "Plotfont")

xs <- seq(-4, 4, 0.01)
ys <- dnorm(xs)

plot(x = xs,
     y = ys,
     type = "l", lwd = the.lwd,
     bty="n", yaxt="n", ylab = "", xlab = "",
     xlim = c(-4, 4), col = "white"
)

shade(xs, ys, 1.645, 4,
      col = the.lightcols[1], border=the.cols[1], lwd = the.lwd)

points(1.645, 0.15,
      col = the.cols[2], pch = 20, cex = 2)
lines(c(0, 1.645), rep(0.15, 2),
      col = the.cols[2], lwd = the.lwd)
lines(rep(0, 2),  c(0.14, 0.16),
      col = the.cols[2], lwd = the.lwd)

lines(x = xs,
     y = ys,
     lwd = the.lwd, col = the.darkgray
)
@

The two-sided z-test at some $sig$-level (in this case $sig=0.05$) is equivalent to checking whether the error interval for $a=1-sig$ contains the population mean $\mu$.
This extends to a single-sided test and a single-sided error interval, which only extends to one side from the measured mean.
See Figure~\ref{fig:convzciss}, where we assume $\mu=0$, $\sigma=\ssigma_{\mu}=1$, and $\xmean=1.65$.
The error interval is left-sided, and it corresponds to the right-sided test.
As $\xmean=1.65$ was chosen as the z-value that reaches $sig=0.05$ for the single-sided z-test, the interval again includes $0$.

\Bigpoint{Tests and Error Intervals}{%
Tests like the z-Test and Error Intervals are based on the same maths, but they're interpretations differ.
Two-sided Error Intervals are probably more appropriate than tests when there is no substantive difference and hence no Null.
Single-sided Error Intervals are rarely calculated, most likely they only make sense if researchers have at least a directional hypothesis, which is compatible with a very informative single-sided test.
All in all, we've shown that Error Intervals contain only very little extra information compared to a test if the standard error, the z-value, and the p-value are reported.
It's never a bad thing to report extra information or report the same information in different ways to make it easier for readers to get the idea.}

\subsection{The Distribution of p-Values}\label{sec:distpz}

<<setupzsim, cache=T, echo=FALSE, include=FALSE, results='asis'>>=
  set.seed(848)
  z.sim.rep <- 1000
  z.sim.n <- 100
  z.sim.mu <- 120
  z.sim.sigma <- 16
  z.sim.se <- z.sim.sigma/sqrt(z.sim.n)

  z.sim <- function(m) {
      .m <- mean(rnorm(n = z.sim.n, mean = m, sd = z.sim.sigma))
      .z <- (.m-z.sim.mu)/z.sim.se
      pnorm(abs(.z), lower.tail = F)*2
  }
@

<<simznull, fig.cap=paste0("Histograms of p-values from z-tests when the Null is true; it's irrelevant what the parameters were as any p-values have a uniformly random distribution under any true Null"), echo=F, fig.pos="t", cache=T>>=
  par.defaults()
  z.sims.null <- unlist(unname(lapply(1:z.sim.rep,
                                      function(x) {z.sim(z.sim.mu)} )))

  h <- hist(z.sims.null, breaks = 20, xaxt = "n", xlab = "p-Values", main ="",
            border=F, col = the.midgray, ylim = c(0,70))
  text(h[["mids"]], h[["counts"]]-5, labels = h[["counts"]])
  axis(1, seq(0, 2, 0.2), seq(0, 1, 0.1))
@

In Section~\ref{sec:pdist} we showed how simulated p-values of Fisher's Exact Test are distributed when the Null is true and when it it's false.
In this section, we'll do the same for the z-test.
First, let's assume the Null is true.
We simulate a two-sided test with $H_0:\mu_1=\mu$.
The measurements are normally distributed with $\mu=\Sexpr{z.sim.mu}$ and $\sigma=\Sexpr{z.sim.sigma}$.
We simulate $n=\Sexpr{z.sim.n}$ data points per sample in $\Sexpr{z.sim.rep}$ replications.
The standard error of the mean is $\ssigma_{\mu}=\Sexpr{z.sim.sigma}\div\sqrt{\Sexpr{z.sim.n}}=\Sexpr{z.sim.se}$.
Figure~\ref{fig:simznull} shows that each p-value has (minor fluctuations aside) the same probability if the Null is true.
If the mean within the population from which our samples are drawn is $\mu_1=\mu=120$, any p-value between 0 and 1 has an equal chance of resulting from the experiment.

<<simzh1, fig.cap=paste0("Histograms of p-values from z-tests when the Null is false with different effect strengths"), echo=F, fig.pos="t", cache=T>>=
  par.defaults()
  par(mfrow=c(3,2))

  z.sims.plotone <- function(m) {
    .z.simses <- unlist(unname(lapply(1:z.sim.rep, function(x) {z.sim(m)} )))
    h <- hist(.z.simses, breaks = 20, xaxt = "n", xlab = "p-Values",
              border=F, col = the.midgray,
              ylim=c(0, ifelse(mean(.z.simses)<0.1, 1000, 110)),
              main = paste0("μ₁=", m))
    #text(h[["mids"]], h[["counts"]]-5, labels = h[["counts"]])
    axis(1, seq(0, 1, 0.1), seq(0, 1, 0.1))
  }

  z.sim.truemeans <- 120+c(-1, 1, -5, 5, -10, 10)
  for (m in z.sim.truemeans) z.sims.plotone(m)
@

We then simulated the same for $\mu_1\not=\mu$.
Clearly, there are many ways to satisfy $\mu_1\not=\mu$.
The difference between the two means could be $-0.1$, $+29$, or what have you.
All these different possible concrete numerical inequalities between $\mu_1$ and $\mu$ make the Null false.
The larger the difference, the larger the effect strength.
For example, if reading times under a marked condition differ from those in the unmarked condition by 200~ms, the effect is much larger than if they differ by only 2~ms.
We simulated \Sexpr{z.sim.rep} samples each for the true means $\mu_1\in\{\Sexpr{paste0(z.sim.truemeans, collapse = ", ")}\}$.
Figure~\ref{fig:simzh1} shows histograms of the results.
Even if the difference between the means is only 5 ($\mu_1=115$ or $\mu=125$), the p-values already collapse close to 0 (middle left and middle right panel).
Once again (remember Section~\ref{sec:pdist}), larger effect strengths make it easier to detect the effect by rejecting the Null.
As in the case of Fisher's Exact Test, a large sample alone does not increase the chance of obtaining a low p-value.
However, as soon as there is even a small true effect, and we draw a relatively large sample, p-values go down rapidly.
If the effect is tiny and irrelevant from a theoretical point of view, we can detect it easily with a large sample, it's our duty to have a good enough understanding of the statistical methods we use to be able to interpret that.
This behaviour of the test itself is, of course, by design.
Frequentist statistics per se is not responsible for misinterpretations and misuse by lazy or corrupt practitioners.

\section{The Undiscovered Population}

\subsection{Accounting for Unknown Variance}\label{sec:tvariance}

The z-test allowed us to test whether some mean $\mu_1$ is different (smaller, larger, or any of the two) from a known population mean if the population variance is unknown.
Some readers might have wondered when on Earth that is actually the case.
Since the t-test doesn't account for any uncertainties in the knwon mean and variance, they must be known beyond any need for further testing.
This is where the \Key{t-test} comes into play.
It allows us to test for mean differences if the variance is unknown (this section) or both means and the variance are unknown (Section~\ref{sec:ttwo}).

The logic of the test is perfectly identical to the z-test.
We simply substitute the sample variance $s^2$ for the population variance $\sigma^2$ and go on with the calculations but call the test statistic $t$ instead of $z$ (hence \textit{t-test}).
To remind us all of Equation~\ref{eq:variance}:

\begin{equation}
  s^2(\xsample)=\cfrac{\sum\limits_{i=1}^{n}(x_i-\bar{\xsample})^2}{n-1}=\cfrac{SQ(\xsample)}{n-1}
\end{equation}

Do you see why this can't be all?
While the sample variance might be the best approximation of the population variance we have, it's not going to be a perfect approximation in most (virtually all) cases.
There might be cases where $s^2$ \textit{overestimates} $\sigma$.
Imagine what this would do to our statistic $t$, which is calculated exactly the same way as $z$:

\begin{equation}
  t=\cfrac{\xmean-\mu}{\ssigma_{\mu}}
  \label{eq:tone}
\end{equation}

\noindent Except $\ssigma_{\mu}$ is calculated from the sample variance:

\begin{center}
  \begin{math}
    \ssigma_{\mu}=\sqrt{\cfrac{s^2}{n}}
  \end{math}
\end{center}

With increasing $s^2$, \ssigma\ increases, and $t$ decreases as the denominator of its formula is $\ssigma_{\mu}$.
However, smaller values of the $z$ or $t$ statistics leads to larger p-values.
Look at Figure~\ref{fig:z4} to convince yourself that this is true.
Whichever criterion we adopt to reject the Null based on an obtained p-value, we'd reject less Nulls.%
\footnote{Once again, we encourage you to go through this argument step by step multiple times until you're satisfied that you've understood and memorised the argument.}
Since we attach no inferential interpretation to a rejected Null, rejecting more Nulls than we should because the sample variance overestimates the population variance is not a big deal.%
\footnote{If you find this claim deeply unsatisfying, we hear you.
Just read on until Chapter~\ref{sec:powerseverity}.}

If, on the other hand, the sample variance $s^2$ underestimates the population variance $\sigma^2$, we might end up rejecting less Nulls than we should.
Convince yourself that this is true by going through the formulas above.
This is potentially harmful as rejections of a Null have a limited inferential interpretation:
\textit{Either the Null is false or a rare event has occurred.}
To compensate for this, a statistician by the name of William Sealy Gosset, who called himself \textit{Student}, came up with a distribution similar to the Standard Normal Distribution: \Key{Student's t-Distribution}.
It accounts for the fact that in the long run a larger sample provides a better estimate of the population variance than a smaller sample.
The distribution has only one parameter: the sample size $n$, or rather the \Key{degrees of freedom} $\nu$ with $\nu=n-1$.
We'll return to the idea of degrees of freedom repeatedly.
For now, we just contemplate why the t-Distribution has this one parameter and don't worry too much about its name.

<<t1, fig.cap=paste0("Densities of the Standard Normal Distribution and a selection of t-Distributions for increasing degrees of freedom ν"), fig.height=5, echo=F, fig.pos="t", cache=T>>=

par.defaults()

t.minmax <- c(-4, 4)

t.dfs <- c(1, 2, 5, 10, 20)
xs <- seq(t.minmax[1], t.minmax[2], 0.01)

plot(x = xs,
     y = dnorm(xs),
     type = "l", lwd = the.lean.lwd,
     bty="n", yaxt="n", ylab = "", xlab = "z or t",
     xlim = t.minmax, col = the.darkgray
)

for (i in 1:length(t.dfs)) {
  .df <- t.dfs[i]
  lines(x = xs,
       y = dt(xs, df = .df),
       lwd = the.lean.lwd, col = the.cols[i]
  )
}

legend("topleft", legend = c("Normal", rev(paste0("t with ν=", t.dfs))),
       col = c(the.darkgray, rev(the.cols[1:length(t.dfs)])), lwd = the.lwd, bty = "n")
@

First, the t-Distribution corresponds to the \textit{Standard} Normal Distribution, not the more general Normal Distribution.
Hence, its variance is fixed at $1$ and its mean is fixed at $0$.
At the same time, the effect of the expected mismatch described above between the sample variance and the true population variance is small when the sample is large and vice versa.
A larger sample approximates any population parameter more accurately than a smaller sample.
Now, see Figure~\ref{fig:t1} to grasp the effect of the $\nu$ parameter.
A lower $\nu$, which corresponds to a lower sample size $n$, flattens the peak of the curve around the mean while lifting both tails up.
However, even at $\nu=20$, the density of the t-Distribution is already almost identical to that of the Standard Normal Distribution.

<<t2, fig.cap=paste0("How the p-value from the t-Distribution at t=1.96 (two-sided) approaches the p-value from the Standard Normal Distribution at z=1.96 (two-sided) with increasing degrees of freedom ν"), fig.height=5, echo=F, fig.pos="t", cache=T>>=

par.defaults()

ts <- 2*pt(1.96, df = 1:100, lower.tail = F)

plot(ts,
     type = "l", lwd = the.lwd,
     bty="n", ylab = "p-value", xlab = "Degrees of freedom ν",
     col = "white"
)
lines(c(0, 100), rep(0.05, 2), lwd = the.lwd, col = the.cols[1])
lines(ts, lwd = the.lwd, col = the.cols[2])

legend("topright", legend = c("p-value from Standard Normal", "p-value from t (with increasing ν)"),
       title = "For z=1.96 and t=1.96, respectively:",
       col = the.cols[1:2], lwd = the.lwd, bty = "n")
@

Figure~\ref{fig:t2} shows how two-sided p-values from the t-Distribution for a fixed t-value of 1.96 but with increasing sample size and thus increasing degrees of freedom.
The t-value of 1.96 was chosen because we know that it corresponds to $p=0.05$ in a z-Test with a Standard Normal Distribution.
The Normal Distribution has no parameter for degrees of freedom, and the p-value is constant at $p=0.05$ for $z=1.96$.
With ever larger samples, the p-value from a t-test approximates that from a z-test very quickly.
With a slight oversimplification, we can say that the t-test makes it harder to reach any standard for an unexpected result compared to the z-test, thus accounting for the added uncertainty from the variance estimated from the sample.
With a larger sample (higher degrees of freedom), that added uncertainty disappears more and more as the estimate becomes more and more accurate.

\Bigpoint{What's Normal?}{%
Sample means are normally distributed.
For normally distributed data, this is always true, and for non-normally distributed data, it's true at large enough sample sizes (see Section~\ref{sec:conditionmeantests}).
The t-Test does not account for non-normality in the distribution of the means.
Rather, it accounts for the fact that our knowledge of the variability in the data and consequently of the variability in the means is limited by being estimated from the sample.
}

<<tsim, fig.cap="Distributions (histogram densities) of p-values from simulations of z-Tests and t-Tests with variances estimated from samples; the z-Tests are inappropriate in this case", fig.height=5, echo=F, fig.pos="t", cache=T>>=

par.defaults()
par(mfrow=c(2, 2), mar=c(4.5,4,2,0.5))

tsim.reps <- 10000
tsim.t.ps <- rep(0, tsim.reps)
tsim.z.ps <- rep(0, tsim.reps)

tsim.1 <- 0.5
tsim.sig <- 0.01
tsim.ns <- c(5, 10)
tsim.props <- rep(0, 8)

for (n in 1:2) {
  tsim.n <- tsim.ns[n]
  for (u in 0:1) {
    for (i in 1:tsim.reps) {
      if (u == 0) {
        .xs <- rnorm(n = tsim.n)
      } else {
        .xs <- rnorm(n = tsim.n, mean = tsim.1)
      }

      .mean <- mean(.xs)
      .var <- var(.xs)
      .sd <- sqrt(.var)
      .se.hat <- sqrt(.var/tsim.n)

      # z-test and t.test
      .t <- .mean/.se.hat

      .z.p <- 2*pnorm(abs(.t), lower.tail = F)
      .t.p <- 2*pt(abs(.t), tsim.n-1, lower.tail = F)

      tsim.z.ps[i] <- .z.p
      tsim.t.ps[i] <- .t.p
    }

    .z.prop <- round(length(which(tsim.z.ps < tsim.sig))/tsim.reps*100, 1)
    .t.prop <- round(length(which(tsim.t.ps < tsim.sig))/tsim.reps*100, 1)

    # For later in the text.
    tsim.props[ifelse(n==1, 1, 5)+u*2] <- .z.prop
    tsim.props[ifelse(n==1, 1, 5)+u*2+1] <- .t.prop

    .main <- paste0(
      ifelse(u==0, "H₀ is true", paste0("H₀ is false (μ₁=", tsim.1, ")")),
      ", n=", tsim.n
      )

    h <- hist(tsim.z.ps, breaks = 40, plot = F)
    z.smoo <- smooth.spline(h$density~h$mids, spar = 0.6)
    h <- hist(tsim.t.ps, breaks = 40, plot = F)
    t.smoo <- smooth.spline(h$density~h$mids, spar = 0.6)

    plot(z.smoo,
         main = .main,
         type = "l", lwd = the.lwd,
         bty="n",
         xlab = "p-value",
         ylab = ifelse(u==0, "", ""), yaxt = "n",
         col = "white"
    )
    lines(z.smoo,
          col = the.cols[3], lwd = the.lwd)
    lines(t.smoo,
          col = the.cols[1], lwd = the.lwd)
    legend("topright",
           legend = c(paste0("z-Test with p<", tsim.sig, ": ", .z.prop, "%"),
                      paste0("t-Test with p<", tsim.sig, ": ", .t.prop, "%")),
           col = the.cols[c(3,1)], lwd = the.lwd, bty = "n"
    )
  }
}
@

Finally, we simulate t-tests and compare them to incorrectly conducted z-tests.
The simulation answers the question of what happens if we disregard the fact that the variance was merely estimated from a sample and was not known, and we decide to perform a z-Test.
For simplicity's sake, let's say the known mean is $\mu=0$, and the true but unknown standard deviation is $\sigma=1$.
Figure~\ref{fig:tsim} shows the distributions of p-values for inappropriate z-Tests based on the sample variance and the corresponding appropriate t-Tests.%
\footnote{These plots show mildly smoothed densities from a histogram, made using \texttt{hist(breaks = 40)}, the resulting densities, and \texttt{spline.smooth(spar = 0.6)} from R.
They're not traditional kernel-smoothed density estimates as these would give a false impression by showing the density going down towards 0 and 1.
Please ignore this information if you don't know what it means.}
The only difference is that for the z-Test, the cumulative distribution function of the Standard Normal Distribution was used, and for the t-Test, the cumulative distribution function of the t-Distribution (with the respective degrees of freedom) was used.
The z-value and the t-value are numerically identical in this case, and they both use the standard error calculated from the sample variance, which is, again, only correct for the t-Test:

\begin{center}
  \begin{math}
    \ssigma_{\xsample}=\sqrt{\cfrac{s^2}{n}}
  \end{math}
\end{center}

Each curve in Figure~\ref{fig:tsim} plots the estimated density of p-values from \Sexpr{format(tsim.reps, scientific=F)} random experiments with the accompanying two-sided tests.
In the two left panels, the Null is true, and the data are generated from a Standard Normal Distribution.
Hence, the the z-values and the t-values should be centred around 0, and all p-values should have the same long-run frequency, which is always assumed under the Null.
We see that for $n=\Sexpr{tsim.ns[1]}$, the z-Test produces incorrect results if the variance is not known but merely estimated.
Under the Null (upper left panel) and with $n=\Sexpr{tsim.ns[1]}$ ($\nu=\Sexpr{tsim.ns[1]-1}$), the t-Tests correctly land at $p<\Sexpr{tsim.sig}$ in \Sexpr{tsim.props[2]}\% of all simulations, but the z-Tests produce many false significant results.
Instead of the nominal 1\% at $sig=0.01$, the Null is rejected in \Sexpr{tsim.props[1]}\% of the simulations.%
\footnote{We arbitrarily chose $sig=\Sexpr{tsim.sig}$ for convenience.
Results would be similar for other sig-levels.}

With more degrees of freedom, this problem is alleviated as the variance estimate gets more precise.
In the lower-left panel, the simulations for $n=\Sexpr{tsim.ns[2]}$ ($\nu=\Sexpr{tsim.ns[2]-1}$) are shown.
The t-Test is still almost spot-on and reaches $p<\Sexpr{tsim.sig}$ in \Sexpr{tsim.props[6]}\% of all simulations.
The z-Tests now produce \Sexpr{tsim.props[5]}\% reaching the threshold, which means at least many fewer false significant results.
Still, this would lead to false rejections of the Null and false positive inferences regarding the substantive hypothesis much more often than it should.
The error rate increases sharply, as it should be 1\% at $sig=\Sexpr{tsim.sig}$, 0.1\% at $sig=0.001$, etc.

In the panels on the right-hand side, p-values from simulations are shown where the Null is false.
The samples now come from a DGP with $\mu_1=\Sexpr{tsim.1}$.
We haven't discussed yet how this effect strength affects the tilt of the p-values numerically.
Therefore, we have no precise expectation of how many simulations we expect to achieve $p<\Sexpr{tsim.sig}$.
However, we clearly see that the inappropriate z-Tests are shifted much more heavily towards 0.
Under the assumption (corroborated in the simulations with a true Null) that the t-Test is appropriate and the z-Test isn't under the given circumstances, the z-Test will lead to many more wrong inferences than we expect from correct z-Tests.
In sum, using the z-Test where a t-Test would be appropriate leads to a considerable and incorrect increase in rejections of the Null.
As this would mar our inferences, the t-Test is a highly useful tool when the true variance is unknown.
But, we hear you say, population means are also largely unknown, which makes the t-Test much less useful than we claim.
Hold that thought!
The solution is presented in the next section.

\Bigpoint{Rogue Estimates}{%
If a test relies on certain parameters being known, you have to know them, and you can't under any circumstances estimate them!
The z-Test requires that you know the true population mean and the true population variance.
If you estimate those parameters from a sample and pretend you knew them, your error rates are off.
We've shown that you'd reject the Null much more often than you should when it's true if you perform a z-Test based on variances estimated from samples.
for example, you'd think you're wrong in 1\% of all cases, but you'd actually be wrong (much) more often.
While this effect gets weaker with larger samples, you never know how large your sample needs to be to make the effect vanish sufficiently.
Hence, whenever we say that \textit{parameter P needs to be known for test T to be applicable}, take that very seriously!
If it's not known, look for alternative tests.
}

\subsection{Accounting for Two Unknown Means}\label{sec:ttwo}

\subsubsection{Extending the Logic of the t-Test to Two Samples}\label{sec:logicttwo}

We now turn to a much more realistic scenario:
You haven't got any precise idea about the population mean to which you could compare a sample mean.
Also, the population variance isn't known with sufficient precision.
In such situations, researchers usually compare two samples \xsample, \ysample\ taken under two conditions, asking whether the sample means differ strongly enough to warrant the inference that they come from two populations where the population means differ.
The logic of the inference is, as usual, frequentist:
The Null states that the population means are identical $H_0: \mu=\mu_1$.
The measured sample means differ by a certain amount, a distance $d=\xmean-\ymean$.
The distance is converted to a test statistic (in this case $t$ by dividing it by the appropriate standard error).
We then ask how often (given the sample sizes and the variance estimated from the samples) a statistic as extreme as (or more extreme than) $t$ would be expected if the Null were true.
If such an event is sufficiently rare, we conclude that the Null is false or a rare event has occurred.

For example, as a socio-phonetician you might be interested in comparing the mean frequency of the F1 formant in articulations of self-described members of US coastal elites to the mean frequency of the same formant in articulations of self-described rural folk from the US Midwest in the long [iː] in \textit{weakling}.%
\footnote{As usual in this book, the examples are completely made up.
To experts in the respective sub-fields they might seem nonsensical.
This approach stems from our conviction that the examples shouldn't matter much for a sound understanding of statistics.
Whether it's mean formant frequencies measured in hertz or mean daily milk yields of cows measured in pints doesn't really matter.}
Some deep sociolinguistic theory predicts that the formant frequency should be lower for rural folk.

<<setupformants, echo=FALSE>>=

set.seed(71)

f.n <- 10

f.elite.m <- 240
f.elite.sd <- 9
f.elite.var <- f.elite.sd^2
f.elite <- round(rnorm(n = f.n, mean = f.elite.m, sd = f.elite.sd), 0)

f.rural.m <- 235
f.rural.sd <- 9
f.rural.var <- f.rural.sd^2
f.rural <- round(rnorm(n = f.n, mean = f.rural.m, sd = f.rural.sd), 0)

f.se <- round(sqrt(var(f.elite)/f.n+var(f.rural)/f.n), 2)
f.t <- round((f.elite.m-f.rural.m)/f.se, 2)
f.p <- round(pt(f.t, df = 2*(f.n-1), lower.tail = F), 2)
@

Per group, $n=\Sexpr{f.n}$ participants read a short text where the word \textit{weakling} occurs once.
Hence, we have 20 data points in total.
The results are (where \xsample\ is from the elites and \ysample\ from the rural folk):

\begin{center}
  \begin{math}
    \xsample=\langle\Sexpr{paste0(f.elite, collapse=", ")}\rangle
  \end{math}
  \begin{math}
    \ysample=\langle\Sexpr{paste0(f.rural, collapse=", ")}\rangle
  \end{math}
\end{center}

\noindent The means are $\xmean=\Sexpr{f.elite.m}$ and $\ymean=\Sexpr{f.rural.m}$.
The standard deviations are $s_{\xsample}=\Sexpr{f.elite.sd}$ and $s_{\ysample}=\Sexpr{f.rural.sd}$.
Indeed, the formant frequency differs by \Sexpr{f.elite.m-f.rural.m} Hz (which is the distance $d$ mentioned above).
How can we apply a t-Test under these circumstances?
What are the logic and the maths of the \Key{Two-Sample t-Test}?

First of all, what is the substantive hypothesis?
It's that the true means between the DGPs producing the formant frequencies differ.
More precisely, it's that $\mu>\mu_1$:
The mean frequency in the coastal population is higher than in the rural population.
In other words, by hypothesis there are two DGPs and thus two populations, one corresponding to coastal speakers (with the unknown true mean $\mu$) and one corresponding to speakers from the Midwest (with the unknown true mean $\mu_1$).
Under the Null, the mean frequencies in the two populations are identical $H_0: \mu=\mu_1$.%
\footnote{Logically, the Null is that the mean frequencies are identical \textit{or the mean frequency in the rural population is lower} $H_0: \mu\leq\mu_1$.
For developing the maths of the t-statistic for two samples, we assume that the Null is just $H_0: \mu=\mu_1$.
This is because the maths only define the PDF (probability density function) of t-values if there is no effect.
Whether we perform a single-sided test or a two-sided test is just a matter of calculating the correct area under the PDF (centre, left, or right).}

This leads us nicely to the maths.
The t-Test, like the z-Test, is based on subtracting a population mean from a sample mean.
Remember Equation~\ref{eq:tone} for a single sample mean \xmean\ and a known true mean $\mu$, repeated here for convenience:

\begin{center}
  \begin{math}
    t=\cfrac{\xmean-\mu}{\ssigma_{\mu}}
  \end{math}
\end{center}

Mathematically, the population mean $\mu$ is a constant.
It's a known and fixed parameter of the population in question.
Furthermore, we assume that the sample means are normally distributed, and hence $\xmean-\mu$ should also be normally distributed.%
\footnote{Notice that the means themselves are always normally distributed, even when we're performing a t-Test.
Only the t-score is t-distributed because it adds an estimated variance to the calculation.}
How can we extend this to a two-sample case?
The t-Test is only applicable when we can subtract a \textit{known population parameter} (mathematically a constant) from a normally distributed sample statistic.
If we knew the \textit{difference between the population means} (mathematically also a constant), we could compare it to (in other words, subtract it from) the difference of the sample means.%
\footnote{As we'll demonstrate in Section~\ref{sec:normaldiffs}, differences of normally distributed random variables are themselves normally distributed random variables.}
The corresponding equation is this:%
\footnote{We'll return to the question of how to calculate the standard error $\ssigma_{\mu-\mu_1}$ presently.
For the moment, please believe us that there is an appropriate standard error for mean differences.}

\begin{equation}
  t=\cfrac{(\xmean-\ymean)-(\mu-\mu_1)}{\ssigma_{\mu-\mu_1}}
  \label{eq:tinterrim}
\end{equation}

\noindent As indicated above, Equation~\ref{eq:tinterrim} might not seem to be very useful as we clearly haven't got any idea what the true means $\mu$ and $\mu_1$ are.
But this isn't a dead end.
To see why, keep in mind that the test is always performed under the assumption that the Null is true.
But what is $\mu-\mu_1$ under the Null?
As both true means are identical under the Null, the difference between them is $\mu-\mu_1=0$.
Hence:

\begin{center}
  \begin{math}
    t=\cfrac{(\xmean-\ymean)-(\mu-\mu_1)}{\ssigma_{\mu-\mu_1}}=\cfrac{(\xmean-\ymean)-0}{\ssigma_{\mu-\mu_1}}
  \end{math}
\end{center}

\noindent The final t-statistic for two samples is thus:

\begin{equation}
  t=\cfrac{\xmean-\ymean}{\ssigma_{\mu-\mu_1}}
  \label{eq:ttwo}
\end{equation}

There's still one open question, though.
What is the appropriate standard error, which we referred to above simply as $\ssigma_{\mu-\mu_1}$?
There's also a minor catch:
The refreshingly simple logic and maths described above only work as expected if the population variances $\sigma$ and $\sigma_1$ are equal.
However, if that is the case \textit{and the two samples have the same size}, the estimated standard error for the mean differences $\ssigma_{\xmean-\ymean}$ is calculated by just adding the variances of the two samples.
Why do we just add the two?
The variance of the difference score $\xmean-\ymean$ obviously comes from \textit{two} means (estimated from two samples).
We subtract the sample means in order to make an inference about a difference between two potential population means.
But as both estimates contribute their own amount of uncertainty to the calculation, we have two sources of error.
Hence, the standard error adds one error component for each sample (see also Section~\ref{sec:normaldiffs}).
In the One-Sample t-Test, there is only one source of error and therefore only one error component.

\begin{equation}
  \ssigma_{\xmean-\ymean}=\sqrt{\frac{s^2_{\xsample}}{n_{\xsample}}+\frac{s^2_{\ysample}}{n_{\ysample}}}
  \label{eq:setwo}
\end{equation}

\noindent With this standard error, we can calculate the t-statistic using Equation~\ref{eq:ttwo}.
While the sample means and their difference are normally distributed, the unreliability of the standard error with low samples sizes means that the t-statistic is t-distributed and not normal.
This unreliability gets lower when the total number of data points from either sample or from both samples gets larger.
To account for his, the degrees of freedom for sample differences $\nu_{\xmean-\ymean}$ are simply the added degrees of freedom of the individual samples:

\begin{equation}
  \nu_{\xmean-\ymean}=\nu_{\xmean}+\nu_{\ymean}=(n_{\xsample}-1)+(n_{\ysample}-1)
  \label{eq:dfttwo}
\end{equation}

\subsubsection{A Sample Calculation}

Let's apply all of this to our example:

\begin{center}
  \begin{math}
    \ssigma_{\xmean-\ymean}=\sqrt{\cfrac{\Sexpr{f.elite.var}}{\Sexpr{f.n}}+\cfrac{\Sexpr{f.rural.var}}{\Sexpr{f.n}}}=\sqrt{\Sexpr{f.elite.var/f.n}+\Sexpr{f.rural.var/f.n}}=\Sexpr{f.se}
  \end{math}
\end{center}

\noindent and:

\begin{center}
  \begin{math}
    \nu_{\xmean-\ymean}=(\Sexpr{f.n}-1)+(\Sexpr{f.n}-1)=\Sexpr{(f.n-1)+(f.n-1)}
  \end{math}
\end{center}

\noindent The t-statistic is calculated as:

\begin{center}
  \begin{math}
    t=\cfrac{\Sexpr{f.elite.m}-\Sexpr{f.rural.m}}{\Sexpr{f.se}}=\cfrac{\Sexpr{f.elite.m-f.rural.m}}{\Sexpr{f.se}}=\Sexpr{f.t}
  \end{math}
\end{center}

\noindent By looking up the statistic $t=\Sexpr{f.t}$ in a table for the cumulative density of the t-Distribution ($\nu=\Sexpr{(f.n-1)+(f.n-1)}$, single-sided lookup), we get $p=\Sexpr{f.p}$.%
\footnote{Convenient tables can be found in the appendices.}
This doesn't even meet the lowest standard for an unexpected result under the Null ($sig=0.05$), and hence we do not conclude anything from this result.

\subsubsection{Standard Errors with Unequal Sample Sizes}

We pointed out that the regular t-Test and its standard error calculated as in Equation~\ref{eq:setwo} are only appropriate if both (potential) populations or DGPs have identical variances, and if the samples have the same size.
If the variances differ (even potentially), please refer to \Key{Welch's t-Test}, a variant of the t-Test with modified calculations to account for unequal variances.
We don't cover Welch's Test in this book for reasons of space and focus.
However, if the two samples are of different sizes but the population variances can be safely assumed to be equal there's a relatively simple solution.
Let's first consider why different sample sizes are a problem at all for calculating the standard error.

We know by now that larger samples represent (on average) any population (or DGP) better than smaller samples.
It is never the case that smaller samples lead to better estimates of any population parameter (such as mean and variance) than larger samples.
Remember that the two populations have to have identical variances for the Two-Sample t-Test to be applicable.
If there is a considerable difference in sample size, one sample---the larger one---is thus a better approximation of the common variance $\sigma^2=\sigma_1^2$ of $\mu$ and $\mu_1$.
By simply adding error components based in the individual samples' variance estimates, both are given the same weight in the calculation of the total error inherent in the estimation of the mean difference.
This cannot be the optimal solution, because smaller samples will inevitable drag down larger samples, at least on average.

At the same time, keep in mind that the amount of error contributed by the two samples depends on their respective sample sizes.
Our strategy to deal with these facts is as follows:
First, we calculate the best possible estimate of the common variance $\sigma^2=\sigma_1^2$ using information from \textit{both} samples.
Then, we use this variance estimate to introduce two error components into the standard error, which depend on their respective sample sizes.
The best possible estimate of the identical population variances $\sigma^2=\sigma_1^2$ is the so-called \Key{pooled variance} $s^2_{\xsample,\ysample}$ of the samples, given by:

\begin{equation}
  s^2_{\xsample,\ysample}=\frac{SQ_{\xsample}+SQ_{\ysample}}{(n_{\xsample}-1)+(n_{\xsample}-1)}
  \label{eq:varpooled}
\end{equation}

SQ stands for \textit{sum of squares}.
Please revise Section~\ref{sec:quantifyvariance} if you don't remember exactly what this means.
For comparison, the variance $s^2_{\xsample}$ of a single sample \xsample\ was given in Equation~\ref{eq:variance}, repeated here in condensed form for convenience:

\begin{center}
  \begin{math}
    s^2_{\xsample}=\cfrac{SQ_{\xsample}}{n-1}
  \end{math}
\end{center}

\noindent The pooled variance is what the name suggests:
We pool both samples to calculate a maximally precise estimate of the population variance.
If one sample is smaller, its sum of squares is smaller, but so is its sample size.
Hence, Equation~\ref{eq:varpooled} intrinsically weighs the samples' contribution to the common variance.
With $s^2_{\xsample,\ysample}$ (the variance estimated from both samples), the standard error is given as in Equation~\ref{eq:setwopooled}.
Notice that we couldn't simply pool the samples at this stage, as each error component depends specifically on the size of one of the two samples.%
\footnote{For further enlightenment:
Because the variance must be identical between the two populations (even if the means differ), we can fully \textit{pool} the samples in the calculation of the variance.
However, the error components in the calculation of the standard error are \textit{not} identical if the samples have different sizes.
Hence, there can't be full pooling.}

\begin{equation}
  \ssigma_{\xmean-\ymean}=\sqrt{\frac{s^2_{\xsample,\ysample}}{n_{\xsample}}+\frac{s^2_{\xsample,\ysample}}{n_{\ysample}}}
  \label{eq:setwopooled}
\end{equation}

<<setuppvar, echo=FALSE>>=
set.seed(723)

pvar.reps <- 100
pvar.n.x <- 5
pvar.n.y <- 20
pvar.sd <- 5

pvar.varx <- rep(0, pvar.reps)
pvar.vary <- rep(0, pvar.reps)
pvar.vara <- rep(0, pvar.reps)
pvar.varp <- rep(0, pvar.reps)

pvar.set <- sqrt(pvar.sd^2/pvar.n.x + pvar.sd^2/pvar.n.y)
@

<<pvarsim, fig.cap=paste0(pvar.reps, " simulations of standard errors for mean differences: calculated with simply added variances (gray squares) and calculated with pooled variances (blue dots and orange dots); sample sizes are ", pvar.n.x, " and ", pvar.n.y, "; the true SE is ", pvar.set, " (green line)"), fig.height=5, echo=F, fig.pos="t", cache=T>>=

par.defaults()

for (i in 1:pvar.reps) {
  .x <- rnorm(n = pvar.n.x, sd = pvar.sd)
  .y <- rnorm(n = pvar.n.y, sd = pvar.sd)

  pvar.varx[i] <- var(.x)
  pvar.vary[i] <- var(.y)
  pvar.vara[i] <- sqrt( ( var(.x)/pvar.n.x  + var(.y)/pvar.n.y ) )
  .pvar <- ( sum((.x-mean(.x))^2) + sum((.y-mean(.y))^2) ) / ( (pvar.n.x-1) + (pvar.n.y-1) )
  pvar.varp[i] <- sqrt( .pvar/pvar.n.x + .pvar/pvar.n.y )
}

the.order <- order(pvar.varp)
pvar.vara <- pvar.vara[the.order]
pvar.varp <- pvar.varp[the.order]

plot(pvar.vara,
     bty="n", xlab = "Replication (sorted)",
     ylab = "Estimate of Standard Error",
     col = "white",
     ylim = c(1, 4.6)
     )

for (i in 1:pvar.reps) {
  lines(x = c(i, i),
        y = c(pvar.vara[i], pvar.varp[i]),
        col = the.lightgray
        )
}

points(pvar.vara,
       pch = 15,
       col = the.lightgray)
points(pvar.varp, pch = 16,
       col = ifelse(abs(pvar.varp-pvar.set)<abs(pvar.vara-pvar.set),
                    alpha(the.cols[2], 0.75), alpha(the.cols[4], 0.75)))
abline(h = pvar.set, lwd = the.lwd, col = alpha(the.cols[1], 0.75))

legend("topleft", legend = c("SE with pooled variance (closer to true SE)",
                             "SE with pooled variance (farther from true SE)",
                             "SE with unweighted variance"),
       col = c(alpha(the.cols[2], 0.75),
               alpha(the.cols[4], 0.75),
               the.lightgray),
       pch = c(16, 16, 15), bty = "n")

pvar.perc <- round(length(which((abs(pvar.varp-pvar.set)<abs(pvar.vara-pvar.set))==T))/pvar.reps*100, 0)
@

Figure~\ref{fig:pvarsim} presents a plot of simulated estimates of standard errors from two samples.
In each of the \Sexpr{pvar.reps} replications, one sample of size \Sexpr{pvar.n.x} and one sample of size \Sexpr{pvar.n.y} were drawn from a DGP with $s=\Sexpr{pvar.sd}$ ($s^2=\Sexpr{pvar.sd^2}$).
We know (as it's a simulation) that the true standard error is:

\begin{center}
  \begin{math}
    \ssigma_{\mu-\mu_1}=\sqrt{\cfrac{\Sexpr{pvar.sd^2}}{\Sexpr{pvar.n.x}} + \cfrac{\Sexpr{pvar.sd^2}}{\Sexpr{pvar.n.y}}}=\Sexpr{pvar.set}
  \end{math}
\end{center}

In each replication, we calculated the standard error both \textit{incorrectly} by just adding the error components with individually computed variances (as if the samples were of the same size, Equation~\ref{eq:setwo}) and \textit{correctly} using the pooled variance (Equation~\ref{eq:setwopooled}).%
\footnote{To make the plot less cluttered, the replications were sorted by the standard error calculated from the pooled variance.}
The standard error using the pooled variance is closer to the true standard error in \Sexpr{pvar.perc}\% of the replications.
This illustrates that with the pooled variance, the (on average) less accurate variance estimate from the smaller sample gets fewer chances to drag down the precision of the (on average) more precise variance estimate from the larger sample.

<<setupformantsrev, echo=FALSE>>=

f.elite <- f.elite[1:(f.n-2)]
f.elite.m <- mean(f.elite)
f.elite.sq <- sum((f.elite-f.elite.m)^2)
f.rural.sq <- sum((f.rural-f.rural.m)^2)
f.pvar <- round( (f.elite.sq+f.rural.sq)/((length(f.rural)-1)+(length(f.elite)-1)), 2)

f.se <- round(sqrt(f.pvar/(f.n-2)+f.pvar/f.n), 2)
f.t <- round((f.elite.m-f.rural.m)/f.se, 2)
f.p <- round(pt(f.t, df = (f.n-3)+(f.n-1), lower.tail = F), 2)
@

\subsubsection{A Sample Calculation}

Returning to our fictitious experiment on fine phonetic details, let's assume that the last two data points from the elite sample \xsample\ had to be removed because it became known after the experiment that the two speakers were actually Canadian.
The updated sample size is $n=\Sexpr{length(f.elite)}$, the mean is $\xmean=\Sexpr{f.elite.m}$, the sample variance is $s^2_{\xsample}=\Sexpr{round(var(f.elite), 2)}$ ($s_{\xsample}=\Sexpr{round(sd(f.elite), 2)}$).
The pooled variance is calculated like so:

\begin{center}
  \begin{math}
    s^2_{\xsample,\ysample}=\cfrac{\Sexpr{f.elite.sq}+\Sexpr{f.rural.sq}}{\Sexpr{f.n-1}+\Sexpr{f.n}}=\Sexpr{round(f.pvar, 2)}
  \end{math}
\end{center}

\noindent The updated standard error is:

\begin{center}
  \begin{math}
    \ssigma_{\xmean-\ymean}=\sqrt{\cfrac{\Sexpr{f.pvar}}{\Sexpr{f.n-2}}+\cfrac{\Sexpr{f.pvar}}{\Sexpr{f.n}}}=\sqrt{\Sexpr{round(f.pvar/(f.n-2), 2)}+\Sexpr{round(f.pvar/f.n, 2)}}=\Sexpr{f.se}
  \end{math}
\end{center}

\noindent and:

\begin{center}
  \begin{math}
    \nu_{\xmean-\ymean}=(\Sexpr{f.n-2}-1)+(\Sexpr{f.n}-1)=\Sexpr{(f.n-3)+(f.n-1)}
  \end{math}
\end{center}

\noindent The t-statistic is calculated as:

\begin{center}
  \begin{math}
    t=\cfrac{\Sexpr{f.elite.m}-\Sexpr{f.rural.m}}{\Sexpr{f.se}}=\cfrac{\Sexpr{f.elite.m-f.rural.m}}{\Sexpr{f.se}}=\Sexpr{f.t}
  \end{math}
\end{center}

\noindent By looking up the statistic $t=\Sexpr{f.t}$ in a table for the cumulative density of the t-Distribution ($\nu=\Sexpr{(f.n-3)+(f.n-1)}$, single-sided lookup), we get $p=\Sexpr{f.p}$.
Without the data points from the two Canadians the mean distance has shrunken considerably.
Also, the standard error went up slightly, leading to a result that is not unexpected at all under the Null.
Please keep in mind that the high p-value still is \textit{not} in any way \textit{evidence in favour of the Null}!
Under the Null, all p-values have the same probability.

\subsubsection{Conditions for Testing Mean Differences}\label{sec:conditionmeantests}

We hope that we could convince you that these tests are (as most people think) quite basic, but that there are lots of small catches and details to take into account.
While the Two-Sample t-Test is applicable to many more situations than the z-Test, we'd like to point out that it's not a test you can apply to any sample means whatsoever.
Table~\ref{tab:tconds} summarises the conditions that must be met for the tests discussed in this chapter to be applicable.
If they are not met, inferences will be marred:
The error rates that you hope to control will \textit{not} be at the level you think they are.
Please revise Section~\ref{sec:tvariance} (especially Figure~\ref{fig:tsim}) in order to make absolutely sure that you understand this.

\begin{table}
  \caption{\label{tab:tconds}Necessary conditions for z-Tests and t-Tests}
  \centering
  %\resizebox{\textwidth}{!}{%
  \begin{tabular}{lccc}
    \toprule
    &                 & \textbf{One-Sample}   & \textbf{Two-Sample}   \\\cmidrule{3-4}
    & \textbf{z-Test} & \multicolumn{2}{c}{\textbf{t-Test}}           \\
    \midrule
    The data are numerical.             & \multicolumn{3}{c}{\CellBlue\dink{192}}             \\
    The means are normally distributed. & \multicolumn{3}{c}{\CellGreen\dink{193}}            \\
    The data are normally distributed.  & \multicolumn{3}{c}{\CellBlue\dink{194}}             \\
    The measurements are independent.   & \multicolumn{3}{c}{\CellGreen\dink{195}}            \\
    The population mean is known.       & \multicolumn{2}{c}{\CellBlue\dink{196}} & \Dim      \\
    The population variance is known.   & \CellGreen\dink{197}     & \multicolumn{2}{c}{\Dim} \\
    The population variances are equal. & \multicolumn{2}{c}{\Dim} & \CellBlue\dink{198}      \\
    The samples are independent.        & \multicolumn{2}{c}{\Dim} & \CellGreen\dink{199}     \\
    \bottomrule
  \end{tabular}
  %}
\end{table}

Some of these requirements have not been mentioned very prominently.
So, let's go through them.

\noindent\ding{192}~\textbf{The data are numerical.}
For any of the tests of mean differences (such as z-Tests and t-Tests) the mean has to be computable, which is the case only for numerical measurements.
In principle, this is all there is to say.
However, we'd like to tell a story:
We know of a paper in linguistics (which we gracefully refuse to reference) where a theoretical syntactician and an empiricist demonstrated something something very convenient for experiments where acceptability ratings are gathered.%
\footnote{For non-linguists, let us explain:
Linguists sometimes ask naive speakers to rate sentences for their acceptability.
Participants are presented with sentences as stimuli, and they're asked to press one button if the sentence sounds \textit{acceptable} to them in their native language and another button if it doesn't, which results in a binary measurement.
Alternatively, they are asked to rate sentences on an absolute or relative scale, which results in (something like) numerical measurements.}
The authors showed that instead of numerical ratings of sentence acceptability (such as split-100 ratings or magnitude estimation) one could also use binary ratings (acceptable/not acceptable).
Calculating a t-Test on the numerical ratings led to the same conclusion as calculating a t-Test on proportions.
The idea was something along the following lines:
When participants rate a sentence as 60\% acceptable in the numerical measurement, 60\% of the participants in the binary measurement rated the sentence as fully acceptable.
So, when you have 10 sentences for condition A and 10 sentences for condition B, then you can take the mean of the 10 proportions of \textit{acceptable} ratings for condition A and B and perform a t-Test.
Mind-boggling!
Besides the legions of devils that lie in the details with this approach, consider this:
They tried this out in a single instance, and the inferences were the same for the mishandled binary ratings as for the numerical ratings.
You're familiar enough with frequentist thinking to know that you can't test whether you can break the rules of a test by trying it out \textit{once}.
The true error rate can only be found in the limit, which requires either mathematical proof or a long---very long---series of experiments.
Please revise Figure~\ref{fig:tsim}.
Frequentism is all about the control of error rates, and you need to obey the rules (meet the conditions of your tests) to ensure that your error rates are what you think they are.
\textit{We tried it out once, and it worked!} isn't acceptable.

\noindent\ding{193}~\textbf{The means are normally distributed.}
The normality of means underlies our entire logic of inferences about them.
It's a typically frequentist condition, and it's quite difficult to check whether it holds in a specific instance.
We usually have only a single sample (or at least not more than a few) from which we calculate the mean.
It's impossible to check whether this one sample comes from a normally distributed population of means, but at the same time the frequentist machinery requires it to be normal.
There is no easy solution, but there is one, which we'll discuss immediately.

\noindent\ding{194}~\textbf{The data are normally distributed.}
Technically speaking, the data themselves (reaction times, formant frequencies, sentence lengths, etc.) don't need to be normally distributed.
Only the means themselves must fulfil this requirement.
Interestingly, there is a fundamental result (which at first appears very counterintuitive to many) which roughly states that sample means drawn from any underlying distribution will be normal at sufficiently large sample sizes.
This fundamental result is called the \Key{Central Limit Theorem} (CLT).
If you watched the videos recommended in Section~\ref{sec:gaussian} you've already got a basic idea of what it is.
In any case, if samples are large enough, the data themselves need not be normally distributed, but the mean will be.
Unfortunately, as nobody knows in advance what the distribution of measurements in your data sets is going to be, nobody can tell you in advance what is \textit{large enough}.
The safest way to meet the condition of normality of means is having normal data, which entails that means will always be normal, regardless of the sample size.
There are specific tests to check whether data are normally distributed, but we believe they do not land anywhere near the perfect spot between rejecting too many data sets as non-normal and accepting too many as normal.
Try a raw plot, a histogram, or a density plot to get a rough idea if your data's normality.

<<lognormal, fig.cap="Left: idealised discrete distribution of sentence lengths measured in words; Right: a continuous Lognormal Distribution corresponding to the left panel", fig.height=5, echo=F, fig.pos="t", cache=T>>=

  par.defaults()
  par(mfrow = c(1,2), mar = c(3,0,2,1), xpd=NA)

  lnorm.ls <- seq(0.1, 10, 0.4)
  lnorm.ls.fine <- seq(0.2, 10, 0.04)
  lnorm.labels <- c(1, seq(5, length(lnorm.ls), 5))
  lnorm.mu <- 1
  lnorm.sd <- 0.6

  lnorm.ys <- dlnorm(lnorm.ls, mean = lnorm.mu, sd = lnorm.sd)

  plot(x = lnorm.ls,
            y = lnorm.ys,
            pch = 20, bty="n",
            xlab = "Sentence length in words", xaxt = "n",
            ylab = "", yaxt = "n",
            col = "white"
            )
  lines(x = lnorm.ls,
        y = lnorm.ys,
        col = the.lightgray, lwd = the.lwd
        )
  points(x = lnorm.ls,
         y = lnorm.ys,
       pch = 20,
       bty="n", xlab = "Sentence length in words", xaxt = "n",
       ylab = "", yaxt = "n",
       col = "black", lwd = the.lwd
       )
  axis(side = 1,
       at = lnorm.ls[lnorm.labels],
       labels = lnorm.labels)

  plot(dlnorm(lnorm.ls.fine, mean = lnorm.mu, sd = lnorm.sd),
            type = "l", lwd = the.lwd, bty="n",
            xlab = "Sentence length in words",
            ylab = "", yaxt = "n", xaxt = "n",
            col = the.cols[1]
            )
    axis(side = 1,
       at = c(1, seq(50, 250, 50)),
       labels = lnorm.labels)

    par.defaults()
@

In linguistics---and especially in corpus linguistics---, many data are known to not be normally distributed.
Because many types of data from corpora are counts or frequencies, their distributions are discrete anyway, but they're often skewed or diverge from the Normal Distribution in other ways, too.
If you look at sentence lengths measured in words, for example, it could never be truly normal because sentence lengths are discrete.
A sentence can consist of 1, 2, 3, \ldots\ words, but not 3.812 words, for example.
Additionally, the distribution of sentence lengths usually looks approximately as in the left panel of Figure~\ref{fig:lognormal}.%
\footnote{Many experts will probably scream at us because distribution of sentence lengths varies greatly by factors such as style, and the distribution as shown is not a good model of sentence lengths, etc.
This is all true, but for the purpose of a mere illustration of basic differences to the Normal Distribution, Figure~\ref{fig:lognormal} is quite accurate.}

According to Figure~\ref{fig:lognormal}, there are very few very short sentences (of 1 and 2 words), but there are a lot of sentences between roughly 3 and 10 words long.
For longer sentence lengths, the frequency decreases sharply.
The distribution clearly isn't normal, but it looks similar to the continuous \Key{Log-Normal Distribution} (maybe with some scaling and offsetting).
The Log-Normal Distribution has the welcome property of being defined for positive values only, which is plausible for things like sentence lengths, as sentences cannot be $0$ or $-1$ words long.
A density function of the corresponding Log-Normal Distribution is shown in the right panel of Figure~\ref{fig:lognormal}.
Interestingly, the Log-Normal Distribution is called Log-Normal because it turns into a Normal Distribution if you logarithmise the values.

Hence, in order to ensure that your sample means are normal, there are at least two strategies.
You can logarithmise the underlying values and thus indirectly force the means to be normal.
Or you can increase the sample size and rely on the CLT to ensure that the sample means are normally distributed.
Increasing the sample size has some side-effects to be taken into account (see Chapter~\ref{sec:powerseverity}), but it's relatively unproblematic if you're aware of the side-effects.
Logarithmising the values is an instance of a \Key{data transformation}.
If the data don't have the properties they should have, a transformation is any mathematical operation that brings the data closer to the expected distribution in order to make correct frequentist inferences possible.
There is a considerable downside, however.
Assume you want to show that mean sentence lengths in forewords to English novels from two decades differ.
Because the data aren't normal and you've got a limited amount of data, you logarithmise them and make an inference, rejecting the Null which says that the means are equal.
You also observe that the mean difference is 0.04.
But what does this mean?
What do logarithmised sentence lengths mean within your theory, and how do you interpret the result?
You've not made an inference about sentence lengths but about logarithmised sentence lengths.
Such questions should be considered with great care, and they are theoretical questions, not mathematical ones.

\noindent\ding{195}~\textbf{The measurements are independent.}
We'll keep this one short as we'll return to it later in the book anyway.
For inferences to be valid, each measurement in your sample(s) must come from a random event \textit{independent} of all the other events (see Section~\ref{sec:probability}).
This condition is violated very easily.
However, it's (once again) statistics can't decide whether it's violated or not.
It's your job when you design an experiment and analyse the data.

Many situations can potentially lead to non-independence of measurements:
asking the same person more than once for an acceptability rating;
taking measurements in a temporal sequence where one event influences the probabilities in the subsequent event(s);
using the same verb more than once in different stimulus sentences;
using corpus data from texts written in two very different styles of writing without accounting for those stylistic differences.
Each of these situations introduces \Key{grouping} into the data.
You get groups of ratings per participant; chains of measurements where the previous one affects the subsequent ones; groups defined by lexical items; groups defined by styles of writing.
We'll discuss advanced techniques of taking care of grouping (but not temporal sequences) in Chapter~\ref{sec:mixed}.
However, in simple tests like the t-Test, such grouping can lead to a situation where the distribution of variance in the data is more complex than the test assumes, which (again) means that your actual error rates might be off from the nominal error rates.
For example, the Two-Sample t-Test is applicable if the variance between the two groups is homogeneous and equal, and if the only difference between the groups is the mean difference, which is causally related to the substantive distinction the researcher is interested in.

\noindent\ding{196}~\textbf{The population mean is known.}
This is straightforward.
The z-Test and the One-Sample t-Test require that the population mean is known beyond doubt.
If it's an estimate, you're introducing variability which is not accounted for in the maths, and your error rates will be off.
There's no way to check for this.
You---the researcher---need to be aware of what you're doing.

\noindent\ding{197}~\textbf{The population variance is known.}
Again, this is a very straightforward matter.
Go back to Figure~\ref{fig:tsim} to see what happens if you pretend you know the population variance (and perform a z-Test) when in fact you just estimated it (and should perform a t-Test).

\noindent\ding{198}~\textbf{The population variances are equal.}
For the Two-Sample t-Test, the variances in the two populations must be equal beyond doubt.
If in doubt, use Welch's variant of the t-Test.

\noindent\ding{199}~\textbf{The samples are independent.}
This is a variation on the theme from \ding{195}.
In a Two-Sample t-Test, there must be no groupings between data points from one group and the other.
For example, if you measure the same participants' reactions under two conditions, there is a pairwise grouping between reactions from individual participants under condition A and condition B.
If you have data that is structured like this, use the t-Test for Repeated Measures.
It's just a small amendment, but it will greatly improve your inferences.

\Bigpoint{You Cannot Just Apply Any Test to Any Data}{%
The whole of Section~\ref{sec:conditionmeantests} is a big point.
Read it twice, and memorise it.
}

\subsection{\Indepth\ Mean Differences Are Normally Distributed}\label{sec:normaldiffs}

<<setupndiff, cache=T, echo=FALSE, include=FALSE, results='asis'>>=
  ndiff.reps <- 10000
  ndiff.a <- rnorm(n = ndiff.reps, mean = 3)
  ndiff.b <- rnorm(n = ndiff.reps, mean = 1)
  ndiff <- sample(ndiff.a)-sample(ndiff.b)
  ndiff.m <- mean(ndiff)
  ndiff.sd <- sd(ndiff)
  @

<<ndiff, fig.cap=paste0("Left: Density plots of ", ndiff.reps, " random draws each from a Normal Distribution with mean μ=3 and σ=1 (blue), from another Normal Distribution with mean μ₁=1 and σ₁=1 (blue) and the subtraction of random pairs from the second from the first; Right: the theoretical distributions corresponding to the left panel"), fig.height=5, echo=F, fig.pos="t", cache=T>>=

  par.defaults()
  par(mfrow = c(1,2), mar = c(3,0,2,0), xpd=NA)

  plot(density(ndiff.a, from = -3, to = 7),
       lwd = the.lwd, main = "Random Draws",
       col = the.cols[1],
       bty="n", xlab = "Value", xaxt = "n",
       yaxt = "n", ylab = ""
  )
  lines(density(ndiff.b, from = -3, to = 7),
        lwd = the.lwd,
        col = the.cols[2]
  )
  lines(density(ndiff, from = -3, to = 7),
        lwd = the.lwd,
        col = the.cols[3]
  )
  axis(1, at = seq(-3, 7, 2), labels = seq(-3, 7, 2))

  xs <- seq(-3, 7, 0.01)
  plot(dnorm(xs, mean = 3), type = "l",
       lwd = the.lwd, main = "Theoretical Distribution",
       col = the.cols[1],
       bty="n", xlab = "Value", xaxt = "n",
       ylab = "", yaxt = "n"
  )
  lines(dnorm(xs, mean = 1),
        lwd = the.lwd,
        col = the.cols[2]
  )
  lines(dnorm(xs, mean = 2, sd = 1.41),
        lwd = the.lwd,
        col = the.cols[3]
  )
  axis(1, at = c(1, seq(200, 1000, 200)), labels = seq(-3, 7, 2))

  legend(-225, 0.4, c("μ=3", "μ₁=1", "μ-μ₁=2"), lwd = the.lwd,
         col = the.cols[1:3], bty = "n"
  )
  par.defaults()
  @

In Section~\ref{sec:logicttwo}, we assumed that subtracting two normally distributed random variables results in another normally distributed random variable.
In this section, we illustrate this fundamental result further.
The claim was that if $\xmean$, $\ymean$ are normally distributed, then $\xmean-\ymean$ is also normally distributed.
We show this in Figure~\ref{fig:ndiff}.
The left panel provides a density plot of \Sexpr{format(ndiff.reps, scientific=F)} random draws (extremely simple simulations) of two normally distributed random variables:
\xsample\ with $\mu=3$ and \ysample\ with $\mu_1=1$, both with $\sigma=\sigma_1=1$.

Also, we plotted $\xsample-\ysample$, a subtraction of the randomly paired values from \xsample\ and \ysample.
As expected, the mean is $\mu-\mu_1=3-1=2$.
Furthermore, as the subtraction of the two random variables introduces \textit{two} sources of uncertainty, the variance of $\xsample-\ysample$ is larger than that of the two independent variables.
The standard deviation is around $\Sexpr{round(ndiff.sd,1)}$.%
\footnote{How to get to this value analytically is a more complex matter which is better reserved for textbooks focussing on the mathematical side of things.}
This directly illustrates what we said in Section~\ref{sec:logicttwo} about adding two error components to calculate the standard error for mean differences (Equations~\ref{eq:setwo} and~\ref{eq:setwopooled}).

To show how simulations and random draws converge with theoretically known facts, we added the right panel of Figure~\ref{fig:ndiff}.
It's the theoretical expectation for the random draws shown in the left panel.
It's a theoretical \textit{expectation} inasmuch as we can safely expect the left panel to look more and more like the right panel as the sample sizes approach infinity.
And on that bombshell, we move on to the next chapter.

\begin{exercises}

\exercise{z1} \xsample\ as given below is a series of numeric measurements.
The known population variance is $\sigma=$.
Under the substantive hypothesis that the mean is smaller than the know population mean $\mu=$, formulate a Null and perform a z-Test.
All required statistics should be calculated by hand with a pocket calculator.
Interpret the result, and make an inference if possible.

\exercise{z2} \xsample\ as given below is a series of numeric measurements.
The known population variance is $\sigma=$.
Under the substantive hypothesis that the mean is greater than the know population mean $\mu=$, formulate a Null and perform a z-Test.
All required statistics should be calculated by hand with a pocket calculator.
Interpret the result, and make an inference if possible.

\exercise{z3} \xsample\ as given below is a series of numeric measurements.
The known population variance is $\sigma=$.
Under the substantive hypothesis that the mean is different from the know population mean $\mu=$, formulate a Null and perform a z-Test.
All required statistics should be calculated by hand with a pocket calculator.
Interpret the result, and make an inference if possible.

\exercise{t1} Assuming the population variance is unknown, perform a t-Test corresponding to Exercise~\ref{exer:z3}.
Interpret the differences in the results between the z-Test and the t-Test.

\exercise{t2} For the samples \xsample\ and \ysample\ given below, perform a t-Test for differences in sample means under the substantive hypothesis that \ymean\ is smaller than \xmean.
Formulate a Null and perform the test.
All required statistics should be calculated by hand with a pocket calculator.
Interpret the result, and make an inference if possible.

\exercise{t3} For the samples \xsample\ and \ysample\ given below, perform a t-Test for differences in sample means under the substantive hypothesis that \xmean\ and \ymean\ are not equal.
Formulate a Null and perform the test.
All required statistics should be calculated by hand with a pocket calculator.
Interpret the result, and make an inference if possible.

\exercise{zandt10} We have argued in Section~\ref{sec:distpz} that larger effect sizes (increasing actual differences between the known population mean and the measured means) tilt the distribution of p-values increasingly towards 0.
There is a tradition where researchers don't look at the concrete p-value but just perform a mechanical significance test at a pre-set sig-level such as $sig=0.05$.
This means that tests where $p\leq sig$ are called \textit{significant}, all others \textit{not significant}.
Under such an approach, do you get in the long run: (i)~more significant results, (ii)~less significant results, or (iii)~a constant number of significant results when the actual effect strength increases (all other things being equal)?
Still under this approach, is your \textit{error rate} constant or does it change with increased effect strength?
The error rate is the percentage of samples which make you erroneously reject the Null when it is true.

\exercise{zandt20} A fictitious big data study in sociolinguistics has found through a popular social media app (and only from users that consented to be part of the experiment) that the mean time it took self-identified men to read a specific message about tomato ketchup was 8.9 seconds ($n_x=45231$, $s_x^2=0.3$), whereas the mean reading time for self-identified women was 9.2 seconds ($n_y=21231$, $s_y^2=0.4$).
The theory-driven substantive hypothesis was that men should read the message faster.
Formulate the Null and perform a t-Test for means from two samples.
Calculate all required statistics using a pocket calculator, including the appropriate variance.
Interpret and evaluate the result.

\exercise{zandt30} The co-supervisor of the dissertation that led to the study described in Exercise~8 reads the report and quickly writes an email to the author, cc'ing the supervisor.
They say that the study is invalid anyway because the sample wasn't \textit{balanced}.
It should have contained an equal number of data points from men and from women in order to be \textit{properly representative of all relevant groups in the study}.
What do you -- the main supervisor -- reply in order to defend your candidate?
(We experienced this exact situation at some university some time ago, except in a different sub-field of linguistics and with real data.)

\exercise{zandt40} The fictitious study on formants in Section~\ref{sec:ttwo} didn't produce surprising (\ie, \textit{significant}) results, and the authors point out in the publication that the experiment did not lend support to their substantive hypothesis, discussing potential reasons for the failure as well as possible future experiments.
The publication was pre-registered (see Chapter~\ref{sec:inference}) and was published despite reporting only negative results.
Swiftly, a proponent of a rival theory submits a short reply to the \textit{Bulletin of the Sociolinguistic Society}.
The rival theory explicitly predicts that rural folk should \textit{not} produce higher F1 frequencies in the word \textit{weakling}.
The author of the short reply argues that the failed experiment indirectly counts as evidence for the rival theory and demands that the original paper be retracted because the theory that motivated the reported experiment is clearly inferior, maybe even utterly false.
You are known as an expert on statistics and asked to review the short reply.
Do you recommend to accept it or to rejected it?
What do you think about the demand that the original paper be retracted?

\end{exercises}
