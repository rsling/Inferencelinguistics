% !Rnw root = ../main.Rnw

<<setupzandt, cache=FALSE, echo=FALSE, include=FALSE, results='asis'>>=
opts_knit$set(self.contained=FALSE)
@

\chapter{Inference: Mean Differences}
\label{sec:zandt}

\section*{Overview}

In this chapter, we introduce the z-test and the t-test.
We have introduced the logic of frequentist testing (Chapter~\ref{sec:fisher}), shown how samples can be summarised through measures of central tendency (Chapter~\ref{sec:describing}), and we've shown how we can adjust the safety and the precision of means and proportions estimated from samples (Chapter~\ref{sec:confidence}).
In this chapter, we return to inferential tests using the results from the chapters on descriptive statistics and estimation.

The tests introduced in this chapter compare means of samples.
The z-test compares the mean of a sample with the mean of a population for which the true mean and the true variance are known for sure.
The t-test for a single sample does the same but for cases where the true variance isn't known.
Finally, the t-test for two samples compares means of two independent samples.

The substantive hypothesis in such tests is usually that the means differ.
Hence, the Null is that they are the same.
We calculate the frequentist probability of measuring the difference between the means that was actually measured (or a larger difference) if the Null is true.
The frequentist probability is not calculated directly as in our presentation of Fisher's Exact Test.
Instead, we calculate a test statistic ($z$ or $t$), for which the probability density is known (in the form of the Normal Distribution and the very similar t Distribution) and we can use it to look up p-values.

\Problem{Z-Tests}{%
Let's assume you know the mean reaction time for a critical region when native speakers process a certain type of relative clause.
This mean reaction time and the corresponding variance in measurements are extremely well established parameters.
They were predicted by a robust theory of syntactic processing, and this prediction has been corroborated by a large number of diverse experiments.
For an emergent subtype of this kind of relative clause, the theory predicts considerably higher precessing effort and thus longer reaction times.
You conduct an experiment and measure reaction times in the critical region.
\alert{Which outcomes of the experiment would you interpret as indidcating that reaction times are indeed longer for the emergent type of relative clause?}}

\section{Population Means and Sample Means}

\subsection{Introducing the Logic}

The Problem Statement exemplifies a simple question:
Given a known population mean, do means measured under a specific condition diverge from this known mean?
In this section, we show through simple frequentist reasoning how measurements from experiments can provide evidence to answer such questions.%
\footnote{Again, we caution readers that we can neither (by no means!) \textit{decide} the questions nor \textit{provide hard evidence} for any possible answer to them, etc.}
The simplest test for such tasks is the \Key{z-Test}.
Notice that for the z-Test to be applicable, the given population mean (and the corresponding variance) must be \textit{known}!
The test does not take into account any uncertainty in the value for the known population mean, and if you disregard that fact, you will end up in inference hell.
This is why the Problem Statement stresses that the mean was predicted by a robust theory and that the prediction was tested in a long series of experiments.
If these conditions are not met, other tests (such as the t-test for two samples) might apply, and we're going to introduce such tests as we go along.

<<z.setup, echo=FALSE>>=
  set.seed(3689)
  z.n <- 1000
  z.mu <- 120
  z.sigma <- 4
  z.sample <- rnorm(n = z.n, mean = z.mu, sd = z.sigma)
  z.minmax <- c(105, 135)
  z.dnorm <- function(x) {
    dnorm(x, mean = z.mu, sd = z.sigma)
  }

  set.seed(673)
  z2.n <- 16
  z2.mu <- 123
  z2.sample <- rnorm(n = z2.n, mean = z2.mu, sd = z.sigma)
  z2.dnorm <- function(x) {
    dnorm(x, mean = z2.mu, sd = z.sigma)
  }

  z2.samplemean <- mean(z2.sample)

  z2.se <- z.sigma/sqrt(z2.n)
  z2.se.rnd <- round(z2.se, 2)
  z2.dnorm.sample <- function(x) {
    dnorm(x, mean = z.mu, sd = z2.se)
  }

  z2.mu <- mean(z2.sample)
  z2.z <- (mean(z2.sample)-z.mu)/z2.se.rnd
  z2.z.rnd <- round(z2.z, 2)

  z2.p <- 1-pnorm(z2.z)
  z2.p.rnd <- round(z2.p, 3)
@

For the sake of illustration, let's assume that the population mean is $\mu=\Sexpr{z.mu}$ (for example milliseconds) and the population variance is\label{page:varianceisfour} $\sigma^2=\Sexpr{z.sigma^2}$, which corresponds to a standard deviation of $\sigma=\Sexpr{z.sigma}$.
If the population values are generated according to a normal distribution, values are distributed according to the now well-known \Key{Probability Density Function} (PDF) in Figure~\ref{fig:z1}.

<<z1, fig.cap=paste0("Theoretical population distribution for a normal distribution with μ=", z.mu, " and σ=", z.sigma, " and a simulated random sample of n=", z.n, " measurements from the population"), echo=F, fig.pos="t", cache=F>>=

par(mfrow = c(2,1), mar = c(2,1,1,1), family = "Plotfont")

plot(x = seq(z.minmax[1], z.minmax[2], 0.1),
     y = z.dnorm(seq(z.minmax[1], z.minmax[2], 0.1)),
     type = "l", lwd = the.lwd,
     bty="n", yaxt="n", ylab = "",
     xlim = z.minmax, col = "white"
)
lines(x = c(z.mu, z.mu), y = c(0, z.dnorm(z.mu)),
      lwd = the.lwd, col = the.cols[2])
lines(x = c(z.mu-z.sigma, z.mu-z.sigma), y = c(0, z.dnorm(z.mu-z.sigma)),
      lwd = the.lwd, col = the.cols[1])
lines(x = c(z.mu+z.sigma, z.mu+z.sigma), y = c(0, z.dnorm(z.mu+z.sigma)),
      lwd = the.lwd, col = the.cols[1])
text(x = z.mu+0.6, y = z.dnorm(z.mu)/2, labels = "μ", col = the.cols[2])
text(x = (z.mu-z.sigma)+1.1, y = z.dnorm(z.mu-z.sigma)/3, labels = "μ-σ",
     col = the.cols[1])
text(x = (z.mu+z.sigma)+1.3, y = z.dnorm(z.mu+z.sigma)/3, labels = "μ+σ",
     col = the.cols[1])
lines(x = seq(z.minmax[1], z.minmax[2], 0.1),
     y = z.dnorm(seq(z.minmax[1], z.minmax[2], 0.1)),
     lwd = the.lwd, col = the.darkgray
)

plot(x = z.sample, y = 1:length(z.sample),
     pch = 20,
     bty="n", yaxt="n", ylab = "", cex = 1.5,
     xlim = z.minmax, col = alpha("darkgray", 0.5))

par(mfrow = c(1,1))
@

To recapitulate, the PDF gives for each measurement (x-axis) the probability with which it occurs (y-axis).
Informally speaking, the curve shows that if we measure random values from this population, the probability of measuring a value close to $\mu$ is highest, and measurements deviate on average by the standard deviation $\sigma$ from $\mu$.
The blue line shows the mean, and the green lines show one standard deviation in each direction from the mean.
As a result, a very much expected sample of $n=\Sexpr{z.n}$ measurements is shown in the form of the point cloud below the curve (90° rotated compared to the raw data plots we've shown before).
It's a simulated sample, and the simulation was set up according to the known facts about the overall population ($\mu=\Sexpr{z.mu}$and $\sigma^2=\Sexpr{z.sigma^2}$).
The measurements are indeed centred around the mean, and they seem to follow the normal distribution.

If, however, we draw a sample from a different population where the true mean is slightly higher (for example because we're measuring reaction times under a condition that incurs a processing penalty) we expect samples to turn out differently on average and have a higher sample mean compared to the known population mean.
However, this on-average expectation can be treacherous because individual samples are not in any way \textit{guaranteed} to represent their population well, as we have shown in Chapter~\ref{sec:confidence}.
Very similar to Ronald A.~Fisher in his experiment with Muriel Bristow (see Chapter~\ref{sec:fisher}), we need to ask whether the actual sample warrants any inference regarding the underlying mechanism.
It does that if it's a sufficiently unexpected result under the assumption that the desired inference is not correct, \ie, under the Null.
It's sufficiently unexpected if it had a low pre-experiment probability of occurring.

We'll show how this works out for the given example.
In the case of the reaction times described in the Problem Statement, substantive hypothesis is that reaction times are higher with the emergent subtype of relative clauses because of assumed processing penalties.
However---especially if our sample is small---inferring anything from a specific result is tricky.
Figure~\ref{fig:z2} shows a possible outcome with $n=\Sexpr{z2.n}$ in red, and it should be obvious why it's tricky to infer anything from it.
The mean from the sample is higher than the theoretically known mean, but this might very well be just a random deviation.
In frequentist terms, such a sample is expected under the Null as well.
We need to know the probability of a sample that deviates from the known mean to such a degree (or a stronger degree) in order to proceed to an inference.

<<z2, fig.cap=paste0("Theoretical population distribution for a normal distribution with μ=", z.mu, " and σ=", z.sigma, " and a simulated random sample of n=", z2.n, " measurements from some population"), echo=F, fig.pos="t", cache=F>>=

par(mfrow = c(2,1), mar = c(2,1,1,1), family = "Plotfont")

plot(x = seq(z.minmax[1], z.minmax[2], 0.1),
     y = z.dnorm(seq(z.minmax[1], z.minmax[2], 0.1)),
     type = "l", lwd = the.lwd,
     bty="n", yaxt="n", ylab = "",
     xlim = z.minmax, col = "white"
)
lines(x = c(z.mu, z.mu), y = c(0, z.dnorm(z.mu)),
      lwd = the.lwd, col = the.cols[2])
lines(x = c(z.mu-z.sigma, z.mu-z.sigma), y = c(0, z.dnorm(z.mu-z.sigma)),
      lwd = the.lwd, col = the.cols[1])
lines(x = c(z.mu+z.sigma, z.mu+z.sigma), y = c(0, z.dnorm(z.mu+z.sigma)),
      lwd = the.lwd, col = the.cols[1])
text(x = z.mu+0.6, y = z.dnorm(z.mu)/2, labels = "μ", col = the.cols[2])
text(x = (z.mu-z.sigma)+1.1, y = z.dnorm(z.mu-z.sigma)/3, labels = "μ-σ",
     col = the.cols[1])
text(x = (z.mu+z.sigma)+1.3, y = z.dnorm(z.mu+z.sigma)/3, labels = "μ+σ",
     col = the.cols[1])
lines(x = seq(z.minmax[1], z.minmax[2], 0.1),
     y = z.dnorm(seq(z.minmax[1], z.minmax[2], 0.1)),
     lwd = the.lwd, col = the.darkgray
)

plot(x = z.sample, y = 1:length(z.sample),
     pch = 20, cex = 1.5,
     bty="n", yaxt="n", ylab = "",
     xlim = z.minmax, col = alpha("darkgray", 0.5))
points(x = z2.sample, y = sample(1:length(z.sample), length(z2.sample)),
     pch = 20, cex = 2,
     bty="n", yaxt="n", ylab = "",
     xlim = z.minmax, col = alpha(the.cols[3], 0.75))
abline(v = rep(mean(z2.sample)),
      lwd = the.lwd, col = the.cols[3])
text(x = mean(z2.sample)+2, y = 300,
     labels = bquote(bar("x")*"="*.(round(mean(z2.sample), 2))),
     col = the.cols[3])

par(mfrow = c(1,1))
@

Let's call the sample plotted in Figure~\ref{fig:z2} \xsample, a tuple of \Sexpr{z2.n} measurements $x_1$ through $x_{\Sexpr{z2.n}}$.
The mean of \xsample\ is $\xmean=\Sexpr{round(mean(z2.sample), 2)}$.
As we've shown, inferences in frequentist logic (see Chapter~\ref{sec:fisher}) are always made by taking into account what the outcome of an experiment could have been under one or several possibly correct hypotheses.
In the case at hand, we're interested in the hypothesis that the true mean under the experimental condition (call it $\mu_1$) is larger than the known mean $\mu$.
In other words, we would like to \alert{gather evidence in support of a substantive hypothesis: $\mu_1>\mu$}.
We use the symbol $\mu_1$ for the hypothesised larger mean as it is the mean of a slightly different theoretical population, and $\mu$ is the symbol reserved for population means.

For several reasons, we cannot gather evidence that supports this hypothesis directly.
First, as a population mean $\mu_1$ is obviously not directly observable.
We can virtually never observe whole populations, all we've got are sample.
Rather, $\mu_1$ is a hypothesised mean in a population that exists as separate from the known population if the substantive hypothesis is correct.
However, if that population is not substantially different from the known one, then we have $\mu_1=\mu$.
While we certainly hope that \xmean\ is a good indicator of the true value $\mu_1$, we have no guarantees whatsoever that it actually is.
Second, as Fisherians we have no formal method of gathering positive evidence.%
\footnote{Only Neyman-Pearson philosophy and the Severity approach will give us this power (pun intended) later in Chapter~\ref{sec:powerseverity}, but with some important caveats.}
Therefore, we can only check \alert{whether the data \xmean\ are in accord with a Null $H_0: \mu_1\not{>}\mu$}, which is equivalent to $H_0: \mu_1\leq\mu$.

Let's recapitulate the frequentist logic of inference and adapt it to the case at hand:
To test this hypothesis, frequentism assigns a certain well-defined kind of probability to (obtaining) the data \xsample\ given that the Null is correct.
This probability can be used to assess whether the data \xsample\ are in accord with the Null.
First, what do we infer from the data \xsample\ (in other words, from our experiment) if they are compatible with the Null?
You're right, absolutely nothing!
If the data are in accord with the Null, we haven't found any evidence that it is not the case that $\mu_1$ is not greater than $\mu$, and that's the end of it.
If that sounds uselessly messed up and disappointing, that's because it is.%
\footnote{Whether you're a linguist or not, please consider that \textit{finding no evidence that A is true} is not the same as \textit{finding evidence that A is false}.}
If you infer anything from such a result, you're not only wrong, but also Bayesians (many of them with Dutch names) will come to haunt you (or at least unfollow you on the messaging platform of your choice)---and they'd be somewhat right to do so.
Second, what do we infer from the data \xsample\ if they are not compatible with the Null?
This is the much more interesting case, but it's difficult to define the admissible inferences without creating false ideas.
Let's say rather informally that in such a case we've found some evidence in support of the substantive hypothesis because it and the Null partition the range of possible values of $\mu_1$:
either it's greater than $\mu$ or it is not greater than (\ie, smaller than or equal to) $\mu$ ($H_0$).
Finding no accordance with the Null despite serious attempts to do so (see Chapter~\ref{sec:powerseverity}) provides at least some indication that the substantive hypothesis might be correct.
However, if you're looking for a \alert{proof} of anything, we recommend that you stick to pure theory, logic, theoretical maths, or pseudoscience.
There is no proof to be found in (non-trivial) experiments, and statistical inferences are weak and fragile.

\subsection{Extreme Means Under the Null}

We have argued that in Fisherian inference, we have to assess whether \xsample\ and its mean \xmean\ are compatible with the Null.
But how do we do this?
Because we're comparing means (\ie, numeric values) and not merely counting occurrences of correct and incorrect tea-first detections, it seems difficult to compute the number of all possible outcomes and the number of outcomes as extreme as or more extreme than the one we actually observed.
After all, we're dealing with numeric measurements (real values), and there's always a result in between two results, \eg, between $\xmean=123.99999$ and $\xmean=124$, there are infinitely many other possible results such as $\xmean=123.999991$, $\xmean=123.9999911$, etc.
Well, it's not totally impossible to calculate exact probabilities, but there's a convenient shortcut.
We'll go through it step by step.

The most naive but not at all wrong thing to do is to calculate the difference between the known population mean $\mu$ and the mean of the obtained sample \xmean.
In our case, this is $\xmean-\mu=\Sexpr{round(mean(z2.sample)-z.mu,2)}$.
Clearly, a minimal requirement for any further calculations is that this difference is positive.
If it were negative, the sample could hardly be interpreted as evidence against N: $\mu_1\leq\mu$.%
\footnote{This way of putting it is slightly sloppy and informal.
We will return to this notion and make it more precise in Chapter~\ref{sec:powerseverity}.
However, in practice it is blatantly obvious that we would never take an experiment that showed lower reaction times as evidence for higher reaction times, etc.}
While this is something one should always do first, it's not suitable for serious inference due to one main reason:
It doesn't take into account how large the sample was.

We now follow a very similar logic as in our introduction to Fisher's Exact Test (Chapter~\ref{sec:fisher}).
The question is:
How often would we expect to see a sample of $\Sexpr{z2.n}$ measurements with a sample mean of $\Sexpr{round(mean(z2.sample),2)}$ or larger if the true mean is that specified by the Null, which is $\mu=\Sexpr{z.mu}$?
Luckily, we have already introduced the tool that we need: the \Key{standard error} of the mean $\ssigma_{\mu}$.
The standard error for $n=\Sexpr{z2.n}$ and the known variance $\sigma=\Sexpr{z.sigma}$ (see p.~\pageref{page:varianceisfour}) tells us how strongly samples of this size deviate from the true mean (on average) in each direction.
The standard error of the mean in this case is (revise Equation~\ref{eq:semean} if necessary):

\begin{center}\begin{math}
  \ssigma_{\mu}=\frac{\Sexpr{z.sigma}}{\sqrt{\Sexpr{z2.n}}}=\sqrt{\frac{\Sexpr{z.sigma}^2}{\Sexpr{z2.n}}}=\Sexpr{z2.se.rnd}
\end{math}\end{center}

<<z3, fig.cap=paste0("Theoretical population distribution for a normal distribution with μ=", z.mu, " and σ=", z.sigma, " (green) the distribution of sample means for samples from this distribution with n=", z2.n, " (black)"), echo=F, fig.pos="t", fig.height=4, cache=F>>=

par(mar=c(3,2,2,2), family = "Plotfont")

z.minmax <- c(110, 130)

plot(x = seq(z.minmax[1], z.minmax[2], 0.1),
     y = z2.dnorm.sample(seq(z.minmax[1], z.minmax[2], 0.1)),
     type = "l", lwd = the.lwd,
     bty="n", yaxt="n", ylab = "", xlab = "",
     xlim = z.minmax, col = "white"
)
lines(x = seq(z.minmax[1], z.minmax[2], 0.1),
      y = z.dnorm(seq(z.minmax[1], z.minmax[2], 0.1)),
      col = the.cols[1], lwd = the.lwd)

lines(x = c(z.mu, z.mu), y = c(0, z.dnorm(z.mu)),
      lwd = the.lwd, col = the.cols[2])
lines(x = c(z.mu, z.mu), y = c(z.dnorm(z.mu), z2.dnorm.sample(z.mu)),
      lwd = the.lwd, col = the.cols[2])

lines(x = seq(z.minmax[1], z.minmax[2], 0.1),
     y = z2.dnorm.sample(seq(z.minmax[1], z.minmax[2], 0.1)),
     lwd = the.lwd, col = the.darkgray
)

text(x = z.mu+0.6, y = z.dnorm(z.mu)/2, labels = "μ", col = the.cols[2])

par.defaults()
@

Remember what the standard error is all about (Chapter~\ref{sec:confidence}).
If the mean in a population is $\mu$ and the standard deviation is $\sigma$, then the sample means of samples of size $n$ are themselves normally distributed, and the standard error $\ssigma_{\mu}$ is the standard deviation of that normal distribution.
Furthermore, keep in mind that we're talking about the distribution of sample means drawn from a known population.
Under the Null, it is also the distribution from which our small sample was drawn.
Figure~\ref{fig:z3} contrasts the density of the distribution of individual data points (in our example: individual reaction times) with the much narrower distribution of sample means.
Mathematically, it is narrower because the standard error is always smaller or equal to the standard deviation.
Intuitively, it should be narrower because on average sample means from samples with $n>1$ approximate the true mean better than single measurements.%
\footnote{Understanding this argument is crucial. If you're not following it, you should go back to Chapter~\ref{sec:confidence} for an introduction to the distribution of sample means, especially the argument concerning samples of increasing sizes in Sections~\ref{sec:samplinberries} and~\ref{sec:milli}.}

Remember from Chapter~\ref{sec:confidence} that a Normal Distribution is exhaustively defined by the parameters $\mu$ and $\sigma$.
Since (i)~the distribution of sample means is Normal, (ii)~we know its mean $\mu$, (iii)~we know its variance $\sigma^2$ and standard deviation $\sigma$, we can calculate how many samples (in the long run) drawn from the known population would have a mean of $\Sexpr{round(mean(z2.sample),2)}$ or larger.
In other words, we can calculate how many sample means (from samples of size $n=\Sexpr{z2.n}$) would deviate by $\Sexpr{round(mean(z2.sample)-z.mu,2)}$ from the population mean anyway due to expected sampling error.
Instead of calculating the numbers and probabilities of individual events as in the Tea-Tasting Experiment, we use the known functions of the Normal Distribution to look up those values.
This gives us a very precise and well-defined measure of how surprising the obtained result would be if the Null were true.

<<z4, fig.cap=paste0("Left: PDF of the distribution of sample means for mean µ and standard error SE with obtained sample mean $\\xmean$ and corresponding z-value; area under the curve for results equal to or greater than $\\xmean$ is shaded; Right: CDF for the same distribution with obtained sample mean $\\xmean$"), echo=F, fig.pos="t", cache=F, fig.height=4>>=

par(mfrow=c(1,2), mar=c(3,2,2,2), family = "Plotfont")

z.minmax <- c(114, 126)
xs <- seq(z.minmax[1], z.minmax[2], 0.1)
ys <- z2.dnorm.sample(seq(z.minmax[1], z.minmax[2], 0.1))

plot(x = xs,
     y = ys,
     type = "l", lwd = the.lwd,
     bty="n", yaxt="n", ylab = "", xlab = "",
     xlim = z.minmax, col = "white"
)

lines(x = c(z.mu, z.mu), y = c(0, z2.dnorm.sample(z.mu)),
      lwd = the.lwd, col = the.cols[2])

shade(xs, ys, z2.mu, z.minmax[2],
      col = the.lightcols[1], border=the.cols[1], lwd = the.lwd)

text(x = z.mu+3.25, y = z2.dnorm.sample(z.mu)/2.5, col = the.cols[2],
     labels = bquote(μ*"="*.(round(mean(z.mu), 2)))
     )
text(x = mean(z2.sample)+2.25, y = z2.dnorm.sample(mean(z2.sample)),
     labels = bquote(bar("x")*"="*.(round(mean(z2.sample), 2))),
     col = the.cols[1]
     )

lines(x = c(
  z.mu, mean(z2.sample)-0.25,
      y = 0.01+rep(z2.dnorm.sample(z2.mu), 2),
      lwd = the.lwd, col = the.cols[1]
      )
)

#text(x = mean(z2.sample)+1.5, y = z2.dnorm.sample(mean(z2.sample))+0.02,
#     labels=paste0("z=", z2.z.rnd),
#     col = the.cols[1]
#     )

lines(x = xs,
     y = ys,
     lwd = the.lwd, col = the.darkgray
)

# CDF

xs <- seq(z.minmax[1], z.minmax[2], 0.1)
ps <- pnorm(xs, mean = z.mu, sd = z2.se)

plot(xs,
     ps,
     type = "l", lwd = the.lwd,
     bty="n", ylab = "", xlab = "", xaxt = "n",
     xlim = c(114, 126), col = "white")

axis(1, seq(114, 126, 4), seq(114, 126, 4))

lines(x = c(z.mu, z.mu),
      y = c(0, 0.5),
      lwd = the.lwd, col = the.cols[2])
lines(x = c(z.mu, 110),
      y = rep(0.5, 2),
      lwd = the.lwd, col = the.lightcols[2])
text(114.5, 0.45, labels = 0.5, col = the.cols[2])
text(z.mu+0.5, 0.2, "μ", col = the.cols[2])

lines(x = c(z2.samplemean, z2.samplemean),
      y = c(0, ps[which.closest(xs, z2.samplemean)]),
      lwd = the.lwd, col = the.cols[1])
lines(x = c(z2.samplemean, 110),
      y = rep(ps[which.closest(xs, z2.samplemean)], 2),
      lwd = the.lwd, col = the.lightcols[1])
text(115, 0.93, labels = round(ps[which.closest(xs, z2.samplemean)], 2), col = the.cols[1])
text(z2.samplemean+0.5, 0.2, labels = bquote(bar(x)), col = the.cols[1])

lines(x = xs,
     ps,
     lwd = the.lwd, col = the.darkgray
)

par.defaults()

@

The left panel of Figure~\ref{fig:z4} show the PDF of the distribution of sample means, and the highlightes area under the curve corresponds to results as extreme or more extreme than \xmean.
The corresponding \Key{cumulative distribution function} (CDF) can be used to calculate the statistics of interest.
The right panel of Figure~\ref{fig:z4} shows the Normal CDF for $\mu=\Sexpr{z.mu}$ and $\sigma=\Sexpr{z2.se}$ (which happens to be $SE_{\mu}$).
For the observed sample mean $\xmean=\Sexpr{round(z2.samplemean,1)}$, the CDF has the value $\Sexpr{round(pnorm(z2.samplemean, z.mu, z2.se), 2)}$, which means that the probability of obtaining this or a more extreme result is $1-\Sexpr{round(pnorm(z2.samplemean, z.mu, z2.se), 2)}=\Sexpr{1-round(pnorm(z2.samplemean, z.mu, z2.se), 2)}$.
Think of the value $\Sexpr{round(pnorm(z2.samplemean, z.mu, z2.se), 2)}$ as indicating that $\Sexpr{round(pnorm(z2.samplemean, z.mu, z2.se), 2)*100}$\% of the probability mass lie to the right of $\xmean=\Sexpr{round(z2.samplemean,1)}$.
If you have access to a software that has built-in functions for such CDFs, this is the most straightforward way to arrive at a p-value for a sample mean and a known Normal Distribution.
In this case, you don't even need to calculate $z$ to perform a z-test.

Otherwise, you can calculate the so-called z-score and look up the p-value in a table.
To to this, we simply count the distance between the distribution mean and the sample mean in multiples of the standard error, which gives us the \Key{z-score}.
It's the same $z$ we introduced in Chapter~\ref{sec:confidence}, except we're using it the other way round.
For estimation, we looked up z-scores corresponding to probabilities, now we're calculating z-scores in order to lookup probabilities.
In this case, it's particularly easy because $SE_{\mu}=1$:

\begin{center}\begin{math}
z=\frac{\xmean-\mu}{SE_{\mu}}=\frac{\Sexpr{round(mean(z2.sample), 2)}-\Sexpr{z.mu}}{\sqrt{\frac{4^2}{16}}}=\frac{\Sexpr{round(mean(z2.sample)-z.mu, 2)}}{\Sexpr{z2.se.rnd}}=\Sexpr{z2.z.rnd}
\end{math}\end{center}

By dividing the distance by the standard error and calculating the z-score, we normalise it and make its interpretation independent of the concrete slope ($\sigma$) of the distribution.
The z-score is the distance from the mean in a Standard Normal Distribution (with $\mu=0$ and $\sigma=1$) that corresponds to the measured distance from the known mean.
Hence, $z=\Sexpr{z2.z.rnd}$ can be interpreted independently of the concrete sample mean and population mean, and it establishes a direct link to the probability we're looking for: another \Key{p-value}.
We can use tables to look up p-values corresponding to z-scores, and for $z=2.1$, we get $Pr=0.02$, which in our application is the p-value $p=0.02$.
Table~\ref{tab:ptable} shows such a table, and it's the reverse of tables such as Table~\ref{tab:ztable}.%
By the way, the values are those for the Standard Normal Distribution with $\mu=0$ and $\sigma=1$.
Do you see why?
We give two columns, one for the single-sided test and one for the two-sided test (see p.~\ref{abs:sided}, immediately below, and Section~\ref{sec:diffciz}).

<<ptable, echo=F, cache=T, results="asis">>=
  zs <- seq(1.5, 2.5, 0.1)
  ps <- round(pnorm(zs, lower.tail = F), 2)
  pstt <- round(2*pnorm(zs, lower.tail = F), 2)
  ztable <- data.frame(Z=zs, "Pr1" = ps, "Pr2" = pstt)

  ztable %>%
    as.data.frame.matrix %>%
    kable(align = c("c", "c"), booktabs = T, linesep = "",
          caption = "Probabilities for Some Z-Values",
          label = "ptable",
          col.names = c("Z", "Pr (one-sided)", "Pr (two-sided)")) %>%
    row_spec(0, bold=TRUE)
@

Clearly, either the Null is false or the Null is true and a relatively rare event has occurred.
Whether a chance of 1 in 50 (equivalent to $p=0.02$) is rare or unexpected enough to make an inference can only be decided by you based on your knowledge of the field you're working in.
How precise are your measurements?
What magnitude does your theory predict the difference should be?%
\footnote{It doesn't make numeric predictions?
Blimey! Then back to the drawing board.}
How does the measured difference in reading times compare to differences observed for similar processing penalties?
In corpus linguistics, we fail to see how $p=0.02$ would be surprising enough in any situation to warrant substantive inferences.

One final word on the \Key{one-sided test} and the \Key{two-sided test}, also called single-tailed and two-tailed.
In the example in this section, the substantive hypothesis was $\mu_1>\mu$, and the Null was consequently $H_0: \mu_1\leq\mu$.
We were interested in the probability of obtaining a mean as high as $\Sexpr{z2.z.rnd}$ or higher if $\mu_1\leq\mu$, and hence values that deviate negatively (to the left of) $\mu=\Sexpr{z.mu}$ wouldn't be surprising at all and aren't counted.
There are two other possible scenarios we could have wished to test.
Our substantive hypothesis could have been $\mu_1<\mu$.
Alternatively, it could have been $\mu_1\not =\mu$.
The corresponding Nulls are $H_0: \mu_1\geq\mu$ and $H_0: \mu_1=\mu$.

<<tails, fig.cap=paste0("Single-sided and two-sided tests"), echo=F, fig.pos="t", fig.height=4, cache=F>>=

par(mfrow=c(1,3), mar=c(2,0.5,1.5,0.5), family = "Plotfont")

xs <- seq(-4, 4, 0.01)
ys <- dnorm(xs)
a.cex <- 1.4

plot(x = xs,
     y = ys,
     type = "l", lwd = the.lean.lwd,
     bty="n", yaxt="n", ylab = "", xlab = "",
     xlim = c(-4, 4), col = "white",
     main="Null: μ₁≤μ"
)

lines(c(0, 0), c(0, ys[round(length(xs)/2+0.1, 0)]),
      lwd = the.lean.lwd, col = the.cols[2])

shade(xs, ys, z2.z.rnd, 4,
      col = the.lightcols[1], border=the.cols[1], lwd = the.lean.lwd)

arrows(0.2, 0.025, z2.z.rnd-0.2, 0.025, length = 0.05,
       lwd = the.lean.lwd, col = the.cols[3])
text(z2.z.rnd/2, 0.05, paste0("z=", z2.z.rnd), col = the.cols[3], cex=a.cex)
text(z2.z.rnd+1.2, 0.05, paste0("p=0.02"), col = the.cols[1], cex=a.cex)

lines(x = xs,
     y = ys,
     lwd = the.lean.lwd, col = the.darkgray
)

plot(x = xs,
     y = ys,
     type = "l", lwd = the.lean.lwd,
     bty="n", yaxt="n", ylab = "", xlab = "",
     xlim = c(-4, 4), col = "white",
     main="Null: μ₁≥μ"
)

lines(c(0, 0), c(0, ys[round(length(xs)/2+0.1, 0)]),
      lwd = the.lean.lwd, col = the.cols[2])

shade(xs, ys, -4, -z2.z.rnd,
      col = the.lightcols[1], border=the.cols[1], lwd = the.lean.lwd)

arrows(-0.2, 0.025, -(z2.z.rnd-0.2), 0.025, length = 0.05,
       lwd = the.lean.lwd, col = the.cols[3])
text(-(z2.z.rnd/2), 0.05, paste0("z=-", z2.z.rnd), col = the.cols[3], cex=a.cex)
text(-(z2.z.rnd+1.2), 0.05, paste0("p=0.02"), col = the.cols[1], cex=a.cex)

lines(x = xs,
     y = ys,
     lwd = the.lean.lwd, col = the.darkgray
)


plot(x = xs,
     y = ys,
     type = "l", lwd = the.lean.lwd,
     bty="n", yaxt="n", ylab = "", xlab = "",
     xlim = c(-4, 4), col = "white",
     main="Null: μ₁=μ"
)
lines(c(0, 0), c(0, ys[round(length(xs)/2+0.1, 0)]),
      lwd = the.lean.lwd, col = the.cols[2])

shade(xs, ys, z2.z.rnd, 4,
      col = the.lightcols[1], border=the.cols[1], lwd = the.lean.lwd)
arrows(0.2, 0.025, z2.z.rnd-0.2, 0.025, length = 0.05,
       lwd = the.lean.lwd, col = the.cols[3])
text(z2.z.rnd/2, 0.05, paste0("z=", z2.z.rnd), col = the.cols[3], cex=a.cex)
text(z2.z.rnd+1.2, 0.05, bquote(frac("p", "2")*" =0.02"), col = the.cols[1], cex=a.cex)

shade(xs, ys, -4, -z2.z.rnd,
      col = the.lightcols[1], border=the.cols[1], lwd = the.lean.lwd)
arrows(-0.2, 0.025, -(z2.z.rnd-0.2), 0.025, length = 0.05,
       lwd = the.lean.lwd, col = the.cols[3])
text(-(z2.z.rnd/2), 0.05, paste0("z=-", z2.z.rnd), col = the.cols[3], cex=a.cex)
text(-(z2.z.rnd+1.2), 0.05, bquote(frac("p", "2")*" =0.02"), col = the.cols[1], cex=a.cex)

lines(x = xs,
     y = ys,
     lwd = the.lean.lwd, col = the.darkgray
)

par.defaults()
@

Each of these cases corresponds to a slightly different test, illustrated in Figure~\ref{fig:tails} for the Standard Normal Distribution as the distribution of sample means, but with the z-value from the example above\ie, $z=\Sexpr{z2.z.rnd}$.
On the left, the test as conducted above is repeated.
We deviate by $z=\Sexpr{z2.z.rnd}$ from the known mean $\mu$, and that defines 2\% of the area under the curve, hence $p=0.02$.
In the middle, the opposite is shown, which is applicable if the Null is: \textit{the unknown mean $\mu_1$ is equal to or larger than the known mean $\mu$}.
In this case, we're only interested in negative deviations from the known mean $\mu$.
The probability corresponding to a negative z-statistic in the one-sided case is the same as the one corresponding to a positive z-statistic, hence $p=0.02$ according to Table~\ref{tab:ptable}.
Finally, if your hypothesis is an \Key{undirected hypothesis} and we only suspect the sample mean \xmean\ represents a mean $\mu_1$ that is different from $\mu$, we have to take into account both tails of the distribution, which simply doubles the p-value to $p=0.04$.
The right panel in Figure~\ref{fig:tails} clearly shows that values as extreme of more extreme than $z=\Sexpr{z2.z.rnd}$ in either direction of $\mu$ correspond to the double area under the curve.

If you think this is trivial, you're lucky.
However, don't be too hard on yourself if you find this difficult to fiddle apart and to memorise.
This is completely normal, and it has nothing to do with you or frequentist statistics.
Fort some reason, many human brains have difficulties of keeping track of positive and negative signs of z-statistics, greater-than and smaller-than relation, while having to flip them around in formulating and evaluating hypotheses.
Just give it time.

\subsection{The Difference Between Error Intervals and the Z-Test}\label{sec:diffciz}

The z-test allows us to check whether a mean measured in an experiment was an unexpected outcome under the Null.
We attach a specific statistic---the p-value---to the outcome.
It's a matter of debate whether the statistic itself should be interpreted directly numerically.
However, you've probably heard of people using the p-value to make decisions when it's smaller than a certain threshold called $\alpha$-level or---as we prefer---$sig$-level
They call results \textit{significant} when they reach a certain level such as $sig=0.05$.
When $p<sig$, \textit{we've reached significance}.
However, when $p=0.009$ in the next experiment, suddenly that result might be called \textit{even more significant}.
While we consider it vital to report the actual p-value and not just $p<sig$, we advise against cascaded conceptions of significance.
Whether an outcome would be unexpected under the Null depends on the phenomenon at hand, the precision of your theory and of your measurement, whether you've ensured that the pre-conditions for the test were met properly, and probably many other factors.
Nobody can tell you or your reseqarch community what your threshold needs to be.
For example, you might have seen reports from nuclear physics (such as results from experiments with the Large Hadron Collider of CERN) where it is claimed that $5\sigma$ was reached.
That's just 5 standard deviations from the expected value under the Null ($z=5$), which corresponds to approximately $p=0.00000029$.
In physics, this is not interpreted as a test but rather as a requirement for measurements to be taken seriously at all, to be accepted as measurements of something.
That's because there is large uncertainty in the measurements, and the phenomena of interest are tiny beyond microscopic and can only observed very indirectly.
Do you need that level of unexpectedness?
Think about it.
If you're uncertain and your field has just gotten started (such as linguistics, cognitive science, or social psychology), we recommend you don't dare to be any more lenient than $sig=0.001$.
Honestly, $sig=0.05$ is borderline ridiculous!

In any case, if you reach some sig-level under a testing approach, you might proceed to an inference, a decision of some sort.
Usually, it should be something like this:

\begin{quote} \itshape
  We're not going to discard our substantive hypothesis just yet, but we'll submit it to further error probing.
\end{quote}

\noindent Do you see how this is different from the following statement?

\begin{quote} \itshape
  \Argh We've statistically proven that our substantive hypothesis is correct with a probability of 95\%.
\end{quote}

\noindent If you don't see this, go back to Chapter~\ref{sec:fisher} and start all over again.

While tests and error intervals are related mathematically (see below), they're entirely different in their interpretation.
You can estimate a value without having any hypothesis at all.
For example, if you just want to find out the average rate at which a manufacturing machine produces defective parts, you might take a sample of 1000 parts, count the proportion of defective parts, calculate a safe error interval (at, say, $a=0.99$), and see whether the resulting interval looks acceptable from an economic point of view.
If you do this with all your machines over many years and decades, your estimate will included the true value in close to 99\% of all times.
Depending on the value of the individual parts that you're working with in your business, this might be acceptable.
In Chapter~\ref{sec:powerseverity}, we'll return to this example from toy manufacturing and move it closer to a testing framework.

<<convzci, fig.cap=paste0("Convergence of z-test and error interval: two-sided test"), echo=F, fig.pos="t", cache=F, fig.height=5>>=

par.defaults()
par(family = "Plotfont")

xs <- seq(-4, 4, 0.01)
ys <- dnorm(xs)

plot(x = xs,
     y = ys,
     type = "l", lwd = the.lwd,
     bty="n", yaxt="n", ylab = "", xlab = "",
     xlim = c(-4, 4), col = "white"
)

shade(xs, ys, -14 , -1.96,
      col = the.lightcols[1], border=the.cols[1], lwd = the.lwd)
shade(xs, ys, 1.96 , 4,
      col = the.lightcols[1], border=the.cols[1], lwd = the.lwd)

points(1.96, 0.1,
      col = the.cols[2], pch = 20, cex = 2)
lines(c(0, 2*1.96), rep(0.1, 2),
      col = the.cols[2], lwd = the.lwd)
lines(rep(0, 2),  c(0.11, 0.09),
      col = the.cols[2], lwd = the.lwd)
lines(rep(2*1.96, 2),  c(0.11, 0.09),
      col = the.cols[2], lwd = the.lwd)

lines(x = xs,
     y = ys,
     lwd = the.lwd, col = the.darkgray
)
@

Their interpretations differ, but frequentist error intervals and frequentist tests are related mathematically.
To illustrate, let's use an even simpler example where the sampling distribution is the Standard Normal Distribution.
Hence, $\mu=0$, $\sigma=\ssigma_{\mu}=1$, and we assume that the measured mean was $\xmean=1.96$.
Since the standard error is 1, all calculations are quite simple.
First, for the test, $z=1.96\div 1= 1.96$.
This gives almost exactly $p=0.05$ for the two-sided test, \ie, the test which merely checks whether the measured mean lies in the outermost 5\% of the probability mass.
In Figure~\ref{fig:convzci}, the shaded areas correspond to those five percent, and they begin at a distance of $1.96$ to both sides of the mean.
If we calculate an error interval with $a=0.95$ around the measurement of $\xmean=1.96$, it is exactly $1.96$ wide to both sides of the measured mean.
Hence, it includes $0$, as you can see in Figure~\ref{fig:convzci}, where the error interval is plotted in blue.

<<convzciss, fig.cap=paste0("Convergence of z-test and error interval: single-sided test"), echo=F, fig.pos="t", cache=F, fig.height=5>>=

par.defaults()
par(family = "Plotfont")

xs <- seq(-4, 4, 0.01)
ys <- dnorm(xs)

plot(x = xs,
     y = ys,
     type = "l", lwd = the.lwd,
     bty="n", yaxt="n", ylab = "", xlab = "",
     xlim = c(-4, 4), col = "white"
)

shade(xs, ys, 1.645, 4,
      col = the.lightcols[1], border=the.cols[1], lwd = the.lwd)

points(1.645, 0.15,
      col = the.cols[2], pch = 20, cex = 2)
lines(c(0, 1.645), rep(0.15, 2),
      col = the.cols[2], lwd = the.lwd)
lines(rep(0, 2),  c(0.14, 0.16),
      col = the.cols[2], lwd = the.lwd)

lines(x = xs,
     y = ys,
     lwd = the.lwd, col = the.darkgray
)
@

The two-sided z-test at some $sig$-level (in this case $sig=0.05$) is equivalent to checking whether the error interval for $a=1-sig$ contains the population mean $\mu$.
This extends to a single-sided test and a single-sided error interval, which only extends to one side from the measured mean.
See Figure~\ref{fig:convzciss}, where we assume $\mu=0$, $\sigma=\ssigma_{\mu}=1$, and $\xmean=1.65$.
The error interval is left-sided, and it corresponds to the right-sided test.
As $\xmean=1.65$ was chosen as the z-value that reaches $sig=0.05$ for the single-sided z-test, the interval again includes $0$.

\subsection{The Distribution of P-Values}\label{sec:distpz}

<<setupzsim, cache=T, echo=FALSE, include=FALSE, results='asis'>>=
  set.seed(848)
  z.sim.rep <- 1000
  z.sim.n <- 100
  z.sim.mu <- 120
  z.sim.sigma <- 16
  z.sim.se <- z.sim.sigma/sqrt(z.sim.n)

  z.sim <- function(m) {
      .m <- mean(rnorm(n = z.sim.n, mean = m, sd = z.sim.sigma))
      .z <- (.m-z.sim.mu)/z.sim.se
      pnorm(abs(.z), lower.tail = F)*2
  }
@

<<simznull, fig.cap=paste0("Histograms of p-values from z-tests when the Null is true; it's irrelevant what the parameters were, as any p-values have a uniformly random distribution under any true Null"), echo=F, fig.pos="t", cache=T>>=
  par.defaults()
  z.sims.null <- unlist(unname(lapply(1:z.sim.rep,
                                      function(x) {z.sim(z.sim.mu)} )))

  h <- hist(z.sims.null, breaks = 20, xaxt = "n", xlab = "P-Values", main ="",
            border=F, col = the.midgray, ylim = c(0,70))
  text(h[["mids"]], h[["counts"]]-5, labels = h[["counts"]])
  axis(1, seq(0, 2, 0.2), seq(0, 1, 0.1))
@

In Section~\ref{sec:pdist} we showed how simulated p-values of Fisher's Exact Test are distributed when the Null is true and when it it's false.
In this section, we'll do the same for the z-test.
First, let's assume the Null is true.
We simulate a two-sided test with $H_0:\mu_1=\mu$.
The measurements are normally distributed with $\mu=\Sexpr{z.sim.mu}$ and $\sigma=\Sexpr{z.sim.sigma}$.
We simulate $n=\Sexpr{z.sim.n}$ data points per sample in $\Sexpr{z.sim.rep}$ replications.
The standard error of the mean is $\ssigma_{\mu}=\Sexpr{z.sim.sigma}\div\sqrt{\Sexpr{z.sim.n}}=\Sexpr{z.sim.se}$.
Figure~\ref{fig:simznull} shows that each p-value has (minor fluctuations aside) the same probability if the Null is true.
If the mean within the population from which our samples are drawn is $\mu_1=\mu=120$, any p-value between 0 and 1 has an equal chance of resulting from the experiment.

<<simzh1, fig.cap=paste0("Histograms of p-values from z-tests when the Null is false with different effect strengths"), echo=F, fig.pos="t", cache=T>>=
  par.defaults()
  par(mfrow=c(3,2))

  z.sims.plotone <- function(m) {
    .z.simses <- unlist(unname(lapply(1:z.sim.rep, function(x) {z.sim(m)} )))
    h <- hist(.z.simses, breaks = 20, xaxt = "n", xlab = "P-Values",
              border=F, col = the.midgray,
              ylim=c(0, ifelse(mean(.z.simses)<0.1, 1000, 110)),
              main = paste0("μ₁=", m))
    #text(h[["mids"]], h[["counts"]]-5, labels = h[["counts"]])
    axis(1, seq(0, 1, 0.1), seq(0, 1, 0.1))
  }

  z.sim.truemeans <- 120+c(-1, 1, -5, 5, -10, 10)
  for (m in z.sim.truemeans) z.sims.plotone(m)
@

We then simulated the same for $\mu_1\not=\mu$.
Clearly, there are many ways to satisfy $\mu_1\not=\mu$.
The difference between the two means could be $-0.1$, $+29$, or what have you.
All these different possible concrete numerical inequalities between $\mu_1$ and $\mu$ make the Null false.
The larger the difference, the larger the effect strength.
For example, if reading times under a marked condition differ from those in the unmarked condition by 200~ms, the effect is much larger than if they differ by only 2~ms.
We simulated \Sexpr{z.sim.rep} samples each for the true means $\mu_1\in\{\Sexpr{paste0(z.sim.truemeans, collapse = ", ")}\}$.
Figure~\ref{fig:simzh1} shows histograms of the results.
Even if the difference between the means is only 5 ($\mu_1=115$ or $\mu=125$), the p-values already collapse close to 0 (middle left and middle right panel).
Once again (remember Section~\ref{sec:pdist}), larger effect strengths make it easier to detect the effect by rejecting the Null.
As in the case of Fisher's Exact Test, a large sample alone does not increase the chance of obtaining a low p-value.
However, as soon as there is even a small true effect, and we draw a relatively large sample, p-values go down rapidly.
If the effect is tiny and irrelevant from a theoretical point of view, we can detect it easily with a large sample, it's our duty to have a good enough understanding of the statistical methods we use to be able to interpret that.
This behaviour of the test itself is, of course, by design.
Frequentist statistics per se is not responsible for misinterpretations and misuse by lazy or corrupt practitioners.

\section{The Undiscovered Population}

\subsection{Accounting for Unknown Variance}

The z-test allowed us to test whether some mean $\mu_1$ is different (smaller, larger, or any of the two) from a known population mean if the population variance is unknown.
Some readers might have wondered when on Earth that is actually the case.
Since the t-test doesn't account for any uncertainties in the knwon mean and variance, they must be known beyond any need for further testing.
This is where the \Key{t-test} comes into play.
It allows us to test for mean differences if the variance is unknown (this section) or both means and the variance are unknown (Section~\ref{sec:ttwo}).

The logic of the test is perfectly identical to the z-test.
We simply substitute the sample variance $s^2$ for the population variance $\sigma^2$ and go on with the calculations but call the test statistic $t$ instead of $z$ (hence \textit{t-test}).
To remind us all of Equation~\ref{eq:variance}:

\begin{equation}
  s^2(\xsample)=\cfrac{\sum\limits_{i=1}^{n}(x_i-\bar{\xsample})^2}{n-1}=\cfrac{SQ(\xsample)}{n-1}
\end{equation}

Do you see why this can't be all?
While the sample variance might be the best approximation of the population variance we have, it's not going to be a perfect approximation in most (virtually all) cases.
There might be cases where $s^2$ \textit{overestimates} $\sigma$.
Imagine what this would do to our statistic $t$, which is calculated exactly the same way as $z$:

\begin{equation}
  t=\frac{\xmean-\mu}{SE_{\mu}}
  \label{eq:tone}
\end{equation}

\noindent Except $\ssigma_{\mu}$ is calculated from the sample variance:

\begin{center}
  \begin{math}
    \ssigma_{\mu}=\sqrt{\frac{s^2}{n}}
  \end{math}
\end{center}

With increasing $s^2$, \ssigma\ increases, and $t$ decreases as the denominator of its formula is \ssigma.
However, smaller values of the $z$ or $t$ statistics leads to larger p-values.
Look at Figure~\ref{fig:z4} to convince yourself that this is true.
Whichever criterion we adopt to reject the Null based on an obtained p-value, we'd reject less Nulls.%
\footnote{Once again, we encourage you to go through this argument step by step multiple times until you're satisfied that you've understood and memorised the argument.}
Since we attach no inferential interpretation to a rejected Null, rejecting more Nulls than we should because the sample variance overestimates the population variance is not a big deal.%
\footnote{If you find this a very unsatisfying claim, read on until Chapter~\ref{sec:powerseverity}.}

If, on the other hand, the sample variance $s^2$ underestimates the population variance $\sigma^2$, we might end up rejecting less Nulls than we should.
Convince yourself that this is true by going through the formulas above.
This is potentially harmful as rejections of a Null have a limited inferential interpretation:
\textit{Either the Null is false or a rare event has occurred.}
To compensate for this, a statistician by the name of William Sealy Gosset, who called himself \textit{Student}, came up with a distribution similar to the Standard Normal Distribution: \Key{Student's t-Distribution}.
It accounts for the fact that in the long run a larger sample provides a better estimate of the population variance than a smaller sample.
The distribution has only one parameter: the sample size $n$, or rather the \Key{degrees of freedom} $\nu$ with $\nu=n-1$.
We'll return to the idea of degrees of freedom repeatedly.
For now, we just contemplate why the t-Distribution has this one parameter and don't worry too much about its name.

<<t1, fig.cap=paste0("Densities of the Standard Normal Distribution and a selection of t-Distributions for increasing degrees of freedom ν"), fig.height=5, echo=F, fig.pos="t", cache=F>>=

par.defaults()

t.minmax <- c(-4, 4)

t.dfs <- c(1, 2, 5, 10, 20)
xs <- seq(t.minmax[1], t.minmax[2], 0.01)

plot(x = xs,
     y = dnorm(xs),
     type = "l", lwd = the.lean.lwd,
     bty="n", yaxt="n", ylab = "Density", xlab = "z or t",
     xlim = t.minmax, col = the.darkgray
)

for (i in 1:length(t.dfs)) {
  .df <- t.dfs[i]
  lines(x = xs,
       y = dt(xs, df = .df),
       lwd = the.lean.lwd, col = the.cols[i]
  )
}

legend("topleft", legend = c("Normal", rev(paste0("t with ν=", t.dfs))),
       col = c(the.darkgray, rev(the.cols[1:length(t.dfs)])), lwd = the.lwd, bty = "n")
@

First, the t-Distribution corresponds to the \textit{Standard} Normal Distribution, not the more general Normal Distribution.
Hence, its variance is fixed at $1$ and its mean is fixed at $0$.
At the same time, the effect of the expected mismatch described above between the sample variance and the true population variance is small when the sample is large and vice versa.
A larger sample approximates any population parameter more accurately than a smaller sample.
Now, see Figure~\ref{fig:t1} to grasp the effect of the $\nu$ parameter.
A lower $\nu$, which corresponds to a lower sample size $n$, flattens the peak of the curve around the mean while lifting both tails up.
However, even at $\nu=20$, the density of the t-Distribution is already almost identical to that of the Standard Normal Distribution.

<<t2, fig.cap=paste0("How the p-value from the t-Distribution at t=1.96 (two-sided) approaches the p-value from the Standard Normal Distribution at z=1.96 (two-sided) with increasing degrees of freedom ν"), fig.height=5, echo=F, fig.pos="t", cache=F>>=

par.defaults()

ts <- 2*pt(1.96, df = 1:100, lower.tail = F)

plot(ts,
     type = "l", lwd = the.lwd,
     bty="n", ylab = "p-value", xlab = "Degrees of freedom ν",
     col = "white"
)
lines(c(0, 100), rep(0.05, 2), lwd = the.lwd, col = the.cols[1])
lines(ts, lwd = the.lwd, col = the.cols[2])

legend("topright", legend = c("p-value from Standard Normal", "p-value from t (with increasing ν)"),
       title = "For z=1.96 and t=1.96, respectively:",
       col = the.cols[1:2], lwd = the.lwd, bty = "n")
@

Figure~\ref{fig:t2} shows how two-sided p-values from the t-Distribution for a fixed t-value of 1.96 but with increasing sample size and thus increasing degrees of freedom.
The t-value of 1.96 was chosen because we know that it corresponds to $p=0.05$ in a z-Test with a Standard Normal Distribution.
The Normal Distribution has no parameter for degrees of freedom, and the p-value is therefore always $p=0.05$ for $z=1.96$.
With ever larger samples, the p-value from a t-test approximates that from a z-test very quickly.
With a slight oversimplification, we can say that the t-test makes it harder to reach any standard for an unexpected result compared to the z-test, thus accounting for the added uncertainty from the variance estimated from the sample.
With a larger sample (higher degrees of freedom), that added uncertainty disappears more and more as the estimate becomes more and more accurate.

<<tsim, fig.cap="Simulations of z-Tests and t-Tests with variances estimated from samples; the z-Tests are therefore inappropriate", fig.height=5, echo=F, fig.pos="t", cache=F>>=

par.defaults()
par(mfrow=c(2, 2), mar=c(4.5,4,2,0.5))

tsim.reps <- 10000
tsim.t.ps <- rep(0, tsim.reps)
tsim.z.ps <- rep(0, tsim.reps)

tsim.1 <- 0.5
tsim.sig <- 0.01
tsim.ns <- c(5, 10)
tsim.props <- rep(0, 8)

for (n in 1:2) {
  tsim.n <- tsim.ns[n]
  for (u in 0:1) {
    for (i in 1:tsim.reps) {
      if (u == 0) {
        .xs <- rnorm(n = tsim.n)
      } else {
        .xs <- rnorm(n = tsim.n, mean = tsim.1)
      }

      .mean <- mean(.xs)
      .var <- var(.xs)
      .sd <- sqrt(.var)
      .se.hat <- sqrt(.var/tsim.n)

      # z-test and t.test
      .t <- .mean/.se.hat

      .z.p <- 2*pnorm(abs(.t), lower.tail = F)
      .t.p <- 2*pt(abs(.t), tsim.n-1, lower.tail = F)

      tsim.z.ps[i] <- .z.p
      tsim.t.ps[i] <- .t.p
    }

    .z.prop <- round(length(which(tsim.z.ps < tsim.sig))/tsim.reps*100, 1)
    .t.prop <- round(length(which(tsim.t.ps < tsim.sig))/tsim.reps*100, 1)

    # For later in the text.
    tsim.props[ifelse(n==1, 1, 5)+u*2] <- .z.prop
    tsim.props[ifelse(n==1, 1, 5)+u*2+1] <- .t.prop

    .main <- paste0(
      ifelse(u==0, "H₀ is true", paste0("H₀ is false (μ₁=", tsim.1, ")")),
      ", n=", tsim.n
      )

    plot(density(tsim.z.ps, from = 0, to = 1),
         main = .main,
         type = "l", lwd = the.lwd,
         bty="n",
         xlab = ifelse(tsim.n==10, "p-value", ""),
         ylab = ifelse(u==0, "Density", ""), yaxt = "n",
         col = "white"
    )
    lines(density(tsim.z.ps, from = 0, to = 1),
          col = the.cols[3], lwd = the.lwd)
    lines(density(tsim.t.ps, from = 0, to = 1),
          col = the.cols[1], lwd = the.lwd)
    legend(ifelse(u==0, "bottom", "topright"),
           legend = c(paste0("z-Test with p<", tsim.sig, ": ", .z.prop, "%"),
                      paste0("t-Test with p<", tsim.sig, ": ", .t.prop, "%")),
           col = the.cols[c(3,1)], lwd = the.lwd, bty = "n"
    )
  }
}
@

Finally, we simulate t-tests and compare them to incorrectly conducted z-tests.
The simulation answers the question of what happens if we disregard the fact that the variance was merely estimated from a sample and was not known, and we decide to perform a z-Test.
For simplicity's sake, let's say the known mean is $\mu=0$, and the true but unknown standard deviation is $\sigma=1$.
Figure~\ref{fig:tsim} shows the distributions of p-values for inappropriate z-Tests based on the sample variance and the corresponding appropriate t-Tests.
The only difference is that for the z-Test, the cumulative distribution function of the Standard Normal Distribution was used, and for the t-Test, the cumulative distribution function of the t-Distribution (with the respective degrees of freedom) was used.
The z-value and the t-value are numerically identical in this case, and they both use the standard error calculated from the sample variance, which is, again, only correct for the t-Test:

\begin{center}
  \begin{math}
    \ssigma_{\xsample}=\sqrt{\frac{s^2}{n}}
  \end{math}
\end{center}

Each curve in Figure~\ref{fig:tsim} plots the estimated density of p-values from \Sexpr{tsim.reps} random experiments with the accompanying two-sided tests.
In the two left panels, the Null is true, and the data are generated from a Standard Normal Distribution.
Hence, the the z-values and the t-values should be centred around 0, and all p-values should have the same long-run frequency, which is always assumed under the Null.
We see that for $n=\Sexpr{tsim.ns[1]}$, the z-Test produces incorrect results if the variance is not known but merely estimated.
Under the Null (upper left panel) and with $n=\Sexpr{tsim.ns[1]}$ ($\nu=\Sexpr{tsim.ns[1]-1}$), the t-Tests correctly land at $p<\Sexpr{tsim.sig}$ in \Sexpr{tsim.props[2]}\% of all simulations, but the z-Tests produce many false significant results, namely in \Sexpr{tsim.props[1]}\% of the simulations.%
\footnote{We arbitrarily chose $sig=\Sexpr{tsim.sig}$ for convenience.
Results would be similar for other sig-levels.}

With more degrees of freedom, this problem is alleviated as the variance estimate gets more precise.
In the lower-left panel, the simulations for $n=\Sexpr{tsim.ns[2]}$ ($\nu=\Sexpr{tsim.ns[2]-1}$) are shown.
The t-Test is still almost spot-on and reaches $p<\Sexpr{tsim.sig}$ in \Sexpr{tsim.props[6]}\% of all simulations.
The z-Tests now produce \Sexpr{tsim.props[5]}\% reaching the threshold, which means at least many fewer false significant results.
Still, this would lead to false rejections of the Null and false positive inferences regarding the substantive hypothesis much more often than it should.
The error rate increases sharply, as it should be 1\% at $sig=\Sexpr{tsim.sig}$, 0.1\% at $sig=0.001$, etc.

In the panels on the right-hand side, p-values from simulations are shown where the Null is false.
The samples now come from a DGP with $\mu_1=\Sexpr{tsim.1}$.
We haven't discussed yet how this effect strength affects the tilt of the p-values numerically.
Therefore, we have no precise expectation of how many simulations we expect to achieve $p<\Sexpr{tsim.sig}$.
However, we clearly see that the inappropriate z-Tests are shifted much more heavily towards 0.
Under the assumption (corroborated in the simulations with a true Null) that the t-Test is appropriate and the z-Test isn't under the given circumstances, the z-Test will lead to many more wrong inferences than we expect from correct z-Tests.
In sum, using the z-Test where a t-Test would be appropriate leads to a considerable and incorrect increase in rejections of the Null.
As this would mar our inferences, the t-Test is a highly useful tool when the true variance is unknown.
But, we hear you say, population means are also largely unknown, which makes the t-Test much less useful than we claim.
Hold that thought!
The solution is presented in the next section.

\subsection{Accounting for Two Unknown Means}\label{sec:ttwo}






\begin{exercises}

\exercise{z1} \xsample\ as given below is a series of numeric measurements.
The known population variance is $\sigma=$.
Under the substantive hypothesis that the mean is smaller than the know population mean $\mu=$, formulate a Null and perform a z-Test.
All required statistics should be calculated by hand with a pocket calculator.
Interpret the result, and make an inference if possible.

\exercise{z2} \xsample\ as given below is a series of numeric measurements.
The known population variance is $\sigma=$.
Under the substantive hypothesis that the mean is greater than the know population mean $\mu=$, formulate a Null and perform a z-Test.
All required statistics should be calculated by hand with a pocket calculator.
Interpret the result, and make an inference if possible.

\exercise{z3} \xsample\ as given below is a series of numeric measurements.
The known population variance is $\sigma=$.
Under the substantive hypothesis that the mean is different from the know population mean $\mu=$, formulate a Null and perform a z-Test.
All required statistics should be calculated by hand with a pocket calculator.
Interpret the result, and make an inference if possible.

\exercise{t1} Assuming the population variance is unknown, perform a t-Test corresponding to Exercise~\ref{exer:z3}.
Interpret the differences in the results between the z-Test and the t-Test.

\exercise{t2} For the samples \xsample\ and \ysample\ given below, perform a t-Test for differences in sample means under the substantive hypothesis that \ymean\ is smaller than \xmean.
Formulate a Null and perform the test.
All required statistics should be calculated by hand with a pocket calculator.
Interpret the result, and make an inference if possible.

\exercise{t3} For the samples \xsample\ and \ysample\ given below, perform a t-Test for differences in sample means under the substantive hypothesis that \xmean\ and \ymean\ are not equal.
Formulate a Null and perform the test.
All required statistics should be calculated by hand with a pocket calculator.
Interpret the result, and make an inference if possible.

\exercise{zandt10} We have argued in Section~\ref{sec:distpz} that larger effect sizes (increasing actual differences between the known population mean and the measured means) tilt the distribution of p-values increasingly towards 0.
There is a tradition where researchers don't look at the concrete p-value but just perform a mechanical significance test at a pre-set sig-level such as $sig=0.05$.
This means that tests where $p\leq sig$ are called \textit{significant}, all others \textit{not significant}.
Under such an approach, do you get in the long run: (i)~more significant results, (ii)~less significant results, or (iii)~a constant number of significant results when the actual effect strength increases (all other things being equal)?
Still under this approach, is your \textit{error rate} constant or does it change with increased effect strength?
The error rate is the percentage of samples which make you erroneously reject the Null when it is true.

\exercise{zandt20} A fictitious big data study in sociolinguistics has found through a popular social media app (and only from users that consented to be part of the experiment) that the mean time it took self-identified men to read a specific message about tomato ketchup was \ldots\ seconds ($n_x=45231$, $s_x=$), whereas the mean reading time for self-identified women was \ldots\ seconds ($n_y=21231$, $s_y=$).
The theory-driven substantive hypothesis was that men should read the message faster.
Formulate the Null and perform a t-Test for means from two samples.
Calculate all required statistics using a calculator, including the correct variance.
Interpret and evaluate the result.

\exercise{zandt30} The co-supervisor of the dissertation that led to the study in Exercise~\ref{exer:zandt20} reads the report and quickly writes an email to the author\slash candidate.
He says that the study is invalid because the sample wasn't \textit{balanced}.
It should have contained an equal number of data points from men and from women.
What do you -- the main supervisor -- reply to him to defend your candidate?

\end{exercises}
